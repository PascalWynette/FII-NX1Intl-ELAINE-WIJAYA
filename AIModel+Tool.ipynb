{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PascalWynette/FII-NX1Intl-ELAINE-WIJAYA/blob/main/NexHax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP6s_M6ynDeB",
        "outputId": "29b95682-e258-45e9-ca2c-ae10a58265e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/My Drive/NexHack/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1qQRAhH0klG"
      },
      "source": [
        "### **The MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UxIVBeBJyZ1c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c7k5SfSw0172",
        "outputId": "7fc3e9c5-9e42-4bb0-b044-abce8b13dd8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4.65G/5.00G [03:34<00:15, 22.4MB/s]"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.Food101(root=\"./data\", split=\"train\", transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_dataset = datasets.Food101(root=\"./data\", split=\"test\", transform=transform, download=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "num_classes = len(train_dataset.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2J7X5ZS4dSt"
      },
      "outputs": [],
      "source": [
        "full_dataset = train_dataset\n",
        "\n",
        "subset_size = int(len(full_dataset))\n",
        "subset_indices = random.sample(range(len(full_dataset)), subset_size)\n",
        "subset = Subset(full_dataset, subset_indices)\n",
        "\n",
        "train_size = int(0.8 * subset_size)\n",
        "val_size = subset_size - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(subset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "num_classes = len(full_dataset.classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QOPUiyxk9xtf"
      },
      "outputs": [],
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0aLZTEI-3Kq"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "train_precisions = []\n",
        "val_precisions = []\n",
        "\n",
        "def evaluate_val(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds, all_labels = [], []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zEQxc5bAw22"
      },
      "outputs": [],
      "source": [
        "training_args = {\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"weight_decay\": 0.02,\n",
        "    \"per_device_train_batch_size\": 32,\n",
        "    \"num_train_epochs\": 20,\n",
        "    \"logging_steps\": 10,\n",
        "    \"step_lr_step_size\": 2,\n",
        "    \"step_lr_gamma\": 0.1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwZwrLiVW7lg"
      },
      "outputs": [],
      "source": [
        "tiny_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "tiny_batch = next(iter(tiny_loader))\n",
        "images, labels = tiny_batch[0].to(device), tiny_batch[1].to(device)\n",
        "\n",
        "model.train()\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"[Step {i+1}] Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QKqr4351081G",
        "outputId": "d533324c-8775-487f-8e60-5b1946ea2c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 10] Loss: 4.1488\n",
            "[Step 20] Loss: 4.5112\n",
            "[Step 30] Loss: 4.0904\n",
            "[Step 40] Loss: 4.0379\n",
            "[Step 50] Loss: 4.2081\n",
            "[Step 60] Loss: 4.0219\n",
            "[Step 70] Loss: 4.3869\n",
            "[Step 80] Loss: 4.1716\n",
            "[Step 90] Loss: 4.0650\n",
            "[Step 100] Loss: 3.9564\n",
            "[Step 110] Loss: 3.8677\n",
            "[Step 120] Loss: 4.0251\n",
            "[Step 130] Loss: 3.9226\n",
            "[Step 140] Loss: 4.3001\n",
            "[Step 150] Loss: 3.9278\n",
            "[Step 160] Loss: 4.1120\n",
            "[Step 170] Loss: 4.2676\n",
            "[Step 180] Loss: 3.9945\n",
            "[Step 190] Loss: 3.7172\n",
            "[Step 200] Loss: 3.7584\n",
            "[Step 210] Loss: 4.0755\n",
            "[Step 220] Loss: 3.8309\n",
            "[Step 230] Loss: 3.8022\n",
            "[Step 240] Loss: 4.1181\n",
            "[Step 250] Loss: 4.0258\n",
            "[Step 260] Loss: 4.0064\n",
            "[Step 270] Loss: 4.2213\n",
            "[Step 280] Loss: 4.1547\n",
            "[Step 290] Loss: 4.0944\n",
            "[Step 300] Loss: 3.9386\n",
            "[Step 310] Loss: 4.0079\n",
            "[Step 320] Loss: 3.7295\n",
            "[Step 330] Loss: 4.1446\n",
            "[Step 340] Loss: 3.8298\n",
            "[Step 350] Loss: 3.9528\n",
            "[Step 360] Loss: 4.1868\n",
            "[Step 370] Loss: 3.7980\n",
            "[Step 380] Loss: 3.8407\n",
            "[Step 390] Loss: 3.6583\n",
            "[Step 400] Loss: 3.7339\n",
            "[Step 410] Loss: 3.8187\n",
            "[Step 420] Loss: 4.1793\n",
            "[Step 430] Loss: 3.8229\n",
            "[Step 440] Loss: 3.8645\n",
            "[Step 450] Loss: 3.9336\n",
            "[Step 460] Loss: 3.9759\n",
            "[Step 470] Loss: 3.7102\n",
            "[Step 480] Loss: 3.8813\n",
            "[Step 490] Loss: 3.7826\n",
            "[Step 500] Loss: 4.0974\n",
            "[Step 510] Loss: 3.9807\n",
            "[Step 520] Loss: 4.1243\n",
            "[Step 530] Loss: 4.0587\n",
            "[Step 540] Loss: 3.6099\n",
            "[Step 550] Loss: 3.8688\n",
            "[Step 560] Loss: 3.7032\n",
            "[Step 570] Loss: 3.7238\n",
            "[Step 580] Loss: 4.1515\n",
            "[Step 590] Loss: 3.9607\n",
            "[Step 600] Loss: 3.8297\n",
            "[Step 610] Loss: 3.9974\n",
            "[Step 620] Loss: 3.8947\n",
            "[Step 630] Loss: 3.6225\n",
            "[Step 640] Loss: 3.9014\n",
            "[Step 650] Loss: 3.6706\n",
            "[Step 660] Loss: 3.7822\n",
            "[Step 670] Loss: 3.9862\n",
            "[Step 680] Loss: 3.9290\n",
            "[Step 690] Loss: 3.7655\n",
            "[Step 700] Loss: 3.5329\n",
            "[Step 710] Loss: 3.8173\n",
            "[Step 720] Loss: 3.9130\n",
            "[Step 730] Loss: 3.8419\n",
            "[Step 740] Loss: 3.4576\n",
            "[Step 750] Loss: 3.8624\n",
            "[Step 760] Loss: 4.0501\n",
            "[Step 770] Loss: 3.6161\n",
            "[Step 780] Loss: 3.8705\n",
            "[Step 790] Loss: 3.8837\n",
            "[Step 800] Loss: 3.7298\n",
            "[Step 810] Loss: 3.7492\n",
            "[Step 820] Loss: 4.0377\n",
            "[Step 830] Loss: 3.8333\n",
            "[Step 840] Loss: 3.5875\n",
            "[Step 850] Loss: 3.8540\n",
            "[Step 860] Loss: 3.6582\n",
            "[Step 870] Loss: 3.8565\n",
            "[Step 880] Loss: 3.7659\n",
            "[Step 890] Loss: 3.7161\n",
            "[Step 900] Loss: 3.7034\n",
            "[Step 910] Loss: 4.0382\n",
            "[Step 920] Loss: 3.9818\n",
            "[Step 930] Loss: 3.7121\n",
            "[Step 940] Loss: 3.8458\n",
            "[Step 950] Loss: 3.6355\n",
            "[Step 960] Loss: 3.7118\n",
            "[Step 970] Loss: 3.7792\n",
            "[Step 980] Loss: 3.6518\n",
            "[Step 990] Loss: 3.7077\n",
            "[Step 1000] Loss: 4.0273\n",
            "[Step 1010] Loss: 4.1693\n",
            "[Step 1020] Loss: 3.7689\n",
            "[Step 1030] Loss: 3.7070\n",
            "[Step 1040] Loss: 3.9301\n",
            "[Step 1050] Loss: 4.0062\n",
            "[Step 1060] Loss: 3.8199\n",
            "[Step 1070] Loss: 3.6000\n",
            "[Step 1080] Loss: 4.0170\n",
            "[Step 1090] Loss: 3.8612\n",
            "[Step 1100] Loss: 3.9168\n",
            "[Step 1110] Loss: 3.7875\n",
            "[Step 1120] Loss: 3.7667\n",
            "[Step 1130] Loss: 3.7837\n",
            "[Step 1140] Loss: 3.8109\n",
            "[Step 1150] Loss: 3.6304\n",
            "[Step 1160] Loss: 3.7177\n",
            "[Step 1170] Loss: 3.5565\n",
            "[Step 1180] Loss: 3.7716\n",
            "[Step 1190] Loss: 3.5931\n",
            "[Step 1200] Loss: 3.6605\n",
            "[Step 1210] Loss: 3.9291\n",
            "[Step 1220] Loss: 3.7323\n",
            "[Step 1230] Loss: 4.0084\n",
            "[Step 1240] Loss: 3.8387\n",
            "[Step 1250] Loss: 3.9075\n",
            "[Step 1260] Loss: 3.5116\n",
            "[Step 1270] Loss: 3.8797\n",
            "[Step 1280] Loss: 3.9117\n",
            "[Step 1290] Loss: 3.6686\n",
            "[Step 1300] Loss: 3.6569\n",
            "[Step 1310] Loss: 3.7262\n",
            "[Step 1320] Loss: 3.4776\n",
            "[Step 1330] Loss: 3.7527\n",
            "[Step 1340] Loss: 3.6652\n",
            "[Step 1350] Loss: 3.7335\n",
            "[Step 1360] Loss: 3.5090\n",
            "[Step 1370] Loss: 3.5898\n",
            "[Step 1380] Loss: 4.0812\n",
            "[Step 1390] Loss: 3.9153\n",
            "[Step 1400] Loss: 4.0156\n",
            "[Step 1410] Loss: 3.7507\n",
            "[Step 1420] Loss: 3.4908\n",
            "[Step 1430] Loss: 4.0768\n",
            "[Step 1440] Loss: 3.5731\n",
            "[Step 1450] Loss: 3.8039\n",
            "[Step 1460] Loss: 3.7513\n",
            "[Step 1470] Loss: 3.4596\n",
            "[Step 1480] Loss: 3.5538\n",
            "[Step 1490] Loss: 3.6095\n",
            "[Step 1500] Loss: 3.4213\n",
            "[Step 1510] Loss: 3.9098\n",
            "[Step 1520] Loss: 3.6246\n",
            "[Step 1530] Loss: 3.6207\n",
            "[Step 1540] Loss: 3.7303\n",
            "[Step 1550] Loss: 3.8414\n",
            "[Step 1560] Loss: 3.5625\n",
            "[Step 1570] Loss: 3.7223\n",
            "[Step 1580] Loss: 3.6728\n",
            "[Step 1590] Loss: 3.6498\n",
            "[Step 1600] Loss: 3.7430\n",
            "[Step 1610] Loss: 3.6163\n",
            "[Step 1620] Loss: 3.8268\n",
            "[Step 1630] Loss: 3.7196\n",
            "[Step 1640] Loss: 3.7870\n",
            "[Step 1650] Loss: 3.7491\n",
            "[Step 1660] Loss: 3.6879\n",
            "[Step 1670] Loss: 3.7960\n",
            "[Step 1680] Loss: 3.8362\n",
            "[Step 1690] Loss: 3.6205\n",
            "[Step 1700] Loss: 3.5327\n",
            "[Step 1710] Loss: 3.6850\n",
            "[Step 1720] Loss: 3.5684\n",
            "[Step 1730] Loss: 3.8498\n",
            "[Step 1740] Loss: 3.8155\n",
            "[Step 1750] Loss: 3.7085\n",
            "[Step 1760] Loss: 3.8524\n",
            "[Step 1770] Loss: 3.5753\n",
            "[Step 1780] Loss: 3.6071\n",
            "[Step 1790] Loss: 3.6957\n",
            "[Step 1800] Loss: 3.8701\n",
            "[Step 1810] Loss: 3.5234\n",
            "[Step 1820] Loss: 3.5098\n",
            "[Step 1830] Loss: 3.4800\n",
            "[Step 1840] Loss: 3.7535\n",
            "[Step 1850] Loss: 3.7789\n",
            "[Step 1860] Loss: 3.6777\n",
            "[Step 1870] Loss: 3.8962\n",
            "[Step 1880] Loss: 4.0395\n",
            "[Step 1890] Loss: 3.5787\n",
            "ðŸ“˜ Epoch 1/20 - Avg Training Loss: 3.8342\n",
            "[Step 1900] Loss: 3.9158\n",
            "[Step 1910] Loss: 3.7815\n",
            "[Step 1920] Loss: 3.6374\n",
            "[Step 1930] Loss: 3.8148\n",
            "[Step 1940] Loss: 3.7931\n",
            "[Step 1950] Loss: 3.5867\n",
            "[Step 1960] Loss: 3.5605\n",
            "[Step 1970] Loss: 3.7066\n",
            "[Step 1980] Loss: 3.7275\n",
            "[Step 1990] Loss: 3.5656\n",
            "[Step 2000] Loss: 3.6302\n",
            "[Step 2010] Loss: 3.9640\n",
            "[Step 2020] Loss: 3.8425\n",
            "[Step 2030] Loss: 3.6235\n",
            "[Step 2040] Loss: 3.8203\n",
            "[Step 2050] Loss: 3.6445\n",
            "[Step 2060] Loss: 3.6572\n",
            "[Step 2070] Loss: 3.6944\n",
            "[Step 2080] Loss: 3.7154\n",
            "[Step 2090] Loss: 3.4887\n",
            "[Step 2100] Loss: 3.3666\n",
            "[Step 2110] Loss: 3.4155\n",
            "[Step 2120] Loss: 3.9774\n",
            "[Step 2130] Loss: 3.6830\n",
            "[Step 2140] Loss: 3.6535\n",
            "[Step 2150] Loss: 3.6973\n",
            "[Step 2160] Loss: 3.7744\n",
            "[Step 2170] Loss: 3.6480\n",
            "[Step 2180] Loss: 3.6998\n",
            "[Step 2190] Loss: 3.4822\n",
            "[Step 2200] Loss: 3.8011\n",
            "[Step 2210] Loss: 3.7937\n",
            "[Step 2220] Loss: 3.5336\n",
            "[Step 2230] Loss: 3.7912\n",
            "[Step 2240] Loss: 3.9803\n",
            "[Step 2250] Loss: 3.5674\n",
            "[Step 2260] Loss: 3.7565\n",
            "[Step 2270] Loss: 3.6352\n",
            "[Step 2280] Loss: 3.3704\n",
            "[Step 2290] Loss: 3.8299\n",
            "[Step 2300] Loss: 3.3628\n",
            "[Step 2310] Loss: 3.4435\n",
            "[Step 2320] Loss: 3.7050\n",
            "[Step 2330] Loss: 3.7273\n",
            "[Step 2340] Loss: 3.8384\n",
            "[Step 2350] Loss: 3.6228\n",
            "[Step 2360] Loss: 3.6039\n",
            "[Step 2370] Loss: 3.6217\n",
            "[Step 2380] Loss: 3.3591\n",
            "[Step 2390] Loss: 3.4852\n",
            "[Step 2400] Loss: 3.6436\n",
            "[Step 2410] Loss: 3.7616\n",
            "[Step 2420] Loss: 3.8672\n",
            "[Step 2430] Loss: 3.7251\n",
            "[Step 2440] Loss: 3.5667\n",
            "[Step 2450] Loss: 3.5045\n",
            "[Step 2460] Loss: 3.6504\n",
            "[Step 2470] Loss: 3.2446\n",
            "[Step 2480] Loss: 3.9524\n",
            "[Step 2490] Loss: 3.7037\n",
            "[Step 2500] Loss: 3.8222\n",
            "[Step 2510] Loss: 3.7475\n",
            "[Step 2520] Loss: 3.5218\n",
            "[Step 2530] Loss: 3.9037\n",
            "[Step 2540] Loss: 3.8481\n",
            "[Step 2550] Loss: 3.6877\n",
            "[Step 2560] Loss: 3.6821\n",
            "[Step 2570] Loss: 3.5244\n",
            "[Step 2580] Loss: 3.8516\n",
            "[Step 2590] Loss: 3.6297\n",
            "[Step 2600] Loss: 3.6054\n",
            "[Step 2610] Loss: 3.9704\n",
            "[Step 2620] Loss: 3.8305\n",
            "[Step 2630] Loss: 3.6495\n",
            "[Step 2640] Loss: 3.7336\n",
            "[Step 2650] Loss: 3.8204\n",
            "[Step 2660] Loss: 3.9208\n",
            "[Step 2670] Loss: 3.6454\n",
            "[Step 2680] Loss: 3.5508\n",
            "[Step 2690] Loss: 3.6586\n",
            "[Step 2700] Loss: 3.6719\n",
            "[Step 2710] Loss: 3.5908\n",
            "[Step 2720] Loss: 3.5006\n",
            "[Step 2730] Loss: 3.4595\n",
            "[Step 2740] Loss: 3.3650\n",
            "[Step 2750] Loss: 3.9064\n",
            "[Step 2760] Loss: 3.7288\n",
            "[Step 2770] Loss: 3.6555\n",
            "[Step 2780] Loss: 3.8375\n",
            "[Step 2790] Loss: 3.4839\n",
            "[Step 2800] Loss: 3.5500\n",
            "[Step 2810] Loss: 3.8047\n",
            "[Step 2820] Loss: 3.5871\n",
            "[Step 2830] Loss: 3.7182\n",
            "[Step 2840] Loss: 3.7167\n",
            "[Step 2850] Loss: 3.4354\n",
            "[Step 2860] Loss: 3.6160\n",
            "[Step 2870] Loss: 3.5741\n",
            "[Step 2880] Loss: 3.8799\n",
            "[Step 2890] Loss: 3.6329\n",
            "[Step 2900] Loss: 3.5480\n",
            "[Step 2910] Loss: 3.7839\n",
            "[Step 2920] Loss: 3.7123\n",
            "[Step 2930] Loss: 3.6898\n",
            "[Step 2940] Loss: 3.5022\n",
            "[Step 2950] Loss: 3.5303\n",
            "[Step 2960] Loss: 3.8042\n",
            "[Step 2970] Loss: 3.8007\n",
            "[Step 2980] Loss: 3.7258\n",
            "[Step 2990] Loss: 3.7675\n",
            "[Step 3000] Loss: 3.6827\n",
            "[Step 3010] Loss: 3.6475\n",
            "[Step 3020] Loss: 3.6121\n",
            "[Step 3030] Loss: 3.7661\n",
            "[Step 3040] Loss: 3.7071\n",
            "[Step 3050] Loss: 3.4751\n",
            "[Step 3060] Loss: 3.5479\n",
            "[Step 3070] Loss: 3.7324\n",
            "[Step 3080] Loss: 3.7339\n",
            "[Step 3090] Loss: 3.3976\n",
            "[Step 3100] Loss: 3.5757\n",
            "[Step 3110] Loss: 3.5669\n",
            "[Step 3120] Loss: 3.7910\n",
            "[Step 3130] Loss: 3.7094\n",
            "[Step 3140] Loss: 3.7462\n",
            "[Step 3150] Loss: 3.8079\n",
            "[Step 3160] Loss: 3.6371\n",
            "[Step 3170] Loss: 3.8956\n",
            "[Step 3180] Loss: 3.6506\n",
            "[Step 3190] Loss: 3.5891\n",
            "[Step 3200] Loss: 3.7311\n",
            "[Step 3210] Loss: 3.6244\n",
            "[Step 3220] Loss: 3.6758\n",
            "[Step 3230] Loss: 3.6091\n",
            "[Step 3240] Loss: 3.9465\n",
            "[Step 3250] Loss: 3.5604\n",
            "[Step 3260] Loss: 3.8002\n",
            "[Step 3270] Loss: 3.6277\n",
            "[Step 3280] Loss: 3.4542\n",
            "[Step 3290] Loss: 3.6317\n",
            "[Step 3300] Loss: 3.3872\n",
            "[Step 3310] Loss: 3.4094\n",
            "[Step 3320] Loss: 3.8181\n",
            "[Step 3330] Loss: 3.7255\n",
            "[Step 3340] Loss: 3.5817\n",
            "[Step 3350] Loss: 3.7816\n",
            "[Step 3360] Loss: 3.7983\n",
            "[Step 3370] Loss: 3.8961\n",
            "[Step 3380] Loss: 3.5182\n",
            "[Step 3390] Loss: 3.7061\n",
            "[Step 3400] Loss: 3.9199\n",
            "[Step 3410] Loss: 3.4869\n",
            "[Step 3420] Loss: 3.7185\n",
            "[Step 3430] Loss: 3.8364\n",
            "[Step 3440] Loss: 3.6037\n",
            "[Step 3450] Loss: 3.6027\n",
            "[Step 3460] Loss: 3.6744\n",
            "[Step 3470] Loss: 3.6839\n",
            "[Step 3480] Loss: 3.7377\n",
            "[Step 3490] Loss: 3.5318\n",
            "[Step 3500] Loss: 3.9254\n",
            "[Step 3510] Loss: 3.5632\n",
            "[Step 3520] Loss: 3.4385\n",
            "[Step 3530] Loss: 3.6789\n",
            "[Step 3540] Loss: 3.5708\n",
            "[Step 3550] Loss: 3.5814\n",
            "[Step 3560] Loss: 3.6977\n",
            "[Step 3570] Loss: 3.6472\n",
            "[Step 3580] Loss: 3.6760\n",
            "[Step 3590] Loss: 3.6937\n",
            "[Step 3600] Loss: 3.6322\n",
            "[Step 3610] Loss: 3.7158\n",
            "[Step 3620] Loss: 3.5559\n",
            "[Step 3630] Loss: 3.5509\n",
            "[Step 3640] Loss: 3.5916\n",
            "[Step 3650] Loss: 3.5235\n",
            "[Step 3660] Loss: 3.4703\n",
            "[Step 3670] Loss: 3.5154\n",
            "[Step 3680] Loss: 3.4851\n",
            "[Step 3690] Loss: 3.6688\n",
            "[Step 3700] Loss: 3.4452\n",
            "[Step 3710] Loss: 3.8874\n",
            "[Step 3720] Loss: 3.4881\n",
            "[Step 3730] Loss: 3.4577\n",
            "[Step 3740] Loss: 3.5671\n",
            "[Step 3750] Loss: 3.6490\n",
            "[Step 3760] Loss: 3.6106\n",
            "[Step 3770] Loss: 3.4121\n",
            "[Step 3780] Loss: 3.5724\n",
            "ðŸ“˜ Epoch 2/20 - Avg Training Loss: 3.6426\n",
            "[Step 3790] Loss: 3.5849\n",
            "[Step 3800] Loss: 3.4835\n",
            "[Step 3810] Loss: 3.6427\n",
            "[Step 3820] Loss: 3.3624\n",
            "[Step 3830] Loss: 3.6437\n",
            "[Step 3840] Loss: 3.6135\n",
            "[Step 3850] Loss: 3.4794\n",
            "[Step 3860] Loss: 3.4660\n",
            "[Step 3870] Loss: 3.7219\n",
            "[Step 3880] Loss: 3.7182\n",
            "[Step 3890] Loss: 3.5641\n",
            "[Step 3900] Loss: 3.7328\n",
            "[Step 3910] Loss: 3.8485\n",
            "[Step 3920] Loss: 3.7157\n",
            "[Step 3930] Loss: 3.7238\n",
            "[Step 3940] Loss: 3.5886\n",
            "[Step 3950] Loss: 3.5178\n",
            "[Step 3960] Loss: 4.0760\n",
            "[Step 3970] Loss: 3.7844\n",
            "[Step 3980] Loss: 3.6756\n",
            "[Step 3990] Loss: 3.7624\n",
            "[Step 4000] Loss: 3.7788\n",
            "[Step 4010] Loss: 3.3432\n",
            "[Step 4020] Loss: 3.5455\n",
            "[Step 4030] Loss: 3.6805\n",
            "[Step 4040] Loss: 3.5411\n",
            "[Step 4050] Loss: 3.7062\n",
            "[Step 4060] Loss: 3.6786\n",
            "[Step 4070] Loss: 3.5623\n",
            "[Step 4080] Loss: 3.5368\n",
            "[Step 4090] Loss: 3.8833\n",
            "[Step 4100] Loss: 3.4258\n",
            "[Step 4110] Loss: 3.6757\n",
            "[Step 4120] Loss: 3.7149\n",
            "[Step 4130] Loss: 3.3606\n",
            "[Step 4140] Loss: 3.7677\n",
            "[Step 4150] Loss: 3.3928\n",
            "[Step 4160] Loss: 3.5933\n",
            "[Step 4170] Loss: 3.7349\n",
            "[Step 4180] Loss: 3.5904\n",
            "[Step 4190] Loss: 3.5600\n",
            "[Step 4200] Loss: 3.7416\n",
            "[Step 4210] Loss: 3.5522\n",
            "[Step 4220] Loss: 3.5560\n",
            "[Step 4230] Loss: 3.6906\n",
            "[Step 4240] Loss: 3.4814\n",
            "[Step 4250] Loss: 3.6734\n",
            "[Step 4260] Loss: 3.5773\n",
            "[Step 4270] Loss: 3.3932\n",
            "[Step 4280] Loss: 3.6276\n",
            "[Step 4290] Loss: 3.5545\n",
            "[Step 4300] Loss: 3.6543\n",
            "[Step 4310] Loss: 3.5383\n",
            "[Step 4320] Loss: 3.6105\n",
            "[Step 4330] Loss: 3.7077\n",
            "[Step 4340] Loss: 3.3480\n",
            "[Step 4350] Loss: 3.6708\n",
            "[Step 4360] Loss: 3.6278\n",
            "[Step 4370] Loss: 3.4993\n",
            "[Step 4380] Loss: 3.8002\n",
            "[Step 4390] Loss: 3.7414\n",
            "[Step 4400] Loss: 3.7703\n",
            "[Step 4410] Loss: 3.6629\n",
            "[Step 4420] Loss: 3.4566\n",
            "[Step 4430] Loss: 3.6347\n",
            "[Step 4440] Loss: 3.7010\n",
            "[Step 4450] Loss: 3.4986\n",
            "[Step 4460] Loss: 3.7875\n",
            "[Step 4470] Loss: 3.6483\n",
            "[Step 4480] Loss: 3.6732\n",
            "[Step 4490] Loss: 3.5061\n",
            "[Step 4500] Loss: 3.5289\n",
            "[Step 4510] Loss: 3.4887\n",
            "[Step 4520] Loss: 3.1819\n",
            "[Step 4530] Loss: 3.1365\n",
            "[Step 4540] Loss: 3.4839\n",
            "[Step 4550] Loss: 3.7920\n",
            "[Step 4560] Loss: 3.6047\n",
            "[Step 4570] Loss: 3.4380\n",
            "[Step 4580] Loss: 3.6101\n",
            "[Step 4590] Loss: 3.3775\n",
            "[Step 4600] Loss: 3.5589\n",
            "[Step 4610] Loss: 3.7174\n",
            "[Step 4620] Loss: 3.7466\n",
            "[Step 4630] Loss: 3.6491\n",
            "[Step 4640] Loss: 3.6616\n",
            "[Step 4650] Loss: 3.6251\n",
            "[Step 4660] Loss: 3.7390\n",
            "[Step 4670] Loss: 3.3744\n",
            "[Step 4680] Loss: 3.6625\n",
            "[Step 4690] Loss: 3.5505\n",
            "[Step 4700] Loss: 3.5279\n",
            "[Step 4710] Loss: 3.7161\n",
            "[Step 4720] Loss: 3.5412\n",
            "[Step 4730] Loss: 3.5090\n",
            "[Step 4740] Loss: 3.6111\n",
            "[Step 4750] Loss: 3.5640\n",
            "[Step 4760] Loss: 3.5651\n",
            "[Step 4770] Loss: 3.7301\n",
            "[Step 4780] Loss: 3.4240\n",
            "[Step 4790] Loss: 3.8030\n",
            "[Step 4800] Loss: 3.7169\n",
            "[Step 4810] Loss: 3.3945\n",
            "[Step 4820] Loss: 3.6615\n",
            "[Step 4830] Loss: 3.5328\n",
            "[Step 4840] Loss: 3.4846\n",
            "[Step 4850] Loss: 3.2835\n",
            "[Step 4860] Loss: 3.4834\n",
            "[Step 4870] Loss: 3.7013\n",
            "[Step 4880] Loss: 3.7187\n",
            "[Step 4890] Loss: 3.6427\n",
            "[Step 4900] Loss: 3.3960\n",
            "[Step 4910] Loss: 3.7932\n",
            "[Step 4920] Loss: 3.4800\n",
            "[Step 4930] Loss: 3.7119\n",
            "[Step 4940] Loss: 3.3453\n",
            "[Step 4950] Loss: 3.5994\n",
            "[Step 4960] Loss: 3.7297\n",
            "[Step 4970] Loss: 3.4783\n",
            "[Step 4980] Loss: 3.2275\n",
            "[Step 4990] Loss: 3.5242\n",
            "[Step 5000] Loss: 3.5396\n",
            "[Step 5010] Loss: 3.4190\n",
            "[Step 5020] Loss: 3.3772\n",
            "[Step 5030] Loss: 3.5727\n",
            "[Step 5040] Loss: 3.5334\n",
            "[Step 5050] Loss: 3.5108\n",
            "[Step 5060] Loss: 3.3563\n",
            "[Step 5070] Loss: 3.3047\n",
            "[Step 5080] Loss: 3.6614\n",
            "[Step 5090] Loss: 3.4440\n",
            "[Step 5100] Loss: 3.8205\n",
            "[Step 5110] Loss: 3.5016\n",
            "[Step 5120] Loss: 3.5789\n",
            "[Step 5130] Loss: 3.6921\n",
            "[Step 5140] Loss: 3.4396\n",
            "[Step 5150] Loss: 3.6052\n",
            "[Step 5160] Loss: 3.6939\n",
            "[Step 5170] Loss: 3.7740\n",
            "[Step 5180] Loss: 3.4516\n",
            "[Step 5190] Loss: 3.4768\n",
            "[Step 5200] Loss: 3.4629\n",
            "[Step 5210] Loss: 3.8009\n",
            "[Step 5220] Loss: 3.7296\n",
            "[Step 5230] Loss: 3.7723\n",
            "[Step 5240] Loss: 3.7077\n",
            "[Step 5250] Loss: 3.4638\n",
            "[Step 5260] Loss: 3.4348\n",
            "[Step 5270] Loss: 3.6124\n",
            "[Step 5280] Loss: 3.3331\n",
            "[Step 5290] Loss: 3.4979\n",
            "[Step 5300] Loss: 3.3653\n",
            "[Step 5310] Loss: 3.6210\n",
            "[Step 5320] Loss: 3.4252\n",
            "[Step 5330] Loss: 3.8791\n",
            "[Step 5340] Loss: 3.7203\n",
            "[Step 5350] Loss: 3.5782\n",
            "[Step 5360] Loss: 3.6780\n",
            "[Step 5370] Loss: 3.7953\n",
            "[Step 5380] Loss: 3.3443\n",
            "[Step 5390] Loss: 3.7370\n",
            "[Step 5400] Loss: 3.3343\n",
            "[Step 5410] Loss: 3.6168\n",
            "[Step 5420] Loss: 3.7265\n",
            "[Step 5430] Loss: 3.7665\n",
            "[Step 5440] Loss: 3.5051\n",
            "[Step 5450] Loss: 3.5222\n",
            "[Step 5460] Loss: 3.4104\n",
            "[Step 5470] Loss: 3.3302\n",
            "[Step 5480] Loss: 3.7140\n",
            "[Step 5490] Loss: 3.6922\n",
            "[Step 5500] Loss: 3.5290\n",
            "[Step 5510] Loss: 3.6366\n",
            "[Step 5520] Loss: 3.5657\n",
            "[Step 5530] Loss: 3.4498\n",
            "[Step 5540] Loss: 3.5570\n",
            "[Step 5550] Loss: 3.4864\n",
            "[Step 5560] Loss: 3.7210\n",
            "[Step 5570] Loss: 3.5794\n",
            "[Step 5580] Loss: 3.6034\n",
            "[Step 5590] Loss: 3.7416\n",
            "[Step 5600] Loss: 3.6685\n",
            "[Step 5610] Loss: 3.8048\n",
            "[Step 5620] Loss: 3.4598\n",
            "[Step 5630] Loss: 3.7668\n",
            "[Step 5640] Loss: 3.4190\n",
            "[Step 5650] Loss: 3.6632\n",
            "[Step 5660] Loss: 3.5122\n",
            "[Step 5670] Loss: 3.3405\n",
            "[Step 5680] Loss: 3.1510\n",
            "ðŸ“˜ Epoch 3/20 - Avg Training Loss: 3.5978\n",
            "[Step 5690] Loss: 3.6749\n",
            "[Step 5700] Loss: 3.5542\n",
            "[Step 5710] Loss: 3.5574\n",
            "[Step 5720] Loss: 3.6199\n",
            "[Step 5730] Loss: 3.5105\n",
            "[Step 5740] Loss: 3.6151\n",
            "[Step 5750] Loss: 3.7207\n",
            "[Step 5760] Loss: 3.6745\n",
            "[Step 5770] Loss: 3.6444\n",
            "[Step 5780] Loss: 3.7163\n",
            "[Step 5790] Loss: 3.9219\n",
            "[Step 5800] Loss: 3.7273\n",
            "[Step 5810] Loss: 3.5416\n",
            "[Step 5820] Loss: 3.7331\n",
            "[Step 5830] Loss: 3.5623\n",
            "[Step 5840] Loss: 3.7438\n",
            "[Step 5850] Loss: 3.6646\n",
            "[Step 5860] Loss: 3.4329\n",
            "[Step 5870] Loss: 3.7187\n",
            "[Step 5880] Loss: 3.5786\n",
            "[Step 5890] Loss: 3.3920\n",
            "[Step 5900] Loss: 3.3452\n",
            "[Step 5910] Loss: 3.2128\n",
            "[Step 5920] Loss: 3.8066\n",
            "[Step 5930] Loss: 3.8129\n",
            "[Step 5940] Loss: 3.3946\n",
            "[Step 5950] Loss: 3.5037\n",
            "[Step 5960] Loss: 3.3231\n",
            "[Step 5970] Loss: 3.7853\n",
            "[Step 5980] Loss: 3.4294\n",
            "[Step 5990] Loss: 3.6157\n",
            "[Step 6000] Loss: 3.4467\n",
            "[Step 6010] Loss: 3.6449\n",
            "[Step 6020] Loss: 3.4435\n",
            "[Step 6030] Loss: 3.5665\n",
            "[Step 6040] Loss: 3.4161\n",
            "[Step 6050] Loss: 3.6674\n",
            "[Step 6060] Loss: 3.5893\n",
            "[Step 6070] Loss: 3.9950\n",
            "[Step 6080] Loss: 3.6737\n",
            "[Step 6090] Loss: 3.5501\n",
            "[Step 6100] Loss: 3.4514\n",
            "[Step 6110] Loss: 3.7987\n",
            "[Step 6120] Loss: 3.4447\n",
            "[Step 6130] Loss: 3.3475\n",
            "[Step 6140] Loss: 3.2936\n",
            "[Step 6150] Loss: 3.4312\n",
            "[Step 6160] Loss: 3.4463\n",
            "[Step 6170] Loss: 3.2414\n",
            "[Step 6180] Loss: 3.4192\n",
            "[Step 6190] Loss: 3.4966\n",
            "[Step 6200] Loss: 3.5286\n",
            "[Step 6210] Loss: 3.7744\n",
            "[Step 6220] Loss: 3.4759\n",
            "[Step 6230] Loss: 3.6034\n",
            "[Step 6240] Loss: 3.3630\n",
            "[Step 6250] Loss: 3.4459\n",
            "[Step 6260] Loss: 3.4784\n",
            "[Step 6270] Loss: 3.4373\n",
            "[Step 6280] Loss: 3.4362\n",
            "[Step 6290] Loss: 3.5679\n",
            "[Step 6300] Loss: 3.4935\n",
            "[Step 6310] Loss: 3.4979\n",
            "[Step 6320] Loss: 3.7651\n",
            "[Step 6330] Loss: 3.4903\n",
            "[Step 6340] Loss: 3.2826\n",
            "[Step 6350] Loss: 3.7542\n",
            "[Step 6360] Loss: 3.6233\n",
            "[Step 6370] Loss: 3.4568\n",
            "[Step 6380] Loss: 3.5121\n",
            "[Step 6390] Loss: 3.7938\n",
            "[Step 6400] Loss: 3.6608\n",
            "[Step 6410] Loss: 3.4988\n",
            "[Step 6420] Loss: 3.6112\n",
            "[Step 6430] Loss: 3.4217\n",
            "[Step 6440] Loss: 3.4020\n",
            "[Step 6450] Loss: 3.6189\n",
            "[Step 6460] Loss: 3.5863\n",
            "[Step 6470] Loss: 3.4429\n",
            "[Step 6480] Loss: 3.7038\n",
            "[Step 6490] Loss: 3.5241\n",
            "[Step 6500] Loss: 3.6996\n",
            "[Step 6510] Loss: 3.5735\n",
            "[Step 6520] Loss: 3.5642\n",
            "[Step 6530] Loss: 3.4681\n",
            "[Step 6540] Loss: 3.6676\n",
            "[Step 6550] Loss: 3.5478\n",
            "[Step 6560] Loss: 3.8307\n",
            "[Step 6570] Loss: 3.4284\n",
            "[Step 6580] Loss: 3.4794\n",
            "[Step 6590] Loss: 3.4074\n",
            "[Step 6600] Loss: 3.6233\n",
            "[Step 6610] Loss: 3.2986\n",
            "[Step 6620] Loss: 3.5456\n",
            "[Step 6630] Loss: 3.3732\n",
            "[Step 6640] Loss: 3.8863\n",
            "[Step 6650] Loss: 3.8823\n",
            "[Step 6660] Loss: 3.6504\n",
            "[Step 6670] Loss: 3.7821\n",
            "[Step 6680] Loss: 3.6057\n",
            "[Step 6690] Loss: 3.2942\n",
            "[Step 6700] Loss: 3.5204\n",
            "[Step 6710] Loss: 3.4425\n",
            "[Step 6720] Loss: 3.7432\n",
            "[Step 6730] Loss: 3.8978\n",
            "[Step 6740] Loss: 3.6361\n",
            "[Step 6750] Loss: 3.4714\n",
            "[Step 6760] Loss: 3.4122\n",
            "[Step 6770] Loss: 3.5892\n",
            "[Step 6780] Loss: 3.7751\n",
            "[Step 6790] Loss: 3.7382\n",
            "[Step 6800] Loss: 3.8178\n",
            "[Step 6810] Loss: 3.5341\n",
            "[Step 6820] Loss: 3.3669\n",
            "[Step 6830] Loss: 3.7392\n",
            "[Step 6840] Loss: 3.6119\n",
            "[Step 6850] Loss: 3.7638\n",
            "[Step 6860] Loss: 3.8066\n",
            "[Step 6870] Loss: 3.4679\n",
            "[Step 6880] Loss: 3.6261\n",
            "[Step 6890] Loss: 3.4245\n",
            "[Step 6900] Loss: 3.6452\n",
            "[Step 6910] Loss: 3.9848\n",
            "[Step 6920] Loss: 3.4764\n",
            "[Step 6930] Loss: 3.4841\n",
            "[Step 6940] Loss: 3.6489\n",
            "[Step 6950] Loss: 3.6151\n",
            "[Step 6960] Loss: 3.8052\n",
            "[Step 6970] Loss: 3.1459\n",
            "[Step 6980] Loss: 3.5963\n",
            "[Step 6990] Loss: 3.7323\n",
            "[Step 7000] Loss: 3.6739\n",
            "[Step 7010] Loss: 3.3445\n",
            "[Step 7020] Loss: 3.6342\n",
            "[Step 7030] Loss: 3.5927\n",
            "[Step 7040] Loss: 3.3483\n",
            "[Step 7050] Loss: 3.5365\n",
            "[Step 7060] Loss: 3.7937\n",
            "[Step 7070] Loss: 3.3900\n",
            "[Step 7080] Loss: 3.4058\n",
            "[Step 7090] Loss: 3.4893\n",
            "[Step 7100] Loss: 3.7115\n",
            "[Step 7110] Loss: 3.6168\n",
            "[Step 7120] Loss: 3.4321\n",
            "[Step 7130] Loss: 3.4162\n",
            "[Step 7140] Loss: 3.8813\n",
            "[Step 7150] Loss: 3.5473\n",
            "[Step 7160] Loss: 3.8831\n",
            "[Step 7170] Loss: 3.7166\n",
            "[Step 7180] Loss: 3.4101\n",
            "[Step 7190] Loss: 3.5821\n",
            "[Step 7200] Loss: 3.5476\n",
            "[Step 7210] Loss: 3.7148\n",
            "[Step 7220] Loss: 3.4760\n",
            "[Step 7230] Loss: 3.7336\n",
            "[Step 7240] Loss: 3.8156\n",
            "[Step 7250] Loss: 3.7786\n",
            "[Step 7260] Loss: 3.6912\n",
            "[Step 7270] Loss: 3.8306\n",
            "[Step 7280] Loss: 3.6470\n",
            "[Step 7290] Loss: 3.6800\n",
            "[Step 7300] Loss: 3.5590\n",
            "[Step 7310] Loss: 3.6491\n",
            "[Step 7320] Loss: 3.5286\n",
            "[Step 7330] Loss: 3.2886\n",
            "[Step 7340] Loss: 3.5510\n",
            "[Step 7350] Loss: 3.5018\n",
            "[Step 7360] Loss: 3.5821\n",
            "[Step 7370] Loss: 3.7489\n",
            "[Step 7380] Loss: 3.5216\n",
            "[Step 7390] Loss: 3.5621\n",
            "[Step 7400] Loss: 3.3763\n",
            "[Step 7410] Loss: 3.4974\n",
            "[Step 7420] Loss: 3.6205\n",
            "[Step 7430] Loss: 3.6985\n",
            "[Step 7440] Loss: 3.6913\n",
            "[Step 7450] Loss: 3.5980\n",
            "[Step 7460] Loss: 3.6717\n",
            "[Step 7470] Loss: 3.7813\n",
            "[Step 7480] Loss: 3.7429\n",
            "[Step 7490] Loss: 3.5865\n",
            "[Step 7500] Loss: 3.5217\n",
            "[Step 7510] Loss: 3.6134\n",
            "[Step 7520] Loss: 3.2999\n",
            "[Step 7530] Loss: 3.6673\n",
            "[Step 7540] Loss: 3.8475\n",
            "[Step 7550] Loss: 3.8162\n",
            "[Step 7560] Loss: 3.5840\n",
            "[Step 7570] Loss: 3.6478\n",
            "ðŸ“˜ Epoch 4/20 - Avg Training Loss: 3.5814\n",
            "[Step 7580] Loss: 3.4639\n",
            "[Step 7590] Loss: 3.3756\n",
            "[Step 7600] Loss: 3.5865\n",
            "[Step 7610] Loss: 3.6043\n",
            "[Step 7620] Loss: 3.8444\n",
            "[Step 7630] Loss: 3.7888\n",
            "[Step 7640] Loss: 3.6250\n",
            "[Step 7650] Loss: 3.6063\n",
            "[Step 7660] Loss: 3.2595\n",
            "[Step 7670] Loss: 3.4707\n",
            "[Step 7680] Loss: 3.2877\n",
            "[Step 7690] Loss: 3.7086\n",
            "[Step 7700] Loss: 3.7321\n",
            "[Step 7710] Loss: 3.5297\n",
            "[Step 7720] Loss: 3.4216\n",
            "[Step 7730] Loss: 3.4530\n",
            "[Step 7740] Loss: 3.5757\n",
            "[Step 7750] Loss: 3.5084\n",
            "[Step 7760] Loss: 3.8767\n",
            "[Step 7770] Loss: 3.6639\n",
            "[Step 7780] Loss: 3.7657\n",
            "[Step 7790] Loss: 3.9233\n",
            "[Step 7800] Loss: 3.5072\n",
            "[Step 7810] Loss: 3.6062\n",
            "[Step 7820] Loss: 3.4233\n",
            "[Step 7830] Loss: 3.4847\n",
            "[Step 7840] Loss: 3.2242\n",
            "[Step 7850] Loss: 3.7330\n",
            "[Step 7860] Loss: 3.6557\n",
            "[Step 7870] Loss: 3.4300\n",
            "[Step 7880] Loss: 3.7242\n",
            "[Step 7890] Loss: 3.4282\n",
            "[Step 7900] Loss: 3.7774\n",
            "[Step 7910] Loss: 3.6279\n",
            "[Step 7920] Loss: 3.4871\n",
            "[Step 7930] Loss: 3.3945\n",
            "[Step 7940] Loss: 3.6788\n",
            "[Step 7950] Loss: 3.5350\n",
            "[Step 7960] Loss: 3.7662\n",
            "[Step 7970] Loss: 3.3606\n",
            "[Step 7980] Loss: 3.4532\n",
            "[Step 7990] Loss: 3.5296\n",
            "[Step 8000] Loss: 3.7246\n",
            "[Step 8010] Loss: 3.5319\n",
            "[Step 8020] Loss: 3.4674\n",
            "[Step 8030] Loss: 3.5068\n",
            "[Step 8040] Loss: 3.6718\n",
            "[Step 8050] Loss: 3.6348\n",
            "[Step 8060] Loss: 3.7082\n",
            "[Step 8070] Loss: 3.6984\n",
            "[Step 8080] Loss: 3.3912\n",
            "[Step 8090] Loss: 3.5189\n",
            "[Step 8100] Loss: 3.6260\n",
            "[Step 8110] Loss: 3.4380\n",
            "[Step 8120] Loss: 3.3076\n",
            "[Step 8130] Loss: 3.4948\n",
            "[Step 8140] Loss: 3.6494\n",
            "[Step 8150] Loss: 3.7084\n",
            "[Step 8160] Loss: 3.6532\n",
            "[Step 8170] Loss: 3.6197\n",
            "[Step 8180] Loss: 3.5396\n",
            "[Step 8190] Loss: 3.3160\n",
            "[Step 8200] Loss: 3.5339\n",
            "[Step 8210] Loss: 3.6617\n",
            "[Step 8220] Loss: 3.5577\n",
            "[Step 8230] Loss: 3.8590\n",
            "[Step 8240] Loss: 3.6177\n",
            "[Step 8250] Loss: 3.8207\n",
            "[Step 8260] Loss: 3.6799\n",
            "[Step 8270] Loss: 3.7603\n",
            "[Step 8280] Loss: 3.8704\n",
            "[Step 8290] Loss: 3.6064\n",
            "[Step 8300] Loss: 3.7827\n",
            "[Step 8310] Loss: 3.6124\n",
            "[Step 8320] Loss: 3.8075\n",
            "[Step 8330] Loss: 3.5271\n",
            "[Step 8340] Loss: 3.1639\n",
            "[Step 8350] Loss: 3.6886\n",
            "[Step 8360] Loss: 3.7337\n",
            "[Step 8370] Loss: 3.4308\n",
            "[Step 8380] Loss: 3.3936\n",
            "[Step 8390] Loss: 3.6634\n",
            "[Step 8400] Loss: 3.7270\n",
            "[Step 8410] Loss: 3.6249\n",
            "[Step 8420] Loss: 3.6316\n",
            "[Step 8430] Loss: 3.4508\n",
            "[Step 8440] Loss: 3.5650\n",
            "[Step 8450] Loss: 3.6360\n",
            "[Step 8460] Loss: 3.9614\n",
            "[Step 8470] Loss: 3.7567\n",
            "[Step 8480] Loss: 3.3573\n",
            "[Step 8490] Loss: 3.5443\n",
            "[Step 8500] Loss: 3.8782\n",
            "[Step 8510] Loss: 3.6273\n",
            "[Step 8520] Loss: 3.7654\n",
            "[Step 8530] Loss: 3.1528\n",
            "[Step 8540] Loss: 3.6706\n",
            "[Step 8550] Loss: 3.3095\n",
            "[Step 8560] Loss: 3.8241\n",
            "[Step 8570] Loss: 3.7965\n",
            "[Step 8580] Loss: 3.9191\n",
            "[Step 8590] Loss: 3.2866\n",
            "[Step 8600] Loss: 3.7056\n",
            "[Step 8610] Loss: 3.5664\n",
            "[Step 8620] Loss: 3.5472\n",
            "[Step 8630] Loss: 3.7623\n",
            "[Step 8640] Loss: 3.6824\n",
            "[Step 8650] Loss: 3.2768\n",
            "[Step 8660] Loss: 3.7653\n",
            "[Step 8670] Loss: 3.4822\n",
            "[Step 8680] Loss: 3.5369\n",
            "[Step 8690] Loss: 3.3953\n",
            "[Step 8700] Loss: 3.6356\n",
            "[Step 8710] Loss: 3.5905\n",
            "[Step 8720] Loss: 3.7540\n",
            "[Step 8730] Loss: 3.2951\n",
            "[Step 8740] Loss: 3.4085\n",
            "[Step 8750] Loss: 3.5592\n",
            "[Step 8760] Loss: 3.5456\n",
            "[Step 8770] Loss: 3.4192\n",
            "[Step 8780] Loss: 3.4047\n",
            "[Step 8790] Loss: 3.5451\n",
            "[Step 8800] Loss: 3.4260\n",
            "[Step 8810] Loss: 3.5620\n",
            "[Step 8820] Loss: 3.3008\n",
            "[Step 8830] Loss: 3.6756\n",
            "[Step 8840] Loss: 3.2897\n",
            "[Step 8850] Loss: 3.6982\n",
            "[Step 8860] Loss: 3.5667\n",
            "[Step 8870] Loss: 3.5933\n",
            "[Step 8880] Loss: 3.5658\n",
            "[Step 8890] Loss: 3.7638\n",
            "[Step 8900] Loss: 3.4870\n",
            "[Step 8910] Loss: 3.9063\n",
            "[Step 8920] Loss: 3.8340\n",
            "[Step 8930] Loss: 3.3142\n",
            "[Step 8940] Loss: 3.5779\n",
            "[Step 8950] Loss: 3.5691\n",
            "[Step 8960] Loss: 3.4171\n",
            "[Step 8970] Loss: 3.5428\n",
            "[Step 8980] Loss: 3.5926\n",
            "[Step 8990] Loss: 3.9051\n",
            "[Step 9000] Loss: 3.3201\n",
            "[Step 9010] Loss: 3.4028\n",
            "[Step 9020] Loss: 3.4245\n",
            "[Step 9030] Loss: 3.5277\n",
            "[Step 9040] Loss: 3.5391\n",
            "[Step 9050] Loss: 3.5845\n",
            "[Step 9060] Loss: 3.5611\n",
            "[Step 9070] Loss: 3.4774\n",
            "[Step 9080] Loss: 3.2751\n",
            "[Step 9090] Loss: 3.5281\n",
            "[Step 9100] Loss: 3.7253\n",
            "[Step 9110] Loss: 3.5131\n",
            "[Step 9120] Loss: 3.3787\n",
            "[Step 9130] Loss: 3.4279\n",
            "[Step 9140] Loss: 3.2550\n",
            "[Step 9150] Loss: 3.6001\n",
            "[Step 9160] Loss: 3.7439\n",
            "[Step 9170] Loss: 3.5861\n",
            "[Step 9180] Loss: 3.5520\n",
            "[Step 9190] Loss: 3.5379\n",
            "[Step 9200] Loss: 3.0493\n",
            "[Step 9210] Loss: 3.6134\n",
            "[Step 9220] Loss: 3.6213\n",
            "[Step 9230] Loss: 3.3799\n",
            "[Step 9240] Loss: 3.1805\n",
            "[Step 9250] Loss: 3.8940\n",
            "[Step 9260] Loss: 3.5639\n",
            "[Step 9270] Loss: 3.1983\n",
            "[Step 9280] Loss: 3.4502\n",
            "[Step 9290] Loss: 3.7055\n",
            "[Step 9300] Loss: 3.4963\n",
            "[Step 9310] Loss: 3.6904\n",
            "[Step 9320] Loss: 3.3041\n",
            "[Step 9330] Loss: 3.7624\n",
            "[Step 9340] Loss: 3.3811\n",
            "[Step 9350] Loss: 3.7380\n",
            "[Step 9360] Loss: 3.7610\n",
            "[Step 9370] Loss: 3.4145\n",
            "[Step 9380] Loss: 3.8419\n",
            "[Step 9390] Loss: 3.7874\n",
            "[Step 9400] Loss: 3.2818\n",
            "[Step 9410] Loss: 3.2442\n",
            "[Step 9420] Loss: 3.5861\n",
            "[Step 9430] Loss: 3.4404\n",
            "[Step 9440] Loss: 3.5920\n",
            "[Step 9450] Loss: 3.6172\n",
            "[Step 9460] Loss: 3.7895\n",
            "[Step 9470] Loss: 4.0431\n",
            "ðŸ“˜ Epoch 5/20 - Avg Training Loss: 3.5665\n",
            "[Step 9480] Loss: 3.3777\n",
            "[Step 9490] Loss: 3.5476\n",
            "[Step 9500] Loss: 3.5524\n",
            "[Step 9510] Loss: 3.6565\n",
            "[Step 9520] Loss: 3.7193\n",
            "[Step 9530] Loss: 3.5342\n",
            "[Step 9540] Loss: 3.3368\n",
            "[Step 9550] Loss: 3.2091\n",
            "[Step 9560] Loss: 3.5017\n",
            "[Step 9570] Loss: 3.6446\n",
            "[Step 9580] Loss: 3.5630\n",
            "[Step 9590] Loss: 3.3514\n",
            "[Step 9600] Loss: 3.4392\n",
            "[Step 9610] Loss: 3.7665\n",
            "[Step 9620] Loss: 3.0146\n",
            "[Step 9630] Loss: 3.6721\n",
            "[Step 9640] Loss: 3.5562\n",
            "[Step 9650] Loss: 3.7911\n",
            "[Step 9660] Loss: 4.0009\n",
            "[Step 9670] Loss: 3.4781\n",
            "[Step 9680] Loss: 3.4197\n",
            "[Step 9690] Loss: 3.5802\n",
            "[Step 9700] Loss: 3.3497\n",
            "[Step 9710] Loss: 3.4135\n",
            "[Step 9720] Loss: 3.5978\n",
            "[Step 9730] Loss: 3.3757\n",
            "[Step 9740] Loss: 3.5095\n",
            "[Step 9750] Loss: 3.5484\n",
            "[Step 9760] Loss: 3.3231\n",
            "[Step 9770] Loss: 3.5014\n",
            "[Step 9780] Loss: 3.7306\n",
            "[Step 9790] Loss: 3.5357\n",
            "[Step 9800] Loss: 3.5729\n",
            "[Step 9810] Loss: 3.4188\n",
            "[Step 9820] Loss: 3.6045\n",
            "[Step 9830] Loss: 3.6828\n",
            "[Step 9840] Loss: 3.6428\n",
            "[Step 9850] Loss: 3.6593\n",
            "[Step 9860] Loss: 3.3616\n",
            "[Step 9870] Loss: 3.5116\n",
            "[Step 9880] Loss: 3.5224\n",
            "[Step 9890] Loss: 3.4857\n",
            "[Step 9900] Loss: 3.4065\n",
            "[Step 9910] Loss: 3.4636\n",
            "[Step 9920] Loss: 3.4781\n",
            "[Step 9930] Loss: 3.5635\n",
            "[Step 9940] Loss: 3.2991\n",
            "[Step 9950] Loss: 3.8631\n",
            "[Step 9960] Loss: 3.6642\n",
            "[Step 9970] Loss: 3.4227\n",
            "[Step 9980] Loss: 3.4704\n",
            "[Step 9990] Loss: 3.4420\n",
            "[Step 10000] Loss: 3.1946\n",
            "[Step 10010] Loss: 3.5508\n",
            "[Step 10020] Loss: 3.7182\n",
            "[Step 10030] Loss: 3.5575\n",
            "[Step 10040] Loss: 3.7102\n",
            "[Step 10050] Loss: 3.6536\n",
            "[Step 10060] Loss: 3.7729\n",
            "[Step 10070] Loss: 3.4492\n",
            "[Step 10080] Loss: 3.5380\n",
            "[Step 10090] Loss: 3.7410\n",
            "[Step 10100] Loss: 3.4383\n",
            "[Step 10110] Loss: 3.7434\n",
            "[Step 10120] Loss: 3.5259\n",
            "[Step 10130] Loss: 3.7139\n",
            "[Step 10140] Loss: 3.6487\n",
            "[Step 10150] Loss: 3.4465\n",
            "[Step 10160] Loss: 3.4420\n",
            "[Step 10170] Loss: 3.6179\n",
            "[Step 10180] Loss: 3.6834\n",
            "[Step 10190] Loss: 3.4434\n",
            "[Step 10200] Loss: 3.6575\n",
            "[Step 10210] Loss: 3.5565\n",
            "[Step 10220] Loss: 3.5215\n",
            "[Step 10230] Loss: 3.6997\n",
            "[Step 10240] Loss: 3.4449\n",
            "[Step 10250] Loss: 3.4081\n",
            "[Step 10260] Loss: 3.4317\n",
            "[Step 10270] Loss: 3.5253\n",
            "[Step 10280] Loss: 3.6424\n",
            "[Step 10290] Loss: 3.5127\n",
            "[Step 10300] Loss: 3.5939\n",
            "[Step 10310] Loss: 3.5069\n",
            "[Step 10320] Loss: 3.5990\n",
            "[Step 10330] Loss: 3.3051\n",
            "[Step 10340] Loss: 3.4018\n",
            "[Step 10350] Loss: 3.7818\n",
            "[Step 10360] Loss: 3.4442\n",
            "[Step 10370] Loss: 3.4334\n",
            "[Step 10380] Loss: 3.5766\n",
            "[Step 10390] Loss: 3.3773\n",
            "[Step 10400] Loss: 3.6278\n",
            "[Step 10410] Loss: 3.6591\n",
            "[Step 10420] Loss: 3.4755\n",
            "[Step 10430] Loss: 3.4036\n",
            "[Step 10440] Loss: 3.2279\n",
            "[Step 10450] Loss: 3.6884\n",
            "[Step 10460] Loss: 3.1453\n",
            "[Step 10470] Loss: 3.6872\n",
            "[Step 10480] Loss: 3.6772\n",
            "[Step 10490] Loss: 3.5233\n",
            "[Step 10500] Loss: 3.6001\n",
            "[Step 10510] Loss: 3.6298\n",
            "[Step 10520] Loss: 3.5569\n",
            "[Step 10530] Loss: 3.5592\n",
            "[Step 10540] Loss: 3.6371\n",
            "[Step 10550] Loss: 3.5039\n",
            "[Step 10560] Loss: 3.4560\n",
            "[Step 10570] Loss: 3.6841\n",
            "[Step 10580] Loss: 3.7131\n",
            "[Step 10590] Loss: 3.1947\n",
            "[Step 10600] Loss: 3.7160\n",
            "[Step 10610] Loss: 3.4806\n",
            "[Step 10620] Loss: 3.8487\n",
            "[Step 10630] Loss: 3.5062\n",
            "[Step 10640] Loss: 3.6023\n",
            "[Step 10650] Loss: 3.4459\n",
            "[Step 10660] Loss: 3.3310\n",
            "[Step 10670] Loss: 3.7147\n",
            "[Step 10680] Loss: 3.5199\n",
            "[Step 10690] Loss: 3.6389\n",
            "[Step 10700] Loss: 3.4058\n",
            "[Step 10710] Loss: 3.3187\n",
            "[Step 10720] Loss: 3.7518\n",
            "[Step 10730] Loss: 3.7881\n",
            "[Step 10740] Loss: 3.4994\n",
            "[Step 10750] Loss: 3.6053\n",
            "[Step 10760] Loss: 3.7761\n",
            "[Step 10770] Loss: 3.4370\n",
            "[Step 10780] Loss: 3.5986\n",
            "[Step 10790] Loss: 3.7143\n",
            "[Step 10800] Loss: 3.5657\n",
            "[Step 10810] Loss: 3.5347\n",
            "[Step 10820] Loss: 3.4543\n",
            "[Step 10830] Loss: 3.4925\n",
            "[Step 10840] Loss: 3.5791\n",
            "[Step 10850] Loss: 3.7045\n",
            "[Step 10860] Loss: 3.3373\n",
            "[Step 10870] Loss: 3.5579\n",
            "[Step 10880] Loss: 3.4706\n",
            "[Step 10890] Loss: 3.4912\n",
            "[Step 10900] Loss: 3.3624\n",
            "[Step 10910] Loss: 3.4284\n",
            "[Step 10920] Loss: 3.7705\n",
            "[Step 10930] Loss: 3.8598\n",
            "[Step 10940] Loss: 3.2912\n",
            "[Step 10950] Loss: 3.6258\n",
            "[Step 10960] Loss: 3.4877\n",
            "[Step 10970] Loss: 3.6586\n",
            "[Step 10980] Loss: 3.7666\n",
            "[Step 10990] Loss: 3.8761\n",
            "[Step 11000] Loss: 3.5197\n",
            "[Step 11010] Loss: 3.8963\n",
            "[Step 11020] Loss: 3.8371\n",
            "[Step 11030] Loss: 3.6517\n",
            "[Step 11040] Loss: 3.5383\n",
            "[Step 11050] Loss: 3.4210\n",
            "[Step 11060] Loss: 3.3659\n",
            "[Step 11070] Loss: 3.3102\n",
            "[Step 11080] Loss: 3.6468\n",
            "[Step 11090] Loss: 3.3592\n",
            "[Step 11100] Loss: 3.5502\n",
            "[Step 11110] Loss: 3.4217\n",
            "[Step 11120] Loss: 3.5515\n",
            "[Step 11130] Loss: 3.8032\n",
            "[Step 11140] Loss: 3.4904\n",
            "[Step 11150] Loss: 3.6759\n",
            "[Step 11160] Loss: 3.9756\n",
            "[Step 11170] Loss: 3.6332\n",
            "[Step 11180] Loss: 3.5076\n",
            "[Step 11190] Loss: 3.4590\n",
            "[Step 11200] Loss: 3.3905\n",
            "[Step 11210] Loss: 3.5114\n",
            "[Step 11220] Loss: 3.6842\n",
            "[Step 11230] Loss: 3.5393\n",
            "[Step 11240] Loss: 3.6047\n",
            "[Step 11250] Loss: 3.5767\n",
            "[Step 11260] Loss: 3.5790\n",
            "[Step 11270] Loss: 3.4535\n",
            "[Step 11280] Loss: 3.7989\n",
            "[Step 11290] Loss: 3.4608\n",
            "[Step 11300] Loss: 3.6315\n",
            "[Step 11310] Loss: 3.9453\n",
            "[Step 11320] Loss: 3.7569\n",
            "[Step 11330] Loss: 3.7217\n",
            "[Step 11340] Loss: 3.7842\n",
            "[Step 11350] Loss: 3.8440\n",
            "[Step 11360] Loss: 3.4746\n",
            "ðŸ“˜ Epoch 6/20 - Avg Training Loss: 3.5517\n",
            "[Step 11370] Loss: 3.4588\n",
            "[Step 11380] Loss: 3.5332\n",
            "[Step 11390] Loss: 3.6824\n",
            "[Step 11400] Loss: 3.7950\n",
            "[Step 11410] Loss: 3.8515\n",
            "[Step 11420] Loss: 3.6722\n",
            "[Step 11430] Loss: 3.2645\n",
            "[Step 11440] Loss: 3.6444\n",
            "[Step 11450] Loss: 3.8416\n",
            "[Step 11460] Loss: 3.5663\n",
            "[Step 11470] Loss: 3.5282\n",
            "[Step 11480] Loss: 3.5170\n",
            "[Step 11490] Loss: 3.1220\n",
            "[Step 11500] Loss: 3.7531\n",
            "[Step 11510] Loss: 3.5649\n",
            "[Step 11520] Loss: 3.6440\n",
            "[Step 11530] Loss: 3.5069\n",
            "[Step 11540] Loss: 3.3503\n",
            "[Step 11550] Loss: 3.3047\n",
            "[Step 11560] Loss: 3.9524\n",
            "[Step 11570] Loss: 3.6908\n",
            "[Step 11580] Loss: 3.2539\n",
            "[Step 11590] Loss: 3.6533\n",
            "[Step 11600] Loss: 3.5590\n",
            "[Step 11610] Loss: 3.6947\n",
            "[Step 11620] Loss: 3.4698\n",
            "[Step 11630] Loss: 3.8438\n",
            "[Step 11640] Loss: 3.3126\n",
            "[Step 11650] Loss: 3.4841\n",
            "[Step 11660] Loss: 3.3600\n",
            "[Step 11670] Loss: 3.5178\n",
            "[Step 11680] Loss: 3.4161\n",
            "[Step 11690] Loss: 3.6355\n",
            "[Step 11700] Loss: 3.6304\n",
            "[Step 11710] Loss: 3.6315\n",
            "[Step 11720] Loss: 3.5462\n",
            "[Step 11730] Loss: 3.4219\n",
            "[Step 11740] Loss: 3.5408\n",
            "[Step 11750] Loss: 3.7973\n",
            "[Step 11760] Loss: 3.3550\n",
            "[Step 11770] Loss: 3.5810\n",
            "[Step 11780] Loss: 3.3649\n",
            "[Step 11790] Loss: 3.5242\n",
            "[Step 11800] Loss: 3.1385\n",
            "[Step 11810] Loss: 3.2793\n",
            "[Step 11820] Loss: 3.5786\n",
            "[Step 11830] Loss: 3.8381\n",
            "[Step 11840] Loss: 3.8810\n",
            "[Step 11850] Loss: 3.5093\n",
            "[Step 11860] Loss: 3.4555\n",
            "[Step 11870] Loss: 3.3729\n",
            "[Step 11880] Loss: 3.5233\n",
            "[Step 11890] Loss: 3.9000\n",
            "[Step 11900] Loss: 3.5301\n",
            "[Step 11910] Loss: 3.7285\n",
            "[Step 11920] Loss: 3.6297\n",
            "[Step 11930] Loss: 3.3536\n",
            "[Step 11940] Loss: 3.5907\n",
            "[Step 11950] Loss: 4.0010\n",
            "[Step 11960] Loss: 3.5020\n",
            "[Step 11970] Loss: 3.2659\n",
            "[Step 11980] Loss: 3.4493\n",
            "[Step 11990] Loss: 3.5668\n",
            "[Step 12000] Loss: 3.8753\n",
            "[Step 12010] Loss: 3.6201\n",
            "[Step 12020] Loss: 3.5304\n",
            "[Step 12030] Loss: 3.5580\n",
            "[Step 12040] Loss: 3.4976\n",
            "[Step 12050] Loss: 3.6142\n",
            "[Step 12060] Loss: 3.7200\n",
            "[Step 12070] Loss: 3.5653\n",
            "[Step 12080] Loss: 3.8646\n",
            "[Step 12090] Loss: 3.7779\n",
            "[Step 12100] Loss: 3.4990\n",
            "[Step 12110] Loss: 3.6212\n",
            "[Step 12120] Loss: 3.8958\n",
            "[Step 12130] Loss: 3.5028\n",
            "[Step 12140] Loss: 3.6479\n",
            "[Step 12150] Loss: 3.2746\n",
            "[Step 12160] Loss: 3.3426\n",
            "[Step 12170] Loss: 3.5689\n",
            "[Step 12180] Loss: 3.4779\n",
            "[Step 12190] Loss: 3.4440\n",
            "[Step 12200] Loss: 3.7201\n",
            "[Step 12210] Loss: 3.7715\n",
            "[Step 12220] Loss: 3.5947\n",
            "[Step 12230] Loss: 3.3811\n",
            "[Step 12240] Loss: 3.5295\n",
            "[Step 12250] Loss: 3.5264\n",
            "[Step 12260] Loss: 3.6209\n",
            "[Step 12270] Loss: 3.5217\n",
            "[Step 12280] Loss: 3.3317\n",
            "[Step 12290] Loss: 3.7969\n",
            "[Step 12300] Loss: 3.7266\n",
            "[Step 12310] Loss: 3.4657\n",
            "[Step 12320] Loss: 3.3654\n",
            "[Step 12330] Loss: 3.4167\n",
            "[Step 12340] Loss: 3.4443\n",
            "[Step 12350] Loss: 3.4611\n",
            "[Step 12360] Loss: 3.5114\n",
            "[Step 12370] Loss: 3.1744\n",
            "[Step 12380] Loss: 3.3816\n",
            "[Step 12390] Loss: 3.1863\n",
            "[Step 12400] Loss: 3.7244\n",
            "[Step 12410] Loss: 3.7111\n",
            "[Step 12420] Loss: 3.6219\n",
            "[Step 12430] Loss: 3.4840\n",
            "[Step 12440] Loss: 3.7250\n",
            "[Step 12450] Loss: 3.6054\n",
            "[Step 12460] Loss: 3.6699\n",
            "[Step 12470] Loss: 3.6702\n",
            "[Step 12480] Loss: 3.7224\n",
            "[Step 12490] Loss: 3.4555\n",
            "[Step 12500] Loss: 3.5883\n",
            "[Step 12510] Loss: 3.3963\n",
            "[Step 12520] Loss: 3.4321\n",
            "[Step 12530] Loss: 3.6768\n",
            "[Step 12540] Loss: 3.8378\n",
            "[Step 12550] Loss: 3.6918\n",
            "[Step 12560] Loss: 3.5362\n",
            "[Step 12570] Loss: 3.4485\n",
            "[Step 12580] Loss: 3.3780\n",
            "[Step 12590] Loss: 3.3442\n",
            "[Step 12600] Loss: 3.2445\n",
            "[Step 12610] Loss: 3.6544\n",
            "[Step 12620] Loss: 3.5640\n",
            "[Step 12630] Loss: 3.7325\n",
            "[Step 12640] Loss: 3.2995\n",
            "[Step 12650] Loss: 3.6745\n",
            "[Step 12660] Loss: 3.6132\n",
            "[Step 12670] Loss: 3.6103\n",
            "[Step 12680] Loss: 3.6494\n",
            "[Step 12690] Loss: 3.6323\n",
            "[Step 12700] Loss: 3.4218\n",
            "[Step 12710] Loss: 3.6015\n",
            "[Step 12720] Loss: 3.5172\n",
            "[Step 12730] Loss: 3.3872\n",
            "[Step 12740] Loss: 3.5767\n",
            "[Step 12750] Loss: 3.6377\n",
            "[Step 12760] Loss: 3.3766\n",
            "[Step 12770] Loss: 3.3422\n",
            "[Step 12780] Loss: 3.6579\n",
            "[Step 12790] Loss: 3.4923\n",
            "[Step 12800] Loss: 3.7974\n",
            "[Step 12810] Loss: 3.4953\n",
            "[Step 12820] Loss: 3.6020\n",
            "[Step 12830] Loss: 3.8115\n",
            "[Step 12840] Loss: 3.7443\n",
            "[Step 12850] Loss: 3.4056\n",
            "[Step 12860] Loss: 3.2858\n",
            "[Step 12870] Loss: 3.6522\n",
            "[Step 12880] Loss: 3.6706\n",
            "[Step 12890] Loss: 3.4245\n",
            "[Step 12900] Loss: 3.6215\n",
            "[Step 12910] Loss: 3.6429\n",
            "[Step 12920] Loss: 3.4174\n",
            "[Step 12930] Loss: 3.6822\n",
            "[Step 12940] Loss: 3.4574\n",
            "[Step 12950] Loss: 3.2934\n",
            "[Step 12960] Loss: 3.2910\n",
            "[Step 12970] Loss: 3.2889\n",
            "[Step 12980] Loss: 3.2441\n",
            "[Step 12990] Loss: 3.5765\n",
            "[Step 13000] Loss: 3.7382\n",
            "[Step 13010] Loss: 3.2300\n",
            "[Step 13020] Loss: 3.7543\n",
            "[Step 13030] Loss: 3.8606\n",
            "[Step 13040] Loss: 3.4134\n",
            "[Step 13050] Loss: 3.7625\n",
            "[Step 13060] Loss: 3.4629\n",
            "[Step 13070] Loss: 3.3815\n",
            "[Step 13080] Loss: 3.3244\n",
            "[Step 13090] Loss: 3.5135\n",
            "[Step 13100] Loss: 3.4896\n",
            "[Step 13110] Loss: 3.5898\n",
            "[Step 13120] Loss: 3.6341\n",
            "[Step 13130] Loss: 3.6712\n",
            "[Step 13140] Loss: 3.1304\n",
            "[Step 13150] Loss: 3.5908\n",
            "[Step 13160] Loss: 3.2889\n",
            "[Step 13170] Loss: 3.4329\n",
            "[Step 13180] Loss: 3.1866\n",
            "[Step 13190] Loss: 3.2306\n",
            "[Step 13200] Loss: 3.8686\n",
            "[Step 13210] Loss: 3.1449\n",
            "[Step 13220] Loss: 3.7249\n",
            "[Step 13230] Loss: 3.6308\n",
            "[Step 13240] Loss: 3.1084\n",
            "[Step 13250] Loss: 3.2804\n",
            "ðŸ“˜ Epoch 7/20 - Avg Training Loss: 3.5433\n",
            "[Step 13260] Loss: 3.7708\n",
            "[Step 13270] Loss: 3.5947\n",
            "[Step 13280] Loss: 3.5060\n",
            "[Step 13290] Loss: 3.6212\n",
            "[Step 13300] Loss: 3.3101\n",
            "[Step 13310] Loss: 3.7880\n",
            "[Step 13320] Loss: 3.4592\n",
            "[Step 13330] Loss: 3.4112\n",
            "[Step 13340] Loss: 3.8007\n",
            "[Step 13350] Loss: 3.5148\n",
            "[Step 13360] Loss: 3.4131\n",
            "[Step 13370] Loss: 3.5559\n",
            "[Step 13380] Loss: 3.6876\n",
            "[Step 13390] Loss: 3.3221\n",
            "[Step 13400] Loss: 3.7432\n",
            "[Step 13410] Loss: 3.5088\n",
            "[Step 13420] Loss: 3.7327\n",
            "[Step 13430] Loss: 3.7638\n",
            "[Step 13440] Loss: 3.5865\n",
            "[Step 13450] Loss: 3.4230\n",
            "[Step 13460] Loss: 3.5752\n",
            "[Step 13470] Loss: 3.5562\n",
            "[Step 13480] Loss: 3.7246\n",
            "[Step 13490] Loss: 3.6469\n",
            "[Step 13500] Loss: 3.3009\n",
            "[Step 13510] Loss: 3.7212\n",
            "[Step 13520] Loss: 3.3434\n",
            "[Step 13530] Loss: 3.8654\n",
            "[Step 13540] Loss: 3.3694\n",
            "[Step 13550] Loss: 3.6934\n",
            "[Step 13560] Loss: 3.5854\n",
            "[Step 13570] Loss: 3.4142\n",
            "[Step 13580] Loss: 3.5258\n",
            "[Step 13590] Loss: 3.4711\n",
            "[Step 13600] Loss: 3.3129\n",
            "[Step 13610] Loss: 3.6718\n",
            "[Step 13620] Loss: 3.7138\n",
            "[Step 13630] Loss: 3.2065\n",
            "[Step 13640] Loss: 3.4249\n",
            "[Step 13650] Loss: 3.4350\n",
            "[Step 13660] Loss: 3.8132\n",
            "[Step 13670] Loss: 3.5264\n",
            "[Step 13680] Loss: 3.5542\n",
            "[Step 13690] Loss: 3.3878\n",
            "[Step 13700] Loss: 3.8606\n",
            "[Step 13710] Loss: 3.8845\n",
            "[Step 13720] Loss: 3.5575\n",
            "[Step 13730] Loss: 3.6243\n",
            "[Step 13740] Loss: 3.6617\n",
            "[Step 13750] Loss: 3.4657\n",
            "[Step 13760] Loss: 3.5706\n",
            "[Step 13770] Loss: 3.4782\n",
            "[Step 13780] Loss: 3.7302\n",
            "[Step 13790] Loss: 3.5024\n",
            "[Step 13800] Loss: 3.6509\n",
            "[Step 13810] Loss: 3.3834\n",
            "[Step 13820] Loss: 3.1928\n",
            "[Step 13830] Loss: 3.3012\n",
            "[Step 13840] Loss: 3.1494\n",
            "[Step 13850] Loss: 3.5501\n",
            "[Step 13860] Loss: 3.4952\n",
            "[Step 13870] Loss: 3.4474\n",
            "[Step 13880] Loss: 3.8395\n",
            "[Step 13890] Loss: 3.7309\n",
            "[Step 13900] Loss: 3.6246\n",
            "[Step 13910] Loss: 3.3924\n",
            "[Step 13920] Loss: 3.3745\n",
            "[Step 13930] Loss: 3.1783\n",
            "[Step 13940] Loss: 3.3435\n",
            "[Step 13950] Loss: 3.6947\n",
            "[Step 13960] Loss: 3.5434\n",
            "[Step 13970] Loss: 3.6587\n",
            "[Step 13980] Loss: 3.5818\n",
            "[Step 13990] Loss: 3.2840\n",
            "[Step 14000] Loss: 3.3115\n",
            "[Step 14010] Loss: 3.7696\n",
            "[Step 14020] Loss: 3.5764\n",
            "[Step 14030] Loss: 3.5718\n",
            "[Step 14040] Loss: 3.6056\n",
            "[Step 14050] Loss: 3.2557\n",
            "[Step 14060] Loss: 3.5402\n",
            "[Step 14070] Loss: 3.9347\n",
            "[Step 14080] Loss: 3.4281\n",
            "[Step 14090] Loss: 3.5501\n",
            "[Step 14100] Loss: 3.5208\n",
            "[Step 14110] Loss: 3.5941\n",
            "[Step 14120] Loss: 3.5887\n",
            "[Step 14130] Loss: 3.4528\n",
            "[Step 14140] Loss: 3.3276\n",
            "[Step 14150] Loss: 3.8066\n",
            "[Step 14160] Loss: 3.3938\n",
            "[Step 14170] Loss: 3.7199\n",
            "[Step 14180] Loss: 3.3143\n",
            "[Step 14190] Loss: 3.5667\n",
            "[Step 14200] Loss: 3.3824\n",
            "[Step 14210] Loss: 3.3639\n",
            "[Step 14220] Loss: 3.4980\n",
            "[Step 14230] Loss: 3.3803\n",
            "[Step 14240] Loss: 3.5753\n",
            "[Step 14250] Loss: 3.5169\n",
            "[Step 14260] Loss: 3.6414\n",
            "[Step 14270] Loss: 3.4950\n",
            "[Step 14280] Loss: 3.3282\n",
            "[Step 14290] Loss: 3.4388\n",
            "[Step 14300] Loss: 3.6317\n",
            "[Step 14310] Loss: 3.5575\n",
            "[Step 14320] Loss: 3.5498\n",
            "[Step 14330] Loss: 3.3455\n",
            "[Step 14340] Loss: 3.5973\n",
            "[Step 14350] Loss: 3.6471\n",
            "[Step 14360] Loss: 3.5143\n",
            "[Step 14370] Loss: 3.6581\n",
            "[Step 14380] Loss: 3.5262\n",
            "[Step 14390] Loss: 3.1743\n",
            "[Step 14400] Loss: 3.6070\n",
            "[Step 14410] Loss: 3.3622\n",
            "[Step 14420] Loss: 3.7245\n",
            "[Step 14430] Loss: 3.3943\n",
            "[Step 14440] Loss: 3.3315\n",
            "[Step 14450] Loss: 3.4228\n",
            "[Step 14460] Loss: 3.4600\n",
            "[Step 14470] Loss: 3.2738\n",
            "[Step 14480] Loss: 3.5880\n",
            "[Step 14490] Loss: 3.4642\n",
            "[Step 14500] Loss: 3.6205\n",
            "[Step 14510] Loss: 3.4326\n",
            "[Step 14520] Loss: 3.5915\n",
            "[Step 14530] Loss: 3.5879\n",
            "[Step 14540] Loss: 3.3686\n",
            "[Step 14550] Loss: 3.1893\n",
            "[Step 14560] Loss: 3.3446\n",
            "[Step 14570] Loss: 3.3525\n",
            "[Step 14580] Loss: 3.4764\n",
            "[Step 14590] Loss: 3.6414\n",
            "[Step 14600] Loss: 3.3162\n",
            "[Step 14610] Loss: 3.7033\n",
            "[Step 14620] Loss: 3.2707\n",
            "[Step 14630] Loss: 3.3049\n",
            "[Step 14640] Loss: 3.2738\n",
            "[Step 14650] Loss: 3.4547\n",
            "[Step 14660] Loss: 3.4471\n",
            "[Step 14670] Loss: 3.6973\n",
            "[Step 14680] Loss: 3.3740\n",
            "[Step 14690] Loss: 3.9692\n",
            "[Step 14700] Loss: 3.2965\n",
            "[Step 14710] Loss: 3.8478\n",
            "[Step 14720] Loss: 3.3829\n",
            "[Step 14730] Loss: 3.3823\n",
            "[Step 14740] Loss: 3.2771\n",
            "[Step 14750] Loss: 3.5539\n",
            "[Step 14760] Loss: 3.7909\n",
            "[Step 14770] Loss: 3.5367\n",
            "[Step 14780] Loss: 3.5927\n",
            "[Step 14790] Loss: 3.6297\n",
            "[Step 14800] Loss: 3.6503\n",
            "[Step 14810] Loss: 3.1959\n",
            "[Step 14820] Loss: 3.4380\n",
            "[Step 14830] Loss: 3.5128\n",
            "[Step 14840] Loss: 3.4942\n",
            "[Step 14850] Loss: 3.7121\n",
            "[Step 14860] Loss: 3.4019\n",
            "[Step 14870] Loss: 3.6666\n",
            "[Step 14880] Loss: 3.4726\n",
            "[Step 14890] Loss: 3.0902\n",
            "[Step 14900] Loss: 3.4299\n",
            "[Step 14910] Loss: 3.3048\n",
            "[Step 14920] Loss: 3.4916\n",
            "[Step 14930] Loss: 3.5445\n",
            "[Step 14940] Loss: 3.5601\n",
            "[Step 14950] Loss: 3.7289\n",
            "[Step 14960] Loss: 3.8473\n",
            "[Step 14970] Loss: 3.5163\n",
            "[Step 14980] Loss: 3.1293\n",
            "[Step 14990] Loss: 3.5974\n",
            "[Step 15000] Loss: 3.4911\n",
            "[Step 15010] Loss: 3.8692\n",
            "[Step 15020] Loss: 3.3653\n",
            "[Step 15030] Loss: 3.4394\n",
            "[Step 15040] Loss: 3.6192\n",
            "[Step 15050] Loss: 3.8867\n",
            "[Step 15060] Loss: 3.5155\n",
            "[Step 15070] Loss: 3.4612\n",
            "[Step 15080] Loss: 3.3696\n",
            "[Step 15090] Loss: 3.5900\n",
            "[Step 15100] Loss: 3.8828\n",
            "[Step 15110] Loss: 3.9514\n",
            "[Step 15120] Loss: 4.0470\n",
            "[Step 15130] Loss: 3.3641\n",
            "[Step 15140] Loss: 3.5751\n",
            "[Step 15150] Loss: 3.6417\n",
            "ðŸ“˜ Epoch 8/20 - Avg Training Loss: 3.5294\n",
            "[Step 15160] Loss: 3.9408\n",
            "[Step 15170] Loss: 3.4308\n",
            "[Step 15180] Loss: 3.4847\n",
            "[Step 15190] Loss: 3.2915\n",
            "[Step 15200] Loss: 3.4669\n",
            "[Step 15210] Loss: 3.2326\n",
            "[Step 15220] Loss: 3.4785\n",
            "[Step 15230] Loss: 3.4211\n",
            "[Step 15240] Loss: 3.6176\n",
            "[Step 15250] Loss: 3.3475\n",
            "[Step 15260] Loss: 3.5294\n",
            "[Step 15270] Loss: 3.7930\n",
            "[Step 15280] Loss: 3.3340\n",
            "[Step 15290] Loss: 3.5464\n",
            "[Step 15300] Loss: 3.5315\n",
            "[Step 15310] Loss: 3.6596\n",
            "[Step 15320] Loss: 3.5033\n",
            "[Step 15330] Loss: 3.2584\n",
            "[Step 15340] Loss: 3.8382\n",
            "[Step 15350] Loss: 3.8053\n",
            "[Step 15360] Loss: 3.5598\n",
            "[Step 15370] Loss: 3.3918\n",
            "[Step 15380] Loss: 3.5758\n",
            "[Step 15390] Loss: 3.3353\n",
            "[Step 15400] Loss: 3.5510\n",
            "[Step 15410] Loss: 3.6486\n",
            "[Step 15420] Loss: 3.3344\n",
            "[Step 15430] Loss: 3.4659\n",
            "[Step 15440] Loss: 3.6090\n",
            "[Step 15450] Loss: 3.4822\n",
            "[Step 15460] Loss: 3.3852\n",
            "[Step 15470] Loss: 3.6828\n",
            "[Step 15480] Loss: 3.7291\n",
            "[Step 15490] Loss: 3.4823\n",
            "[Step 15500] Loss: 3.7426\n",
            "[Step 15510] Loss: 3.3772\n",
            "[Step 15520] Loss: 3.6843\n",
            "[Step 15530] Loss: 3.5670\n",
            "[Step 15540] Loss: 3.4306\n",
            "[Step 15550] Loss: 3.4913\n",
            "[Step 15560] Loss: 3.7004\n",
            "[Step 15570] Loss: 3.4261\n",
            "[Step 15580] Loss: 3.5161\n",
            "[Step 15590] Loss: 3.4630\n",
            "[Step 15600] Loss: 3.3601\n",
            "[Step 15610] Loss: 3.6136\n",
            "[Step 15620] Loss: 3.5872\n",
            "[Step 15630] Loss: 3.2520\n",
            "[Step 15640] Loss: 3.3317\n",
            "[Step 15650] Loss: 3.4568\n",
            "[Step 15660] Loss: 3.4982\n",
            "[Step 15670] Loss: 3.4453\n",
            "[Step 15680] Loss: 3.3910\n",
            "[Step 15690] Loss: 3.5454\n",
            "[Step 15700] Loss: 3.5600\n",
            "[Step 15710] Loss: 3.4578\n",
            "[Step 15720] Loss: 3.7305\n",
            "[Step 15730] Loss: 3.6084\n",
            "[Step 15740] Loss: 3.5419\n",
            "[Step 15750] Loss: 3.5292\n",
            "[Step 15760] Loss: 3.3779\n",
            "[Step 15770] Loss: 3.2132\n",
            "[Step 15780] Loss: 3.5072\n",
            "[Step 15790] Loss: 3.4062\n",
            "[Step 15800] Loss: 3.2644\n",
            "[Step 15810] Loss: 3.4790\n",
            "[Step 15820] Loss: 3.4240\n",
            "[Step 15830] Loss: 3.5955\n",
            "[Step 15840] Loss: 3.5131\n",
            "[Step 15850] Loss: 3.6183\n",
            "[Step 15860] Loss: 3.5901\n",
            "[Step 15870] Loss: 3.6201\n",
            "[Step 15880] Loss: 3.7195\n",
            "[Step 15890] Loss: 3.5480\n",
            "[Step 15900] Loss: 3.4641\n",
            "[Step 15910] Loss: 3.5542\n",
            "[Step 15920] Loss: 3.5735\n",
            "[Step 15930] Loss: 3.5998\n",
            "[Step 15940] Loss: 3.3845\n",
            "[Step 15950] Loss: 3.4772\n",
            "[Step 15960] Loss: 3.6718\n",
            "[Step 15970] Loss: 3.5830\n",
            "[Step 15980] Loss: 3.7315\n",
            "[Step 15990] Loss: 3.4805\n",
            "[Step 16000] Loss: 3.7673\n",
            "[Step 16010] Loss: 3.6232\n",
            "[Step 16020] Loss: 3.1934\n",
            "[Step 16030] Loss: 3.4368\n",
            "[Step 16040] Loss: 3.6481\n",
            "[Step 16050] Loss: 3.5935\n",
            "[Step 16060] Loss: 3.3347\n",
            "[Step 16070] Loss: 3.7155\n",
            "[Step 16080] Loss: 3.5259\n",
            "[Step 16090] Loss: 3.3135\n",
            "[Step 16100] Loss: 3.3149\n",
            "[Step 16110] Loss: 3.3767\n",
            "[Step 16120] Loss: 3.4415\n",
            "[Step 16130] Loss: 3.4530\n",
            "[Step 16140] Loss: 3.0452\n",
            "[Step 16150] Loss: 3.5490\n",
            "[Step 16160] Loss: 3.2859\n",
            "[Step 16170] Loss: 3.3867\n",
            "[Step 16180] Loss: 3.5300\n",
            "[Step 16190] Loss: 3.5359\n",
            "[Step 16200] Loss: 3.5376\n",
            "[Step 16210] Loss: 3.3727\n",
            "[Step 16220] Loss: 3.2127\n",
            "[Step 16230] Loss: 3.6975\n",
            "[Step 16240] Loss: 3.7211\n",
            "[Step 16250] Loss: 3.8656\n",
            "[Step 16260] Loss: 3.4989\n",
            "[Step 16270] Loss: 3.7467\n",
            "[Step 16280] Loss: 3.5404\n",
            "[Step 16290] Loss: 3.6720\n",
            "[Step 16300] Loss: 3.5839\n",
            "[Step 16310] Loss: 3.5474\n",
            "[Step 16320] Loss: 3.4018\n",
            "[Step 16330] Loss: 3.3468\n",
            "[Step 16340] Loss: 3.4824\n",
            "[Step 16350] Loss: 3.5122\n",
            "[Step 16360] Loss: 3.5563\n",
            "[Step 16370] Loss: 3.6802\n",
            "[Step 16380] Loss: 3.3390\n",
            "[Step 16390] Loss: 3.7236\n",
            "[Step 16400] Loss: 3.5443\n",
            "[Step 16410] Loss: 3.3831\n",
            "[Step 16420] Loss: 3.4318\n",
            "[Step 16430] Loss: 3.5165\n",
            "[Step 16440] Loss: 3.5134\n",
            "[Step 16450] Loss: 3.6794\n",
            "[Step 16460] Loss: 3.4236\n",
            "[Step 16470] Loss: 3.4437\n",
            "[Step 16480] Loss: 3.3503\n",
            "[Step 16490] Loss: 3.6785\n",
            "[Step 16500] Loss: 3.5458\n",
            "[Step 16510] Loss: 3.4528\n",
            "[Step 16520] Loss: 3.3901\n",
            "[Step 16530] Loss: 3.1835\n",
            "[Step 16540] Loss: 3.6201\n",
            "[Step 16550] Loss: 3.4304\n",
            "[Step 16560] Loss: 3.5468\n",
            "[Step 16570] Loss: 3.3012\n",
            "[Step 16580] Loss: 3.8211\n",
            "[Step 16590] Loss: 3.8357\n",
            "[Step 16600] Loss: 3.3032\n",
            "[Step 16610] Loss: 3.3391\n",
            "[Step 16620] Loss: 3.5647\n",
            "[Step 16630] Loss: 3.7225\n",
            "[Step 16640] Loss: 3.3318\n",
            "[Step 16650] Loss: 3.3213\n",
            "[Step 16660] Loss: 3.2043\n",
            "[Step 16670] Loss: 3.2545\n",
            "[Step 16680] Loss: 3.3545\n",
            "[Step 16690] Loss: 3.4409\n",
            "[Step 16700] Loss: 3.3772\n",
            "[Step 16710] Loss: 3.6392\n",
            "[Step 16720] Loss: 3.5226\n",
            "[Step 16730] Loss: 3.4241\n",
            "[Step 16740] Loss: 3.6425\n",
            "[Step 16750] Loss: 3.2581\n",
            "[Step 16760] Loss: 3.8415\n",
            "[Step 16770] Loss: 3.6785\n",
            "[Step 16780] Loss: 3.5406\n",
            "[Step 16790] Loss: 3.3886\n",
            "[Step 16800] Loss: 3.5074\n",
            "[Step 16810] Loss: 3.6554\n",
            "[Step 16820] Loss: 3.4116\n",
            "[Step 16830] Loss: 3.6113\n",
            "[Step 16840] Loss: 3.3294\n",
            "[Step 16850] Loss: 3.8094\n",
            "[Step 16860] Loss: 3.2727\n",
            "[Step 16870] Loss: 3.7160\n",
            "[Step 16880] Loss: 3.4788\n",
            "[Step 16890] Loss: 3.3485\n",
            "[Step 16900] Loss: 3.5448\n",
            "[Step 16910] Loss: 3.6150\n",
            "[Step 16920] Loss: 3.4645\n",
            "[Step 16930] Loss: 3.4081\n",
            "[Step 16940] Loss: 3.2710\n",
            "[Step 16950] Loss: 3.3320\n",
            "[Step 16960] Loss: 3.5196\n",
            "[Step 16970] Loss: 3.7242\n",
            "[Step 16980] Loss: 3.3480\n",
            "[Step 16990] Loss: 3.3981\n",
            "[Step 17000] Loss: 3.6369\n",
            "[Step 17010] Loss: 3.2407\n",
            "[Step 17020] Loss: 3.4430\n",
            "[Step 17030] Loss: 3.5265\n",
            "[Step 17040] Loss: 3.5826\n",
            "ðŸ“˜ Epoch 9/20 - Avg Training Loss: 3.5181\n",
            "[Step 17050] Loss: 3.5407\n",
            "[Step 17060] Loss: 3.5112\n",
            "[Step 17070] Loss: 3.7127\n",
            "[Step 17080] Loss: 3.7889\n",
            "[Step 17090] Loss: 3.3964\n",
            "[Step 17100] Loss: 3.5067\n",
            "[Step 17110] Loss: 3.3618\n",
            "[Step 17120] Loss: 3.4927\n",
            "[Step 17130] Loss: 3.3694\n",
            "[Step 17140] Loss: 3.4972\n",
            "[Step 17150] Loss: 3.7467\n",
            "[Step 17160] Loss: 3.4553\n",
            "[Step 17170] Loss: 3.4609\n",
            "[Step 17180] Loss: 3.4963\n",
            "[Step 17190] Loss: 3.6529\n",
            "[Step 17200] Loss: 3.4411\n",
            "[Step 17210] Loss: 3.2544\n",
            "[Step 17220] Loss: 3.5878\n",
            "[Step 17230] Loss: 3.2787\n",
            "[Step 17240] Loss: 3.5906\n",
            "[Step 17250] Loss: 3.5379\n",
            "[Step 17260] Loss: 3.6168\n",
            "[Step 17270] Loss: 3.3560\n",
            "[Step 17280] Loss: 3.2575\n",
            "[Step 17290] Loss: 3.4090\n",
            "[Step 17300] Loss: 3.2612\n",
            "[Step 17310] Loss: 3.5941\n",
            "[Step 17320] Loss: 3.7408\n",
            "[Step 17330] Loss: 3.6374\n",
            "[Step 17340] Loss: 3.1800\n",
            "[Step 17350] Loss: 3.5329\n",
            "[Step 17360] Loss: 3.4302\n",
            "[Step 17370] Loss: 3.4613\n",
            "[Step 17380] Loss: 3.5576\n",
            "[Step 17390] Loss: 3.2154\n",
            "[Step 17400] Loss: 3.4186\n",
            "[Step 17410] Loss: 3.2764\n",
            "[Step 17420] Loss: 3.6649\n",
            "[Step 17430] Loss: 3.3684\n",
            "[Step 17440] Loss: 3.4317\n",
            "[Step 17450] Loss: 3.4231\n",
            "[Step 17460] Loss: 3.5641\n",
            "[Step 17470] Loss: 3.8374\n",
            "[Step 17480] Loss: 3.5675\n",
            "[Step 17490] Loss: 3.4935\n",
            "[Step 17500] Loss: 3.3462\n",
            "[Step 17510] Loss: 3.3727\n",
            "[Step 17520] Loss: 3.5104\n",
            "[Step 17530] Loss: 3.3332\n",
            "[Step 17540] Loss: 3.8505\n",
            "[Step 17550] Loss: 3.6174\n",
            "[Step 17560] Loss: 3.1337\n",
            "[Step 17570] Loss: 3.7745\n",
            "[Step 17580] Loss: 3.4344\n",
            "[Step 17590] Loss: 3.5258\n",
            "[Step 17600] Loss: 3.7045\n",
            "[Step 17610] Loss: 3.5497\n",
            "[Step 17620] Loss: 3.8718\n",
            "[Step 17630] Loss: 3.2784\n",
            "[Step 17640] Loss: 3.4902\n",
            "[Step 17650] Loss: 3.2857\n",
            "[Step 17660] Loss: 3.4347\n",
            "[Step 17670] Loss: 3.5002\n",
            "[Step 17680] Loss: 3.3216\n",
            "[Step 17690] Loss: 3.4997\n",
            "[Step 17700] Loss: 3.2079\n",
            "[Step 17710] Loss: 3.5623\n",
            "[Step 17720] Loss: 3.6203\n",
            "[Step 17730] Loss: 3.4065\n",
            "[Step 17740] Loss: 3.4957\n",
            "[Step 17750] Loss: 3.5939\n",
            "[Step 17760] Loss: 3.4723\n",
            "[Step 17770] Loss: 3.2500\n",
            "[Step 17780] Loss: 3.4072\n",
            "[Step 17790] Loss: 3.5422\n",
            "[Step 17800] Loss: 3.8277\n",
            "[Step 17810] Loss: 3.2585\n",
            "[Step 17820] Loss: 3.5600\n",
            "[Step 17830] Loss: 3.6888\n",
            "[Step 17840] Loss: 3.3542\n",
            "[Step 17850] Loss: 3.3136\n",
            "[Step 17860] Loss: 3.5660\n",
            "[Step 17870] Loss: 3.6025\n",
            "[Step 17880] Loss: 3.0263\n",
            "[Step 17890] Loss: 3.7153\n",
            "[Step 17900] Loss: 3.3308\n",
            "[Step 17910] Loss: 3.5212\n",
            "[Step 17920] Loss: 3.5956\n",
            "[Step 17930] Loss: 3.6271\n",
            "[Step 17940] Loss: 3.7642\n",
            "[Step 17950] Loss: 3.1219\n",
            "[Step 17960] Loss: 3.6307\n",
            "[Step 17970] Loss: 3.6452\n",
            "[Step 17980] Loss: 3.5023\n",
            "[Step 17990] Loss: 3.4444\n",
            "[Step 18000] Loss: 3.6678\n",
            "[Step 18010] Loss: 3.3264\n",
            "[Step 18020] Loss: 3.5296\n",
            "[Step 18030] Loss: 3.5847\n",
            "[Step 18040] Loss: 3.1346\n",
            "[Step 18050] Loss: 3.6623\n",
            "[Step 18060] Loss: 3.2932\n",
            "[Step 18070] Loss: 3.4154\n",
            "[Step 18080] Loss: 3.4090\n",
            "[Step 18090] Loss: 3.3068\n",
            "[Step 18100] Loss: 3.5438\n",
            "[Step 18110] Loss: 3.7160\n",
            "[Step 18120] Loss: 3.6570\n",
            "[Step 18130] Loss: 3.2482\n",
            "[Step 18140] Loss: 3.7374\n",
            "[Step 18150] Loss: 3.5062\n",
            "[Step 18160] Loss: 3.6713\n",
            "[Step 18170] Loss: 3.6022\n",
            "[Step 18180] Loss: 3.4958\n",
            "[Step 18190] Loss: 3.2759\n",
            "[Step 18200] Loss: 3.4425\n",
            "[Step 18210] Loss: 3.5330\n",
            "[Step 18220] Loss: 3.7945\n",
            "[Step 18230] Loss: 3.4412\n",
            "[Step 18240] Loss: 3.6464\n",
            "[Step 18250] Loss: 3.7572\n",
            "[Step 18260] Loss: 3.6080\n",
            "[Step 18270] Loss: 3.6873\n",
            "[Step 18280] Loss: 3.2517\n",
            "[Step 18290] Loss: 3.3886\n",
            "[Step 18300] Loss: 3.3326\n",
            "[Step 18310] Loss: 3.7594\n",
            "[Step 18320] Loss: 3.4196\n",
            "[Step 18330] Loss: 3.4973\n",
            "[Step 18340] Loss: 3.5014\n",
            "[Step 18350] Loss: 3.3497\n",
            "[Step 18360] Loss: 3.4632\n",
            "[Step 18370] Loss: 3.3050\n",
            "[Step 18380] Loss: 3.1361\n",
            "[Step 18390] Loss: 3.3568\n",
            "[Step 18400] Loss: 3.5069\n",
            "[Step 18410] Loss: 3.8448\n",
            "[Step 18420] Loss: 3.5707\n",
            "[Step 18430] Loss: 3.8757\n",
            "[Step 18440] Loss: 3.3222\n",
            "[Step 18450] Loss: 3.3037\n",
            "[Step 18460] Loss: 3.5849\n",
            "[Step 18470] Loss: 3.6219\n",
            "[Step 18480] Loss: 3.1732\n",
            "[Step 18490] Loss: 3.4898\n",
            "[Step 18500] Loss: 3.6363\n",
            "[Step 18510] Loss: 3.7077\n",
            "[Step 18520] Loss: 3.8955\n",
            "[Step 18530] Loss: 3.9430\n",
            "[Step 18540] Loss: 3.5622\n",
            "[Step 18550] Loss: 3.4084\n",
            "[Step 18560] Loss: 3.3170\n",
            "[Step 18570] Loss: 3.4102\n",
            "[Step 18580] Loss: 3.5461\n",
            "[Step 18590] Loss: 3.2123\n",
            "[Step 18600] Loss: 3.2578\n",
            "[Step 18610] Loss: 3.5613\n",
            "[Step 18620] Loss: 3.4644\n",
            "[Step 18630] Loss: 3.6300\n",
            "[Step 18640] Loss: 3.9053\n",
            "[Step 18650] Loss: 3.3200\n",
            "[Step 18660] Loss: 3.5099\n",
            "[Step 18670] Loss: 3.3383\n",
            "[Step 18680] Loss: 3.4625\n",
            "[Step 18690] Loss: 3.3570\n",
            "[Step 18700] Loss: 3.4832\n",
            "[Step 18710] Loss: 3.5884\n",
            "[Step 18720] Loss: 3.3549\n",
            "[Step 18730] Loss: 3.3952\n",
            "[Step 18740] Loss: 3.3982\n",
            "[Step 18750] Loss: 3.3454\n",
            "[Step 18760] Loss: 3.3197\n",
            "[Step 18770] Loss: 3.7735\n",
            "[Step 18780] Loss: 3.4314\n",
            "[Step 18790] Loss: 3.4219\n",
            "[Step 18800] Loss: 3.3332\n",
            "[Step 18810] Loss: 3.5866\n",
            "[Step 18820] Loss: 3.4183\n",
            "[Step 18830] Loss: 3.4224\n",
            "[Step 18840] Loss: 3.7608\n",
            "[Step 18850] Loss: 3.6534\n",
            "[Step 18860] Loss: 3.5307\n",
            "[Step 18870] Loss: 3.4960\n",
            "[Step 18880] Loss: 3.4502\n",
            "[Step 18890] Loss: 3.4925\n",
            "[Step 18900] Loss: 3.5932\n",
            "[Step 18910] Loss: 3.3260\n",
            "[Step 18920] Loss: 3.6258\n",
            "[Step 18930] Loss: 3.8041\n",
            "[Step 18940] Loss: 3.7481\n",
            "ðŸ“˜ Epoch 10/20 - Avg Training Loss: 3.5101\n",
            "[Step 18950] Loss: 3.4990\n",
            "[Step 18960] Loss: 3.3958\n",
            "[Step 18970] Loss: 3.7846\n",
            "[Step 18980] Loss: 3.5836\n",
            "[Step 18990] Loss: 3.3877\n",
            "[Step 19000] Loss: 3.6252\n",
            "[Step 19010] Loss: 3.1240\n",
            "[Step 19020] Loss: 3.4282\n",
            "[Step 19030] Loss: 3.4878\n",
            "[Step 19040] Loss: 3.7658\n",
            "[Step 19050] Loss: 3.4157\n",
            "[Step 19060] Loss: 3.6246\n",
            "[Step 19070] Loss: 3.4458\n",
            "[Step 19080] Loss: 3.2771\n",
            "[Step 19090] Loss: 3.6332\n",
            "[Step 19100] Loss: 3.4231\n",
            "[Step 19110] Loss: 3.4530\n",
            "[Step 19120] Loss: 3.5270\n",
            "[Step 19130] Loss: 3.7014\n",
            "[Step 19140] Loss: 3.4209\n",
            "[Step 19150] Loss: 3.3769\n",
            "[Step 19160] Loss: 3.5479\n",
            "[Step 19170] Loss: 3.5800\n",
            "[Step 19180] Loss: 3.2645\n",
            "[Step 19190] Loss: 3.3872\n",
            "[Step 19200] Loss: 3.7462\n",
            "[Step 19210] Loss: 3.3489\n",
            "[Step 19220] Loss: 3.4971\n",
            "[Step 19230] Loss: 3.2042\n",
            "[Step 19240] Loss: 3.5792\n",
            "[Step 19250] Loss: 3.5371\n",
            "[Step 19260] Loss: 3.9870\n",
            "[Step 19270] Loss: 3.6208\n",
            "[Step 19280] Loss: 3.6067\n",
            "[Step 19290] Loss: 3.4115\n",
            "[Step 19300] Loss: 3.3607\n",
            "[Step 19310] Loss: 3.7986\n",
            "[Step 19320] Loss: 3.5662\n",
            "[Step 19330] Loss: 3.3097\n",
            "[Step 19340] Loss: 3.0994\n",
            "[Step 19350] Loss: 3.2395\n",
            "[Step 19360] Loss: 3.4984\n",
            "[Step 19370] Loss: 3.9190\n",
            "[Step 19380] Loss: 3.6481\n",
            "[Step 19390] Loss: 3.3266\n",
            "[Step 19400] Loss: 3.4936\n",
            "[Step 19410] Loss: 3.5639\n",
            "[Step 19420] Loss: 3.8552\n",
            "[Step 19430] Loss: 3.3943\n",
            "[Step 19440] Loss: 3.7682\n",
            "[Step 19450] Loss: 3.4221\n",
            "[Step 19460] Loss: 3.6760\n",
            "[Step 19470] Loss: 3.8109\n",
            "[Step 19480] Loss: 3.2361\n",
            "[Step 19490] Loss: 3.5188\n",
            "[Step 19500] Loss: 3.6401\n",
            "[Step 19510] Loss: 3.3121\n",
            "[Step 19520] Loss: 3.2834\n",
            "[Step 19530] Loss: 3.7442\n",
            "[Step 19540] Loss: 3.5154\n",
            "[Step 19550] Loss: 3.4659\n",
            "[Step 19560] Loss: 3.3325\n",
            "[Step 19570] Loss: 3.7072\n",
            "[Step 19580] Loss: 3.3894\n",
            "[Step 19590] Loss: 3.2724\n",
            "[Step 19600] Loss: 3.5578\n",
            "[Step 19610] Loss: 3.3024\n",
            "[Step 19620] Loss: 3.2888\n",
            "[Step 19630] Loss: 3.5784\n",
            "[Step 19640] Loss: 3.9136\n",
            "[Step 19650] Loss: 3.4847\n",
            "[Step 19660] Loss: 3.2126\n",
            "[Step 19670] Loss: 3.1874\n",
            "[Step 19680] Loss: 3.7105\n",
            "[Step 19690] Loss: 3.3545\n",
            "[Step 19700] Loss: 3.4875\n",
            "[Step 19710] Loss: 3.8229\n",
            "[Step 19720] Loss: 3.4151\n",
            "[Step 19730] Loss: 3.5203\n",
            "[Step 19740] Loss: 3.7107\n",
            "[Step 19750] Loss: 4.0362\n",
            "[Step 19760] Loss: 3.3679\n",
            "[Step 19770] Loss: 3.5408\n",
            "[Step 19780] Loss: 3.3893\n",
            "[Step 19790] Loss: 3.2694\n",
            "[Step 19800] Loss: 3.6142\n",
            "[Step 19810] Loss: 3.3402\n",
            "[Step 19820] Loss: 3.6437\n",
            "[Step 19830] Loss: 3.7315\n",
            "[Step 19840] Loss: 3.3964\n",
            "[Step 19850] Loss: 3.4715\n",
            "[Step 19860] Loss: 3.4358\n",
            "[Step 19870] Loss: 3.7747\n",
            "[Step 19880] Loss: 3.7175\n",
            "[Step 19890] Loss: 3.2259\n",
            "[Step 19900] Loss: 3.1222\n",
            "[Step 19910] Loss: 3.6209\n",
            "[Step 19920] Loss: 3.3537\n",
            "[Step 19930] Loss: 3.1631\n",
            "[Step 19940] Loss: 3.5521\n",
            "[Step 19950] Loss: 3.6313\n",
            "[Step 19960] Loss: 3.4165\n",
            "[Step 19970] Loss: 3.4645\n",
            "[Step 19980] Loss: 3.4927\n",
            "[Step 19990] Loss: 3.4927\n",
            "[Step 20000] Loss: 3.4236\n",
            "[Step 20010] Loss: 3.6264\n",
            "[Step 20020] Loss: 3.4905\n",
            "[Step 20030] Loss: 3.2963\n",
            "[Step 20040] Loss: 3.6256\n",
            "[Step 20050] Loss: 3.5776\n",
            "[Step 20060] Loss: 3.3400\n",
            "[Step 20070] Loss: 3.8384\n",
            "[Step 20080] Loss: 3.2941\n",
            "[Step 20090] Loss: 3.6857\n",
            "[Step 20100] Loss: 3.7454\n",
            "[Step 20110] Loss: 3.5372\n",
            "[Step 20120] Loss: 3.7902\n",
            "[Step 20130] Loss: 3.3898\n",
            "[Step 20140] Loss: 3.3922\n",
            "[Step 20150] Loss: 3.4580\n",
            "[Step 20160] Loss: 3.3148\n",
            "[Step 20170] Loss: 3.4877\n",
            "[Step 20180] Loss: 3.4709\n",
            "[Step 20190] Loss: 3.5125\n",
            "[Step 20200] Loss: 3.1184\n",
            "[Step 20210] Loss: 3.4727\n",
            "[Step 20220] Loss: 3.8420\n",
            "[Step 20230] Loss: 3.3549\n",
            "[Step 20240] Loss: 3.4382\n",
            "[Step 20250] Loss: 3.7480\n",
            "[Step 20260] Loss: 3.4647\n",
            "[Step 20270] Loss: 3.3390\n",
            "[Step 20280] Loss: 3.6181\n",
            "[Step 20290] Loss: 3.6090\n",
            "[Step 20300] Loss: 3.7941\n",
            "[Step 20310] Loss: 3.6664\n",
            "[Step 20320] Loss: 3.6721\n",
            "[Step 20330] Loss: 3.4140\n",
            "[Step 20340] Loss: 3.5266\n",
            "[Step 20350] Loss: 3.3847\n",
            "[Step 20360] Loss: 3.4597\n",
            "[Step 20370] Loss: 3.4975\n",
            "[Step 20380] Loss: 3.4727\n",
            "[Step 20390] Loss: 3.6532\n",
            "[Step 20400] Loss: 3.4823\n",
            "[Step 20410] Loss: 3.5210\n",
            "[Step 20420] Loss: 3.2589\n",
            "[Step 20430] Loss: 3.5566\n",
            "[Step 20440] Loss: 3.3557\n",
            "[Step 20450] Loss: 3.6399\n",
            "[Step 20460] Loss: 3.2846\n",
            "[Step 20470] Loss: 3.3366\n",
            "[Step 20480] Loss: 3.5733\n",
            "[Step 20490] Loss: 3.4814\n",
            "[Step 20500] Loss: 3.5200\n",
            "[Step 20510] Loss: 3.2978\n",
            "[Step 20520] Loss: 3.6203\n",
            "[Step 20530] Loss: 4.0028\n",
            "[Step 20540] Loss: 3.4731\n",
            "[Step 20550] Loss: 3.3958\n",
            "[Step 20560] Loss: 3.4262\n",
            "[Step 20570] Loss: 3.4708\n",
            "[Step 20580] Loss: 3.2675\n",
            "[Step 20590] Loss: 3.4259\n",
            "[Step 20600] Loss: 3.5380\n",
            "[Step 20610] Loss: 3.4122\n",
            "[Step 20620] Loss: 3.5252\n",
            "[Step 20630] Loss: 3.4848\n",
            "[Step 20640] Loss: 3.1913\n",
            "[Step 20650] Loss: 3.6620\n",
            "[Step 20660] Loss: 3.5914\n",
            "[Step 20670] Loss: 3.7014\n",
            "[Step 20680] Loss: 3.5021\n",
            "[Step 20690] Loss: 3.5135\n",
            "[Step 20700] Loss: 3.2387\n",
            "[Step 20710] Loss: 3.5135\n",
            "[Step 20720] Loss: 3.4254\n",
            "[Step 20730] Loss: 3.2920\n",
            "[Step 20740] Loss: 3.4442\n",
            "[Step 20750] Loss: 3.4183\n",
            "[Step 20760] Loss: 3.3392\n",
            "[Step 20770] Loss: 3.4792\n",
            "[Step 20780] Loss: 3.5295\n",
            "[Step 20790] Loss: 3.8159\n",
            "[Step 20800] Loss: 3.3792\n",
            "[Step 20810] Loss: 3.3978\n",
            "[Step 20820] Loss: 3.6817\n",
            "[Step 20830] Loss: 3.4556\n",
            "ðŸ“˜ Epoch 11/20 - Avg Training Loss: 3.5038\n",
            "[Step 20840] Loss: 3.6748\n",
            "[Step 20850] Loss: 3.3869\n",
            "[Step 20860] Loss: 3.9564\n",
            "[Step 20870] Loss: 3.7198\n",
            "[Step 20880] Loss: 3.3969\n",
            "[Step 20890] Loss: 3.6048\n",
            "[Step 20900] Loss: 3.2692\n",
            "[Step 20910] Loss: 3.6129\n",
            "[Step 20920] Loss: 3.6416\n",
            "[Step 20930] Loss: 3.6465\n",
            "[Step 20940] Loss: 3.5081\n",
            "[Step 20950] Loss: 3.6635\n",
            "[Step 20960] Loss: 3.2160\n",
            "[Step 20970] Loss: 3.6237\n",
            "[Step 20980] Loss: 3.5192\n",
            "[Step 20990] Loss: 3.2817\n",
            "[Step 21000] Loss: 3.4163\n",
            "[Step 21010] Loss: 3.2703\n",
            "[Step 21020] Loss: 3.4002\n",
            "[Step 21030] Loss: 3.6952\n",
            "[Step 21040] Loss: 3.6941\n",
            "[Step 21050] Loss: 3.5049\n",
            "[Step 21060] Loss: 3.0941\n",
            "[Step 21070] Loss: 3.5730\n",
            "[Step 21080] Loss: 3.5806\n",
            "[Step 21090] Loss: 3.5055\n",
            "[Step 21100] Loss: 3.5372\n",
            "[Step 21110] Loss: 3.4458\n",
            "[Step 21120] Loss: 3.3624\n",
            "[Step 21130] Loss: 3.4506\n",
            "[Step 21140] Loss: 3.6394\n",
            "[Step 21150] Loss: 3.4858\n",
            "[Step 21160] Loss: 3.4656\n",
            "[Step 21170] Loss: 3.5209\n",
            "[Step 21180] Loss: 3.0361\n",
            "[Step 21190] Loss: 3.5481\n",
            "[Step 21200] Loss: 3.6489\n",
            "[Step 21210] Loss: 3.4288\n",
            "[Step 21220] Loss: 3.4773\n",
            "[Step 21230] Loss: 3.5855\n",
            "[Step 21240] Loss: 3.5868\n",
            "[Step 21250] Loss: 3.5895\n",
            "[Step 21260] Loss: 3.3278\n",
            "[Step 21270] Loss: 3.6675\n",
            "[Step 21280] Loss: 3.5835\n",
            "[Step 21290] Loss: 3.3172\n",
            "[Step 21300] Loss: 3.4742\n",
            "[Step 21310] Loss: 3.2703\n",
            "[Step 21320] Loss: 3.3418\n",
            "[Step 21330] Loss: 3.4010\n",
            "[Step 21340] Loss: 3.4815\n",
            "[Step 21350] Loss: 3.5322\n",
            "[Step 21360] Loss: 3.5200\n",
            "[Step 21370] Loss: 3.5339\n",
            "[Step 21380] Loss: 3.3836\n",
            "[Step 21390] Loss: 3.8565\n",
            "[Step 21400] Loss: 3.7198\n",
            "[Step 21410] Loss: 3.4312\n",
            "[Step 21420] Loss: 3.3006\n",
            "[Step 21430] Loss: 3.4066\n",
            "[Step 21440] Loss: 3.4858\n",
            "[Step 21450] Loss: 3.3264\n",
            "[Step 21460] Loss: 3.4164\n",
            "[Step 21470] Loss: 3.3057\n",
            "[Step 21480] Loss: 3.3724\n",
            "[Step 21490] Loss: 3.1614\n",
            "[Step 21500] Loss: 3.4528\n",
            "[Step 21510] Loss: 3.7621\n",
            "[Step 21520] Loss: 3.3199\n",
            "[Step 21530] Loss: 3.8466\n",
            "[Step 21540] Loss: 3.7247\n",
            "[Step 21550] Loss: 3.4548\n",
            "[Step 21560] Loss: 3.4746\n",
            "[Step 21570] Loss: 3.5468\n",
            "[Step 21580] Loss: 3.4663\n",
            "[Step 21590] Loss: 3.4963\n",
            "[Step 21600] Loss: 3.4518\n",
            "[Step 21610] Loss: 3.2046\n",
            "[Step 21620] Loss: 3.5815\n",
            "[Step 21630] Loss: 3.2577\n",
            "[Step 21640] Loss: 3.6132\n",
            "[Step 21650] Loss: 3.2489\n",
            "[Step 21660] Loss: 3.4915\n",
            "[Step 21670] Loss: 3.4679\n",
            "[Step 21680] Loss: 3.1851\n",
            "[Step 21690] Loss: 3.6005\n",
            "[Step 21700] Loss: 3.6480\n",
            "[Step 21710] Loss: 3.5681\n",
            "[Step 21720] Loss: 3.6490\n",
            "[Step 21730] Loss: 3.7426\n",
            "[Step 21740] Loss: 3.5318\n",
            "[Step 21750] Loss: 3.6181\n",
            "[Step 21760] Loss: 3.3885\n",
            "[Step 21770] Loss: 3.6598\n",
            "[Step 21780] Loss: 3.2768\n",
            "[Step 21790] Loss: 3.5137\n",
            "[Step 21800] Loss: 3.4345\n",
            "[Step 21810] Loss: 3.3442\n",
            "[Step 21820] Loss: 3.5232\n",
            "[Step 21830] Loss: 3.3853\n",
            "[Step 21840] Loss: 3.6313\n",
            "[Step 21850] Loss: 3.5109\n",
            "[Step 21860] Loss: 3.7132\n",
            "[Step 21870] Loss: 3.5631\n",
            "[Step 21880] Loss: 3.5703\n",
            "[Step 21890] Loss: 3.3148\n",
            "[Step 21900] Loss: 3.4767\n",
            "[Step 21910] Loss: 3.6177\n",
            "[Step 21920] Loss: 3.6383\n",
            "[Step 21930] Loss: 3.0876\n",
            "[Step 21940] Loss: 3.2560\n",
            "[Step 21950] Loss: 3.6249\n",
            "[Step 21960] Loss: 3.2420\n",
            "[Step 21970] Loss: 3.3557\n",
            "[Step 21980] Loss: 3.4102\n",
            "[Step 21990] Loss: 3.8466\n",
            "[Step 22000] Loss: 3.1774\n",
            "[Step 22010] Loss: 3.3917\n",
            "[Step 22020] Loss: 3.5528\n",
            "[Step 22030] Loss: 3.5248\n",
            "[Step 22040] Loss: 3.5434\n",
            "[Step 22050] Loss: 3.4445\n",
            "[Step 22060] Loss: 3.5809\n",
            "[Step 22070] Loss: 3.6073\n",
            "[Step 22080] Loss: 3.5279\n",
            "[Step 22090] Loss: 3.4399\n",
            "[Step 22100] Loss: 3.3577\n",
            "[Step 22110] Loss: 3.3953\n",
            "[Step 22120] Loss: 3.5000\n",
            "[Step 22130] Loss: 3.5210\n",
            "[Step 22140] Loss: 3.5904\n",
            "[Step 22150] Loss: 3.4202\n",
            "[Step 22160] Loss: 3.5756\n",
            "[Step 22170] Loss: 3.9410\n",
            "[Step 22180] Loss: 3.4256\n",
            "[Step 22190] Loss: 3.4381\n",
            "[Step 22200] Loss: 3.3091\n",
            "[Step 22210] Loss: 3.4492\n",
            "[Step 22220] Loss: 3.4940\n",
            "[Step 22230] Loss: 3.3660\n",
            "[Step 22240] Loss: 3.5135\n",
            "[Step 22250] Loss: 3.3550\n",
            "[Step 22260] Loss: 3.8812\n",
            "[Step 22270] Loss: 3.3678\n",
            "[Step 22280] Loss: 3.4290\n",
            "[Step 22290] Loss: 3.5099\n",
            "[Step 22300] Loss: 3.5412\n",
            "[Step 22310] Loss: 3.5976\n",
            "[Step 22320] Loss: 3.2394\n",
            "[Step 22330] Loss: 3.6307\n",
            "[Step 22340] Loss: 3.2316\n",
            "[Step 22350] Loss: 3.3714\n",
            "[Step 22360] Loss: 3.3817\n",
            "[Step 22370] Loss: 3.2855\n",
            "[Step 22380] Loss: 3.3392\n",
            "[Step 22390] Loss: 3.7883\n",
            "[Step 22400] Loss: 3.2791\n",
            "[Step 22410] Loss: 3.6191\n",
            "[Step 22420] Loss: 3.3774\n",
            "[Step 22430] Loss: 3.3223\n",
            "[Step 22440] Loss: 3.6523\n",
            "[Step 22450] Loss: 3.4412\n",
            "[Step 22460] Loss: 3.4504\n",
            "[Step 22470] Loss: 3.5964\n",
            "[Step 22480] Loss: 3.8101\n",
            "[Step 22490] Loss: 3.3307\n",
            "[Step 22500] Loss: 3.6093\n",
            "[Step 22510] Loss: 3.5116\n",
            "[Step 22520] Loss: 3.6450\n",
            "[Step 22530] Loss: 3.6420\n",
            "[Step 22540] Loss: 3.5369\n",
            "[Step 22550] Loss: 3.3936\n",
            "[Step 22560] Loss: 3.3967\n",
            "[Step 22570] Loss: 3.3225\n",
            "[Step 22580] Loss: 3.5447\n",
            "[Step 22590] Loss: 3.6381\n",
            "[Step 22600] Loss: 3.4310\n",
            "[Step 22610] Loss: 3.5459\n",
            "[Step 22620] Loss: 3.4733\n",
            "[Step 22630] Loss: 3.5477\n",
            "[Step 22640] Loss: 3.5048\n",
            "[Step 22650] Loss: 3.3313\n",
            "[Step 22660] Loss: 3.6658\n",
            "[Step 22670] Loss: 3.3356\n",
            "[Step 22680] Loss: 3.2934\n",
            "[Step 22690] Loss: 3.6219\n",
            "[Step 22700] Loss: 3.6303\n",
            "[Step 22710] Loss: 3.5268\n",
            "[Step 22720] Loss: 3.4864\n",
            "ðŸ“˜ Epoch 12/20 - Avg Training Loss: 3.4936\n",
            "[Step 22730] Loss: 3.1878\n",
            "[Step 22740] Loss: 3.3828\n",
            "[Step 22750] Loss: 3.9165\n",
            "[Step 22760] Loss: 3.4832\n",
            "[Step 22770] Loss: 3.4248\n",
            "[Step 22780] Loss: 3.3275\n",
            "[Step 22790] Loss: 3.5810\n",
            "[Step 22800] Loss: 3.7348\n",
            "[Step 22810] Loss: 3.4200\n",
            "[Step 22820] Loss: 3.5332\n",
            "[Step 22830] Loss: 3.7152\n",
            "[Step 22840] Loss: 3.6796\n",
            "[Step 22850] Loss: 3.3282\n",
            "[Step 22860] Loss: 3.6090\n",
            "[Step 22870] Loss: 3.4482\n",
            "[Step 22880] Loss: 3.5908\n",
            "[Step 22890] Loss: 3.3180\n",
            "[Step 22900] Loss: 3.4305\n",
            "[Step 22910] Loss: 3.6837\n",
            "[Step 22920] Loss: 3.3891\n",
            "[Step 22930] Loss: 3.4106\n",
            "[Step 22940] Loss: 3.5512\n",
            "[Step 22950] Loss: 3.6308\n",
            "[Step 22960] Loss: 3.1308\n",
            "[Step 22970] Loss: 3.6422\n",
            "[Step 22980] Loss: 3.1209\n",
            "[Step 22990] Loss: 3.5281\n",
            "[Step 23000] Loss: 3.5489\n",
            "[Step 23010] Loss: 3.2674\n",
            "[Step 23020] Loss: 3.2599\n",
            "[Step 23030] Loss: 3.5889\n",
            "[Step 23040] Loss: 3.4762\n",
            "[Step 23050] Loss: 3.4107\n",
            "[Step 23060] Loss: 3.5283\n",
            "[Step 23070] Loss: 3.5987\n",
            "[Step 23080] Loss: 3.2675\n",
            "[Step 23090] Loss: 3.5802\n",
            "[Step 23100] Loss: 3.7441\n",
            "[Step 23110] Loss: 3.4427\n",
            "[Step 23120] Loss: 3.2531\n",
            "[Step 23130] Loss: 3.4970\n",
            "[Step 23140] Loss: 3.9031\n",
            "[Step 23150] Loss: 3.5382\n",
            "[Step 23160] Loss: 3.8658\n",
            "[Step 23170] Loss: 3.6549\n",
            "[Step 23180] Loss: 3.1360\n",
            "[Step 23190] Loss: 3.5588\n",
            "[Step 23200] Loss: 3.4819\n",
            "[Step 23210] Loss: 3.6329\n",
            "[Step 23220] Loss: 3.3586\n",
            "[Step 23230] Loss: 3.5172\n",
            "[Step 23240] Loss: 3.6202\n",
            "[Step 23250] Loss: 3.5455\n",
            "[Step 23260] Loss: 3.4709\n",
            "[Step 23270] Loss: 3.7319\n",
            "[Step 23280] Loss: 3.4856\n",
            "[Step 23290] Loss: 3.5250\n",
            "[Step 23300] Loss: 3.4013\n",
            "[Step 23310] Loss: 3.4207\n",
            "[Step 23320] Loss: 3.5457\n",
            "[Step 23330] Loss: 3.1768\n",
            "[Step 23340] Loss: 3.4101\n",
            "[Step 23350] Loss: 3.6705\n",
            "[Step 23360] Loss: 3.4710\n",
            "[Step 23370] Loss: 3.5978\n",
            "[Step 23380] Loss: 3.2540\n",
            "[Step 23390] Loss: 3.6570\n",
            "[Step 23400] Loss: 3.5556\n",
            "[Step 23410] Loss: 3.4523\n",
            "[Step 23420] Loss: 3.4383\n",
            "[Step 23430] Loss: 3.3715\n",
            "[Step 23440] Loss: 3.7660\n",
            "[Step 23450] Loss: 3.8053\n",
            "[Step 23460] Loss: 3.2379\n",
            "[Step 23470] Loss: 3.4571\n",
            "[Step 23480] Loss: 3.5589\n",
            "[Step 23490] Loss: 3.7058\n",
            "[Step 23500] Loss: 3.6988\n",
            "[Step 23510] Loss: 3.5983\n",
            "[Step 23520] Loss: 3.3084\n",
            "[Step 23530] Loss: 3.4091\n",
            "[Step 23540] Loss: 3.5035\n",
            "[Step 23550] Loss: 3.4051\n",
            "[Step 23560] Loss: 3.5169\n",
            "[Step 23570] Loss: 3.8537\n",
            "[Step 23580] Loss: 3.6694\n",
            "[Step 23590] Loss: 3.5430\n",
            "[Step 23600] Loss: 3.8408\n",
            "[Step 23610] Loss: 3.3205\n",
            "[Step 23620] Loss: 3.3168\n",
            "[Step 23630] Loss: 3.1305\n",
            "[Step 23640] Loss: 3.2120\n",
            "[Step 23650] Loss: 3.6643\n",
            "[Step 23660] Loss: 3.4950\n",
            "[Step 23670] Loss: 3.7963\n",
            "[Step 23680] Loss: 3.5573\n",
            "[Step 23690] Loss: 3.6012\n",
            "[Step 23700] Loss: 3.2727\n",
            "[Step 23710] Loss: 3.3561\n",
            "[Step 23720] Loss: 3.4139\n",
            "[Step 23730] Loss: 3.3497\n",
            "[Step 23740] Loss: 3.2458\n",
            "[Step 23750] Loss: 3.5834\n",
            "[Step 23760] Loss: 3.4304\n",
            "[Step 23770] Loss: 3.6054\n",
            "[Step 23780] Loss: 3.3864\n",
            "[Step 23790] Loss: 3.4347\n",
            "[Step 23800] Loss: 3.5951\n",
            "[Step 23810] Loss: 3.4198\n",
            "[Step 23820] Loss: 3.2975\n",
            "[Step 23830] Loss: 3.6481\n",
            "[Step 23840] Loss: 3.3818\n",
            "[Step 23850] Loss: 3.6167\n",
            "[Step 23860] Loss: 3.5799\n",
            "[Step 23870] Loss: 3.4119\n",
            "[Step 23880] Loss: 3.8001\n",
            "[Step 23890] Loss: 3.5371\n",
            "[Step 23900] Loss: 3.5444\n",
            "[Step 23910] Loss: 3.4139\n",
            "[Step 23920] Loss: 3.4495\n",
            "[Step 23930] Loss: 3.3130\n",
            "[Step 23940] Loss: 3.2157\n",
            "[Step 23950] Loss: 3.2880\n",
            "[Step 23960] Loss: 3.4800\n",
            "[Step 23970] Loss: 3.4774\n",
            "[Step 23980] Loss: 2.8973\n",
            "[Step 23990] Loss: 3.2342\n",
            "[Step 24000] Loss: 3.2690\n",
            "[Step 24010] Loss: 3.5720\n",
            "[Step 24020] Loss: 3.6315\n",
            "[Step 24030] Loss: 3.3835\n",
            "[Step 24040] Loss: 3.3672\n",
            "[Step 24050] Loss: 3.6267\n",
            "[Step 24060] Loss: 3.5707\n",
            "[Step 24070] Loss: 3.5773\n",
            "[Step 24080] Loss: 3.3006\n",
            "[Step 24090] Loss: 3.3882\n",
            "[Step 24100] Loss: 3.2859\n",
            "[Step 24110] Loss: 3.5399\n",
            "[Step 24120] Loss: 3.8834\n",
            "[Step 24130] Loss: 3.2625\n",
            "[Step 24140] Loss: 3.7309\n",
            "[Step 24150] Loss: 3.4378\n",
            "[Step 24160] Loss: 3.8810\n",
            "[Step 24170] Loss: 3.5901\n",
            "[Step 24180] Loss: 3.5189\n",
            "[Step 24190] Loss: 3.6012\n",
            "[Step 24200] Loss: 3.5845\n",
            "[Step 24210] Loss: 3.5015\n",
            "[Step 24220] Loss: 3.2768\n",
            "[Step 24230] Loss: 3.3990\n",
            "[Step 24240] Loss: 3.5446\n",
            "[Step 24250] Loss: 3.7729\n",
            "[Step 24260] Loss: 3.4102\n",
            "[Step 24270] Loss: 3.4456\n",
            "[Step 24280] Loss: 3.2592\n",
            "[Step 24290] Loss: 3.5907\n",
            "[Step 24300] Loss: 3.3818\n",
            "[Step 24310] Loss: 3.3437\n",
            "[Step 24320] Loss: 3.3101\n",
            "[Step 24330] Loss: 3.3071\n",
            "[Step 24340] Loss: 3.2595\n",
            "[Step 24350] Loss: 3.1792\n",
            "[Step 24360] Loss: 3.5028\n",
            "[Step 24370] Loss: 3.3884\n",
            "[Step 24380] Loss: 3.4930\n",
            "[Step 24390] Loss: 3.5930\n",
            "[Step 24400] Loss: 3.5860\n",
            "[Step 24410] Loss: 3.3404\n",
            "[Step 24420] Loss: 3.4908\n",
            "[Step 24430] Loss: 3.2881\n",
            "[Step 24440] Loss: 3.0907\n",
            "[Step 24450] Loss: 3.1666\n",
            "[Step 24460] Loss: 3.2485\n",
            "[Step 24470] Loss: 3.2745\n",
            "[Step 24480] Loss: 3.4831\n",
            "[Step 24490] Loss: 4.0298\n",
            "[Step 24500] Loss: 3.4374\n",
            "[Step 24510] Loss: 3.3647\n",
            "[Step 24520] Loss: 3.3889\n",
            "[Step 24530] Loss: 3.2757\n",
            "[Step 24540] Loss: 3.4306\n",
            "[Step 24550] Loss: 3.0795\n",
            "[Step 24560] Loss: 3.6309\n",
            "[Step 24570] Loss: 3.4387\n",
            "[Step 24580] Loss: 3.2352\n",
            "[Step 24590] Loss: 3.5306\n",
            "[Step 24600] Loss: 3.4687\n",
            "[Step 24610] Loss: 3.6479\n",
            "[Step 24620] Loss: 3.3899\n",
            "ðŸ“˜ Epoch 13/20 - Avg Training Loss: 3.4856\n",
            "[Step 24630] Loss: 3.1873\n",
            "[Step 24640] Loss: 3.6438\n",
            "[Step 24650] Loss: 3.7283\n",
            "[Step 24660] Loss: 3.5811\n",
            "[Step 24670] Loss: 3.3542\n",
            "[Step 24680] Loss: 3.5254\n",
            "[Step 24690] Loss: 3.2075\n",
            "[Step 24700] Loss: 3.5167\n",
            "[Step 24710] Loss: 3.3966\n",
            "[Step 24720] Loss: 3.8973\n",
            "[Step 24730] Loss: 3.3842\n",
            "[Step 24740] Loss: 3.4601\n",
            "[Step 24750] Loss: 3.4258\n",
            "[Step 24760] Loss: 3.6131\n",
            "[Step 24770] Loss: 3.8996\n",
            "[Step 24780] Loss: 3.6079\n",
            "[Step 24790] Loss: 3.3084\n",
            "[Step 24800] Loss: 3.5556\n",
            "[Step 24810] Loss: 3.4619\n",
            "[Step 24820] Loss: 3.2975\n",
            "[Step 24830] Loss: 3.5386\n",
            "[Step 24840] Loss: 3.3082\n",
            "[Step 24850] Loss: 3.5058\n",
            "[Step 24860] Loss: 3.0993\n",
            "[Step 24870] Loss: 3.4979\n",
            "[Step 24880] Loss: 3.6898\n",
            "[Step 24890] Loss: 3.3185\n",
            "[Step 24900] Loss: 3.4796\n",
            "[Step 24910] Loss: 3.5973\n",
            "[Step 24920] Loss: 3.3556\n",
            "[Step 24930] Loss: 3.5964\n",
            "[Step 24940] Loss: 3.3897\n",
            "[Step 24950] Loss: 3.5289\n",
            "[Step 24960] Loss: 3.3938\n",
            "[Step 24970] Loss: 3.2185\n",
            "[Step 24980] Loss: 3.3031\n",
            "[Step 24990] Loss: 3.5975\n",
            "[Step 25000] Loss: 3.3490\n",
            "[Step 25010] Loss: 3.6344\n",
            "[Step 25020] Loss: 3.2754\n",
            "[Step 25030] Loss: 3.6082\n",
            "[Step 25040] Loss: 3.3019\n",
            "[Step 25050] Loss: 3.5356\n",
            "[Step 25060] Loss: 3.6203\n",
            "[Step 25070] Loss: 3.5048\n",
            "[Step 25080] Loss: 3.6656\n",
            "[Step 25090] Loss: 3.6277\n",
            "[Step 25100] Loss: 3.1259\n",
            "[Step 25110] Loss: 3.2235\n",
            "[Step 25120] Loss: 3.4498\n",
            "[Step 25130] Loss: 3.3931\n",
            "[Step 25140] Loss: 3.1396\n",
            "[Step 25150] Loss: 3.3638\n",
            "[Step 25160] Loss: 3.5658\n",
            "[Step 25170] Loss: 3.7070\n",
            "[Step 25180] Loss: 3.5048\n",
            "[Step 25190] Loss: 3.2961\n",
            "[Step 25200] Loss: 3.5381\n",
            "[Step 25210] Loss: 3.5665\n",
            "[Step 25220] Loss: 3.6650\n",
            "[Step 25230] Loss: 3.4568\n",
            "[Step 25240] Loss: 3.3938\n",
            "[Step 25250] Loss: 3.2403\n",
            "[Step 25260] Loss: 3.4493\n",
            "[Step 25270] Loss: 3.5484\n",
            "[Step 25280] Loss: 3.1710\n",
            "[Step 25290] Loss: 3.5591\n",
            "[Step 25300] Loss: 3.5540\n",
            "[Step 25310] Loss: 3.3925\n",
            "[Step 25320] Loss: 3.6436\n",
            "[Step 25330] Loss: 3.5405\n",
            "[Step 25340] Loss: 3.3099\n",
            "[Step 25350] Loss: 3.6056\n",
            "[Step 25360] Loss: 3.5563\n",
            "[Step 25370] Loss: 3.5459\n",
            "[Step 25380] Loss: 3.4146\n",
            "[Step 25390] Loss: 3.6241\n",
            "[Step 25400] Loss: 3.4815\n",
            "[Step 25410] Loss: 3.4005\n",
            "[Step 25420] Loss: 3.4396\n",
            "[Step 25430] Loss: 3.4314\n",
            "[Step 25440] Loss: 3.6879\n",
            "[Step 25450] Loss: 3.5473\n",
            "[Step 25460] Loss: 3.3222\n",
            "[Step 25470] Loss: 3.3966\n",
            "[Step 25480] Loss: 3.3826\n",
            "[Step 25490] Loss: 3.8134\n",
            "[Step 25500] Loss: 3.0724\n",
            "[Step 25510] Loss: 3.5020\n",
            "[Step 25520] Loss: 3.5550\n",
            "[Step 25530] Loss: 3.4353\n",
            "[Step 25540] Loss: 3.6013\n",
            "[Step 25550] Loss: 3.4573\n",
            "[Step 25560] Loss: 3.1567\n",
            "[Step 25570] Loss: 3.1760\n",
            "[Step 25580] Loss: 3.5986\n",
            "[Step 25590] Loss: 3.5201\n",
            "[Step 25600] Loss: 3.6133\n",
            "[Step 25610] Loss: 3.2179\n",
            "[Step 25620] Loss: 3.2539\n",
            "[Step 25630] Loss: 3.2315\n",
            "[Step 25640] Loss: 3.6393\n",
            "[Step 25650] Loss: 3.1423\n",
            "[Step 25660] Loss: 3.2751\n",
            "[Step 25670] Loss: 3.5268\n",
            "[Step 25680] Loss: 3.7314\n",
            "[Step 25690] Loss: 3.4544\n",
            "[Step 25700] Loss: 3.4983\n",
            "[Step 25710] Loss: 3.5627\n",
            "[Step 25720] Loss: 3.1943\n",
            "[Step 25730] Loss: 3.6977\n",
            "[Step 25740] Loss: 3.3264\n",
            "[Step 25750] Loss: 3.6239\n",
            "[Step 25760] Loss: 3.5002\n",
            "[Step 25770] Loss: 3.1510\n",
            "[Step 25780] Loss: 3.6844\n",
            "[Step 25790] Loss: 3.2459\n",
            "[Step 25800] Loss: 3.5338\n",
            "[Step 25810] Loss: 3.7815\n",
            "[Step 25820] Loss: 3.4683\n",
            "[Step 25830] Loss: 3.6312\n",
            "[Step 25840] Loss: 3.7195\n",
            "[Step 25850] Loss: 3.7200\n",
            "[Step 25860] Loss: 3.6398\n",
            "[Step 25870] Loss: 3.1658\n",
            "[Step 25880] Loss: 3.2753\n",
            "[Step 25890] Loss: 3.6656\n",
            "[Step 25900] Loss: 3.2346\n",
            "[Step 25910] Loss: 3.6591\n",
            "[Step 25920] Loss: 3.6432\n",
            "[Step 25930] Loss: 3.4174\n",
            "[Step 25940] Loss: 3.3177\n",
            "[Step 25950] Loss: 3.4445\n",
            "[Step 25960] Loss: 3.3514\n",
            "[Step 25970] Loss: 3.5766\n",
            "[Step 25980] Loss: 3.4477\n",
            "[Step 25990] Loss: 3.4367\n",
            "[Step 26000] Loss: 3.7260\n",
            "[Step 26010] Loss: 3.6554\n",
            "[Step 26020] Loss: 3.6440\n",
            "[Step 26030] Loss: 3.3960\n",
            "[Step 26040] Loss: 3.4573\n",
            "[Step 26050] Loss: 3.5210\n",
            "[Step 26060] Loss: 3.3624\n",
            "[Step 26070] Loss: 3.1967\n",
            "[Step 26080] Loss: 3.4146\n",
            "[Step 26090] Loss: 3.7622\n",
            "[Step 26100] Loss: 3.4825\n",
            "[Step 26110] Loss: 3.5714\n",
            "[Step 26120] Loss: 3.5648\n",
            "[Step 26130] Loss: 3.5707\n",
            "[Step 26140] Loss: 3.6792\n",
            "[Step 26150] Loss: 3.4294\n",
            "[Step 26160] Loss: 3.5404\n",
            "[Step 26170] Loss: 3.2849\n",
            "[Step 26180] Loss: 3.5469\n",
            "[Step 26190] Loss: 3.5515\n",
            "[Step 26200] Loss: 3.5421\n",
            "[Step 26210] Loss: 3.5242\n",
            "[Step 26220] Loss: 3.4435\n",
            "[Step 26230] Loss: 3.6482\n",
            "[Step 26240] Loss: 3.5948\n",
            "[Step 26250] Loss: 3.6757\n",
            "[Step 26260] Loss: 3.4400\n",
            "[Step 26270] Loss: 3.7831\n",
            "[Step 26280] Loss: 3.4352\n",
            "[Step 26290] Loss: 3.4850\n",
            "[Step 26300] Loss: 3.4523\n",
            "[Step 26310] Loss: 3.4722\n",
            "[Step 26320] Loss: 3.7933\n",
            "[Step 26330] Loss: 3.5446\n",
            "[Step 26340] Loss: 3.6427\n",
            "[Step 26350] Loss: 3.6542\n",
            "[Step 26360] Loss: 3.4908\n",
            "[Step 26370] Loss: 3.4192\n",
            "[Step 26380] Loss: 3.4158\n",
            "[Step 26390] Loss: 3.8281\n",
            "[Step 26400] Loss: 3.4872\n",
            "[Step 26410] Loss: 3.3786\n",
            "[Step 26420] Loss: 3.3920\n",
            "[Step 26430] Loss: 3.2469\n",
            "[Step 26440] Loss: 3.3677\n",
            "[Step 26450] Loss: 3.6878\n",
            "[Step 26460] Loss: 3.3900\n",
            "[Step 26470] Loss: 3.6453\n",
            "[Step 26480] Loss: 3.5351\n",
            "[Step 26490] Loss: 3.5347\n",
            "[Step 26500] Loss: 3.2626\n",
            "[Step 26510] Loss: 3.6751\n",
            "ðŸ“˜ Epoch 14/20 - Avg Training Loss: 3.4773\n",
            "[Step 26520] Loss: 3.3098\n",
            "[Step 26530] Loss: 3.0751\n",
            "[Step 26540] Loss: 3.6730\n",
            "[Step 26550] Loss: 3.2188\n",
            "[Step 26560] Loss: 3.4372\n",
            "[Step 26570] Loss: 3.1216\n",
            "[Step 26580] Loss: 3.5612\n",
            "[Step 26590] Loss: 3.5901\n",
            "[Step 26600] Loss: 3.4400\n",
            "[Step 26610] Loss: 3.4408\n",
            "[Step 26620] Loss: 3.5927\n",
            "[Step 26630] Loss: 3.1331\n",
            "[Step 26640] Loss: 3.5689\n",
            "[Step 26650] Loss: 3.5745\n",
            "[Step 26660] Loss: 3.1215\n",
            "[Step 26670] Loss: 3.2525\n",
            "[Step 26680] Loss: 3.1412\n",
            "[Step 26690] Loss: 3.4311\n",
            "[Step 26700] Loss: 3.6355\n",
            "[Step 26710] Loss: 3.3409\n",
            "[Step 26720] Loss: 3.2574\n",
            "[Step 26730] Loss: 3.3469\n",
            "[Step 26740] Loss: 3.5886\n",
            "[Step 26750] Loss: 3.2472\n",
            "[Step 26760] Loss: 3.6337\n",
            "[Step 26770] Loss: 3.5712\n",
            "[Step 26780] Loss: 3.3479\n",
            "[Step 26790] Loss: 3.5988\n",
            "[Step 26800] Loss: 3.3335\n",
            "[Step 26810] Loss: 3.7806\n",
            "[Step 26820] Loss: 3.2771\n",
            "[Step 26830] Loss: 3.1039\n",
            "[Step 26840] Loss: 3.1818\n",
            "[Step 26850] Loss: 3.3358\n",
            "[Step 26860] Loss: 3.3127\n",
            "[Step 26870] Loss: 3.3352\n",
            "[Step 26880] Loss: 3.3228\n",
            "[Step 26890] Loss: 3.6480\n",
            "[Step 26900] Loss: 3.6281\n",
            "[Step 26910] Loss: 3.4495\n",
            "[Step 26920] Loss: 3.2686\n",
            "[Step 26930] Loss: 3.6529\n",
            "[Step 26940] Loss: 3.4153\n",
            "[Step 26950] Loss: 3.3068\n",
            "[Step 26960] Loss: 3.3559\n",
            "[Step 26970] Loss: 3.3330\n",
            "[Step 26980] Loss: 3.1876\n",
            "[Step 26990] Loss: 3.6123\n",
            "[Step 27000] Loss: 3.7183\n",
            "[Step 27010] Loss: 3.5863\n",
            "[Step 27020] Loss: 3.6930\n",
            "[Step 27030] Loss: 3.2279\n",
            "[Step 27040] Loss: 3.5176\n",
            "[Step 27050] Loss: 3.7463\n",
            "[Step 27060] Loss: 3.2733\n",
            "[Step 27070] Loss: 3.1839\n",
            "[Step 27080] Loss: 3.2671\n",
            "[Step 27090] Loss: 3.3945\n",
            "[Step 27100] Loss: 3.4374\n",
            "[Step 27110] Loss: 3.3979\n",
            "[Step 27120] Loss: 3.2540\n",
            "[Step 27130] Loss: 3.4894\n",
            "[Step 27140] Loss: 3.6392\n",
            "[Step 27150] Loss: 3.3389\n",
            "[Step 27160] Loss: 3.3814\n",
            "[Step 27170] Loss: 3.3374\n",
            "[Step 27180] Loss: 3.2905\n",
            "[Step 27190] Loss: 3.5962\n",
            "[Step 27200] Loss: 3.2020\n",
            "[Step 27210] Loss: 3.5390\n",
            "[Step 27220] Loss: 3.6687\n",
            "[Step 27230] Loss: 3.2588\n",
            "[Step 27240] Loss: 3.7240\n",
            "[Step 27250] Loss: 3.2133\n",
            "[Step 27260] Loss: 3.6945\n",
            "[Step 27270] Loss: 3.7031\n",
            "[Step 27280] Loss: 3.2345\n",
            "[Step 27290] Loss: 3.4502\n",
            "[Step 27300] Loss: 3.6207\n",
            "[Step 27310] Loss: 3.5194\n",
            "[Step 27320] Loss: 3.3992\n",
            "[Step 27330] Loss: 3.2759\n",
            "[Step 27340] Loss: 3.7267\n",
            "[Step 27350] Loss: 3.5567\n",
            "[Step 27360] Loss: 3.5769\n",
            "[Step 27370] Loss: 3.4162\n",
            "[Step 27380] Loss: 3.6363\n",
            "[Step 27390] Loss: 3.4596\n",
            "[Step 27400] Loss: 3.3790\n",
            "[Step 27410] Loss: 3.6796\n",
            "[Step 27420] Loss: 3.3326\n",
            "[Step 27430] Loss: 3.3155\n",
            "[Step 27440] Loss: 3.4297\n",
            "[Step 27450] Loss: 3.3268\n",
            "[Step 27460] Loss: 3.5814\n",
            "[Step 27470] Loss: 3.2427\n",
            "[Step 27480] Loss: 3.8106\n",
            "[Step 27490] Loss: 3.2574\n",
            "[Step 27500] Loss: 3.5891\n",
            "[Step 27510] Loss: 3.3268\n",
            "[Step 27520] Loss: 3.4019\n",
            "[Step 27530] Loss: 3.2994\n",
            "[Step 27540] Loss: 3.3323\n",
            "[Step 27550] Loss: 3.4305\n",
            "[Step 27560] Loss: 3.0828\n",
            "[Step 27570] Loss: 3.6452\n",
            "[Step 27580] Loss: 3.3459\n",
            "[Step 27590] Loss: 3.7063\n",
            "[Step 27600] Loss: 3.4036\n",
            "[Step 27610] Loss: 3.5704\n",
            "[Step 27620] Loss: 3.3417\n",
            "[Step 27630] Loss: 3.5125\n",
            "[Step 27640] Loss: 3.5181\n",
            "[Step 27650] Loss: 3.4946\n",
            "[Step 27660] Loss: 3.7839\n",
            "[Step 27670] Loss: 3.5114\n",
            "[Step 27680] Loss: 3.0774\n",
            "[Step 27690] Loss: 3.4841\n",
            "[Step 27700] Loss: 3.2433\n",
            "[Step 27710] Loss: 3.3345\n",
            "[Step 27720] Loss: 3.7010\n",
            "[Step 27730] Loss: 3.5982\n",
            "[Step 27740] Loss: 3.2512\n",
            "[Step 27750] Loss: 3.2024\n",
            "[Step 27760] Loss: 3.4012\n",
            "[Step 27770] Loss: 3.0977\n",
            "[Step 27780] Loss: 3.6602\n",
            "[Step 27790] Loss: 3.2959\n",
            "[Step 27800] Loss: 3.3023\n",
            "[Step 27810] Loss: 3.4440\n",
            "[Step 27820] Loss: 3.7458\n",
            "[Step 27830] Loss: 3.2747\n",
            "[Step 27840] Loss: 3.0627\n",
            "[Step 27850] Loss: 3.2336\n",
            "[Step 27860] Loss: 3.7059\n",
            "[Step 27870] Loss: 3.6031\n",
            "[Step 27880] Loss: 3.7891\n",
            "[Step 27890] Loss: 3.3098\n",
            "[Step 27900] Loss: 3.5261\n",
            "[Step 27910] Loss: 3.6230\n",
            "[Step 27920] Loss: 3.5685\n",
            "[Step 27930] Loss: 3.4407\n",
            "[Step 27940] Loss: 3.6031\n",
            "[Step 27950] Loss: 3.5113\n",
            "[Step 27960] Loss: 3.5911\n",
            "[Step 27970] Loss: 3.3963\n",
            "[Step 27980] Loss: 3.2890\n",
            "[Step 27990] Loss: 3.5006\n",
            "[Step 28000] Loss: 3.4574\n",
            "[Step 28010] Loss: 3.5800\n",
            "[Step 28020] Loss: 3.4323\n",
            "[Step 28030] Loss: 3.6014\n",
            "[Step 28040] Loss: 3.5877\n",
            "[Step 28050] Loss: 3.2489\n",
            "[Step 28060] Loss: 3.5069\n",
            "[Step 28070] Loss: 3.2000\n",
            "[Step 28080] Loss: 3.5764\n",
            "[Step 28090] Loss: 3.3363\n",
            "[Step 28100] Loss: 3.3449\n",
            "[Step 28110] Loss: 3.6195\n",
            "[Step 28120] Loss: 3.5796\n",
            "[Step 28130] Loss: 3.2861\n",
            "[Step 28140] Loss: 3.5179\n",
            "[Step 28150] Loss: 3.4241\n",
            "[Step 28160] Loss: 3.7373\n",
            "[Step 28170] Loss: 3.4361\n",
            "[Step 28180] Loss: 3.1969\n",
            "[Step 28190] Loss: 3.3400\n",
            "[Step 28200] Loss: 3.5073\n",
            "[Step 28210] Loss: 3.5820\n",
            "[Step 28220] Loss: 3.5913\n",
            "[Step 28230] Loss: 3.2392\n",
            "[Step 28240] Loss: 3.3649\n",
            "[Step 28250] Loss: 3.4823\n",
            "[Step 28260] Loss: 3.1745\n",
            "[Step 28270] Loss: 3.4021\n",
            "[Step 28280] Loss: 3.2943\n",
            "[Step 28290] Loss: 3.7320\n",
            "[Step 28300] Loss: 3.5988\n",
            "[Step 28310] Loss: 3.6441\n",
            "[Step 28320] Loss: 3.7102\n",
            "[Step 28330] Loss: 3.4750\n",
            "[Step 28340] Loss: 3.7685\n",
            "[Step 28350] Loss: 3.4768\n",
            "[Step 28360] Loss: 3.5744\n",
            "[Step 28370] Loss: 3.5638\n",
            "[Step 28380] Loss: 3.4470\n",
            "[Step 28390] Loss: 3.5012\n",
            "[Step 28400] Loss: 3.5865\n",
            "[Step 28410] Loss: 3.5807\n",
            "ðŸ“˜ Epoch 15/20 - Avg Training Loss: 3.4694\n",
            "[Step 28420] Loss: 3.3115\n",
            "[Step 28430] Loss: 3.0183\n",
            "[Step 28440] Loss: 3.5177\n",
            "[Step 28450] Loss: 3.2963\n",
            "[Step 28460] Loss: 3.3584\n",
            "[Step 28470] Loss: 3.5957\n",
            "[Step 28480] Loss: 3.2242\n",
            "[Step 28490] Loss: 3.5045\n",
            "[Step 28500] Loss: 3.4799\n",
            "[Step 28510] Loss: 3.6757\n",
            "[Step 28520] Loss: 3.4090\n",
            "[Step 28530] Loss: 3.0874\n",
            "[Step 28540] Loss: 3.5639\n",
            "[Step 28550] Loss: 3.5999\n",
            "[Step 28560] Loss: 3.6791\n",
            "[Step 28570] Loss: 3.5951\n",
            "[Step 28580] Loss: 3.3896\n",
            "[Step 28590] Loss: 3.6656\n",
            "[Step 28600] Loss: 3.4843\n",
            "[Step 28610] Loss: 3.5998\n",
            "[Step 28620] Loss: 3.1354\n",
            "[Step 28630] Loss: 3.3447\n",
            "[Step 28640] Loss: 3.2980\n",
            "[Step 28650] Loss: 3.7512\n",
            "[Step 28660] Loss: 3.5645\n",
            "[Step 28670] Loss: 3.2437\n",
            "[Step 28680] Loss: 3.5299\n",
            "[Step 28690] Loss: 3.4336\n",
            "[Step 28700] Loss: 3.5388\n",
            "[Step 28710] Loss: 3.1914\n",
            "[Step 28720] Loss: 3.4719\n",
            "[Step 28730] Loss: 3.5024\n",
            "[Step 28740] Loss: 3.3400\n",
            "[Step 28750] Loss: 3.4233\n",
            "[Step 28760] Loss: 3.5915\n",
            "[Step 28770] Loss: 3.6149\n",
            "[Step 28780] Loss: 3.5992\n",
            "[Step 28790] Loss: 3.2403\n",
            "[Step 28800] Loss: 3.7218\n",
            "[Step 28810] Loss: 3.3684\n",
            "[Step 28820] Loss: 3.4779\n",
            "[Step 28830] Loss: 3.5142\n",
            "[Step 28840] Loss: 3.4426\n",
            "[Step 28850] Loss: 3.7820\n",
            "[Step 28860] Loss: 3.4273\n",
            "[Step 28870] Loss: 3.3284\n",
            "[Step 28880] Loss: 3.4921\n",
            "[Step 28890] Loss: 3.0926\n",
            "[Step 28900] Loss: 3.1322\n",
            "[Step 28910] Loss: 3.6810\n",
            "[Step 28920] Loss: 3.1565\n",
            "[Step 28930] Loss: 3.5562\n",
            "[Step 28940] Loss: 3.6104\n",
            "[Step 28950] Loss: 3.3290\n",
            "[Step 28960] Loss: 3.5606\n",
            "[Step 28970] Loss: 3.5912\n",
            "[Step 28980] Loss: 3.2501\n",
            "[Step 28990] Loss: 3.5949\n",
            "[Step 29000] Loss: 3.5121\n",
            "[Step 29010] Loss: 3.1994\n",
            "[Step 29020] Loss: 3.7657\n",
            "[Step 29030] Loss: 3.8728\n",
            "[Step 29040] Loss: 3.4443\n",
            "[Step 29050] Loss: 3.4406\n",
            "[Step 29060] Loss: 3.5647\n",
            "[Step 29070] Loss: 3.6880\n",
            "[Step 29080] Loss: 3.2997\n",
            "[Step 29090] Loss: 3.5486\n",
            "[Step 29100] Loss: 3.1812\n",
            "[Step 29110] Loss: 3.5985\n",
            "[Step 29120] Loss: 3.5124\n",
            "[Step 29130] Loss: 3.4497\n",
            "[Step 29140] Loss: 3.5093\n",
            "[Step 29150] Loss: 3.5275\n",
            "[Step 29160] Loss: 3.5022\n",
            "[Step 29170] Loss: 3.3068\n",
            "[Step 29180] Loss: 3.3860\n",
            "[Step 29190] Loss: 3.4255\n",
            "[Step 29200] Loss: 3.4547\n",
            "[Step 29210] Loss: 3.4372\n",
            "[Step 29220] Loss: 3.5084\n",
            "[Step 29230] Loss: 3.3038\n",
            "[Step 29240] Loss: 3.6086\n",
            "[Step 29250] Loss: 3.5398\n",
            "[Step 29260] Loss: 3.3095\n",
            "[Step 29270] Loss: 3.8044\n",
            "[Step 29280] Loss: 3.3101\n",
            "[Step 29290] Loss: 3.4409\n",
            "[Step 29300] Loss: 3.5656\n",
            "[Step 29310] Loss: 3.5088\n",
            "[Step 29320] Loss: 3.3548\n",
            "[Step 29330] Loss: 3.2680\n",
            "[Step 29340] Loss: 3.2472\n",
            "[Step 29350] Loss: 3.2243\n",
            "[Step 29360] Loss: 3.8610\n",
            "[Step 29370] Loss: 3.4675\n",
            "[Step 29380] Loss: 3.5204\n",
            "[Step 29390] Loss: 3.5654\n",
            "[Step 29400] Loss: 3.0486\n",
            "[Step 29410] Loss: 3.6117\n",
            "[Step 29420] Loss: 3.4231\n",
            "[Step 29430] Loss: 3.2772\n",
            "[Step 29440] Loss: 3.3669\n",
            "[Step 29450] Loss: 3.4411\n",
            "[Step 29460] Loss: 3.5325\n",
            "[Step 29470] Loss: 3.3641\n",
            "[Step 29480] Loss: 3.2663\n",
            "[Step 29490] Loss: 3.3618\n",
            "[Step 29500] Loss: 3.3547\n",
            "[Step 29510] Loss: 3.4541\n",
            "[Step 29520] Loss: 3.3440\n",
            "[Step 29530] Loss: 3.4834\n",
            "[Step 29540] Loss: 3.7159\n",
            "[Step 29550] Loss: 3.5055\n",
            "[Step 29560] Loss: 3.4850\n",
            "[Step 29570] Loss: 3.4368\n",
            "[Step 29580] Loss: 3.6376\n",
            "[Step 29590] Loss: 3.7243\n",
            "[Step 29600] Loss: 3.3911\n",
            "[Step 29610] Loss: 3.3714\n",
            "[Step 29620] Loss: 3.3515\n",
            "[Step 29630] Loss: 3.4599\n",
            "[Step 29640] Loss: 3.4921\n",
            "[Step 29650] Loss: 3.2012\n",
            "[Step 29660] Loss: 3.7494\n",
            "[Step 29670] Loss: 3.2789\n",
            "[Step 29680] Loss: 3.2612\n",
            "[Step 29690] Loss: 3.3147\n",
            "[Step 29700] Loss: 3.2906\n",
            "[Step 29710] Loss: 3.4417\n",
            "[Step 29720] Loss: 3.3696\n",
            "[Step 29730] Loss: 3.3669\n",
            "[Step 29740] Loss: 3.6446\n",
            "[Step 29750] Loss: 3.3061\n",
            "[Step 29760] Loss: 3.4311\n",
            "[Step 29770] Loss: 3.5158\n",
            "[Step 29780] Loss: 3.4990\n",
            "[Step 29790] Loss: 3.4404\n",
            "[Step 29800] Loss: 3.2881\n",
            "[Step 29810] Loss: 3.5909\n",
            "[Step 29820] Loss: 3.9985\n",
            "[Step 29830] Loss: 3.5835\n",
            "[Step 29840] Loss: 3.7223\n",
            "[Step 29850] Loss: 3.0878\n",
            "[Step 29860] Loss: 3.5493\n",
            "[Step 29870] Loss: 3.3598\n",
            "[Step 29880] Loss: 3.5094\n",
            "[Step 29890] Loss: 3.5121\n",
            "[Step 29900] Loss: 3.8649\n",
            "[Step 29910] Loss: 3.5255\n",
            "[Step 29920] Loss: 3.6624\n",
            "[Step 29930] Loss: 3.3918\n",
            "[Step 29940] Loss: 3.7234\n",
            "[Step 29950] Loss: 3.9102\n",
            "[Step 29960] Loss: 3.3285\n",
            "[Step 29970] Loss: 3.6356\n",
            "[Step 29980] Loss: 3.6664\n",
            "[Step 29990] Loss: 3.2740\n",
            "[Step 30000] Loss: 3.5382\n",
            "[Step 30010] Loss: 3.1832\n",
            "[Step 30020] Loss: 3.4998\n",
            "[Step 30030] Loss: 3.5070\n",
            "[Step 30040] Loss: 3.6911\n",
            "[Step 30050] Loss: 3.1335\n",
            "[Step 30060] Loss: 3.4674\n",
            "[Step 30070] Loss: 3.5751\n",
            "[Step 30080] Loss: 3.6703\n",
            "[Step 30090] Loss: 3.6332\n",
            "[Step 30100] Loss: 3.7894\n",
            "[Step 30110] Loss: 3.1276\n",
            "[Step 30120] Loss: 3.5752\n",
            "[Step 30130] Loss: 3.4232\n",
            "[Step 30140] Loss: 3.2879\n",
            "[Step 30150] Loss: 3.3839\n",
            "[Step 30160] Loss: 3.4997\n",
            "[Step 30170] Loss: 3.6392\n",
            "[Step 30180] Loss: 3.5076\n",
            "[Step 30190] Loss: 3.3188\n",
            "[Step 30200] Loss: 3.5112\n",
            "[Step 30210] Loss: 3.5183\n",
            "[Step 30220] Loss: 3.5541\n",
            "[Step 30230] Loss: 3.7251\n",
            "[Step 30240] Loss: 3.3455\n",
            "[Step 30250] Loss: 3.2271\n",
            "[Step 30260] Loss: 3.6343\n",
            "[Step 30270] Loss: 3.3569\n",
            "[Step 30280] Loss: 3.5256\n",
            "[Step 30290] Loss: 3.2441\n",
            "[Step 30300] Loss: 3.5315\n",
            "ðŸ“˜ Epoch 16/20 - Avg Training Loss: 3.4639\n",
            "[Step 30310] Loss: 3.6443\n",
            "[Step 30320] Loss: 3.3476\n",
            "[Step 30330] Loss: 3.6555\n",
            "[Step 30340] Loss: 3.5641\n",
            "[Step 30350] Loss: 3.4534\n",
            "[Step 30360] Loss: 3.4901\n",
            "[Step 30370] Loss: 3.6240\n",
            "[Step 30380] Loss: 3.7160\n",
            "[Step 30390] Loss: 3.1329\n",
            "[Step 30400] Loss: 3.3941\n",
            "[Step 30410] Loss: 3.1763\n",
            "[Step 30420] Loss: 3.5412\n",
            "[Step 30430] Loss: 3.5327\n",
            "[Step 30440] Loss: 3.4757\n",
            "[Step 30450] Loss: 3.5605\n",
            "[Step 30460] Loss: 3.4882\n",
            "[Step 30470] Loss: 3.1882\n",
            "[Step 30480] Loss: 3.4142\n",
            "[Step 30490] Loss: 3.4164\n",
            "[Step 30500] Loss: 3.4706\n",
            "[Step 30510] Loss: 3.2701\n",
            "[Step 30520] Loss: 3.2082\n",
            "[Step 30530] Loss: 3.5529\n",
            "[Step 30540] Loss: 3.4402\n",
            "[Step 30550] Loss: 3.5198\n",
            "[Step 30560] Loss: 3.5159\n",
            "[Step 30570] Loss: 3.2644\n",
            "[Step 30580] Loss: 3.8420\n",
            "[Step 30590] Loss: 3.6164\n",
            "[Step 30600] Loss: 3.5105\n",
            "[Step 30610] Loss: 3.0997\n",
            "[Step 30620] Loss: 3.3597\n",
            "[Step 30630] Loss: 3.3797\n",
            "[Step 30640] Loss: 3.3943\n",
            "[Step 30650] Loss: 3.5878\n",
            "[Step 30660] Loss: 3.7598\n",
            "[Step 30670] Loss: 3.7591\n",
            "[Step 30680] Loss: 3.4465\n",
            "[Step 30690] Loss: 3.3108\n",
            "[Step 30700] Loss: 3.2478\n",
            "[Step 30710] Loss: 3.2215\n",
            "[Step 30720] Loss: 3.4918\n",
            "[Step 30730] Loss: 3.8240\n",
            "[Step 30740] Loss: 3.3219\n",
            "[Step 30750] Loss: 3.6589\n",
            "[Step 30760] Loss: 3.7021\n",
            "[Step 30770] Loss: 3.4611\n",
            "[Step 30780] Loss: 3.5296\n",
            "[Step 30790] Loss: 3.4396\n",
            "[Step 30800] Loss: 3.4286\n",
            "[Step 30810] Loss: 3.5344\n",
            "[Step 30820] Loss: 3.5239\n",
            "[Step 30830] Loss: 3.8140\n",
            "[Step 30840] Loss: 3.6175\n",
            "[Step 30850] Loss: 3.6910\n",
            "[Step 30860] Loss: 3.3657\n",
            "[Step 30870] Loss: 3.4124\n",
            "[Step 30880] Loss: 3.7106\n",
            "[Step 30890] Loss: 3.9896\n",
            "[Step 30900] Loss: 3.1593\n",
            "[Step 30910] Loss: 3.2478\n",
            "[Step 30920] Loss: 3.4073\n",
            "[Step 30930] Loss: 3.4151\n",
            "[Step 30940] Loss: 3.4140\n",
            "[Step 30950] Loss: 3.5378\n",
            "[Step 30960] Loss: 3.3516\n",
            "[Step 30970] Loss: 3.4076\n",
            "[Step 30980] Loss: 3.5493\n",
            "[Step 30990] Loss: 3.3410\n",
            "[Step 31000] Loss: 3.4148\n",
            "[Step 31010] Loss: 3.1570\n",
            "[Step 31020] Loss: 3.5883\n",
            "[Step 31030] Loss: 3.2427\n",
            "[Step 31040] Loss: 3.3148\n",
            "[Step 31050] Loss: 3.6551\n",
            "[Step 31060] Loss: 3.6036\n",
            "[Step 31070] Loss: 3.4050\n",
            "[Step 31080] Loss: 3.5472\n",
            "[Step 31090] Loss: 3.5554\n",
            "[Step 31100] Loss: 3.7715\n",
            "[Step 31110] Loss: 3.5253\n",
            "[Step 31120] Loss: 3.5184\n",
            "[Step 31130] Loss: 3.1924\n",
            "[Step 31140] Loss: 3.7258\n",
            "[Step 31150] Loss: 3.6416\n",
            "[Step 31160] Loss: 3.5161\n",
            "[Step 31170] Loss: 3.7014\n",
            "[Step 31180] Loss: 3.4507\n",
            "[Step 31190] Loss: 3.3995\n",
            "[Step 31200] Loss: 3.5995\n",
            "[Step 31210] Loss: 3.3692\n",
            "[Step 31220] Loss: 3.3519\n",
            "[Step 31230] Loss: 3.0033\n",
            "[Step 31240] Loss: 3.4254\n",
            "[Step 31250] Loss: 3.2629\n",
            "[Step 31260] Loss: 3.4513\n",
            "[Step 31270] Loss: 3.3191\n",
            "[Step 31280] Loss: 3.4171\n",
            "[Step 31290] Loss: 3.2862\n",
            "[Step 31300] Loss: 3.3834\n",
            "[Step 31310] Loss: 3.4793\n",
            "[Step 31320] Loss: 3.1725\n",
            "[Step 31330] Loss: 3.6017\n",
            "[Step 31340] Loss: 3.6204\n",
            "[Step 31350] Loss: 3.3023\n",
            "[Step 31360] Loss: 3.5104\n",
            "[Step 31370] Loss: 3.1557\n",
            "[Step 31380] Loss: 3.6120\n",
            "[Step 31390] Loss: 3.3908\n",
            "[Step 31400] Loss: 3.2775\n",
            "[Step 31410] Loss: 3.8206\n",
            "[Step 31420] Loss: 3.4553\n",
            "[Step 31430] Loss: 3.2407\n",
            "[Step 31440] Loss: 3.2358\n",
            "[Step 31450] Loss: 3.7207\n",
            "[Step 31460] Loss: 3.4678\n",
            "[Step 31470] Loss: 3.7213\n",
            "[Step 31480] Loss: 3.4731\n",
            "[Step 31490] Loss: 3.3835\n",
            "[Step 31500] Loss: 3.4477\n",
            "[Step 31510] Loss: 3.5166\n",
            "[Step 31520] Loss: 3.4849\n",
            "[Step 31530] Loss: 3.5566\n",
            "[Step 31540] Loss: 3.5245\n",
            "[Step 31550] Loss: 3.3517\n",
            "[Step 31560] Loss: 3.4323\n",
            "[Step 31570] Loss: 3.2394\n",
            "[Step 31580] Loss: 3.2315\n",
            "[Step 31590] Loss: 3.7185\n",
            "[Step 31600] Loss: 3.4441\n",
            "[Step 31610] Loss: 3.4023\n",
            "[Step 31620] Loss: 3.5826\n",
            "[Step 31630] Loss: 3.3885\n",
            "[Step 31640] Loss: 3.2765\n",
            "[Step 31650] Loss: 3.8446\n",
            "[Step 31660] Loss: 3.4283\n",
            "[Step 31670] Loss: 3.6440\n",
            "[Step 31680] Loss: 3.4861\n",
            "[Step 31690] Loss: 3.4906\n",
            "[Step 31700] Loss: 3.7785\n",
            "[Step 31710] Loss: 3.3563\n",
            "[Step 31720] Loss: 3.4479\n",
            "[Step 31730] Loss: 3.6358\n",
            "[Step 31740] Loss: 3.6218\n",
            "[Step 31750] Loss: 3.2918\n",
            "[Step 31760] Loss: 3.3376\n",
            "[Step 31770] Loss: 3.4868\n",
            "[Step 31780] Loss: 3.6242\n",
            "[Step 31790] Loss: 3.5428\n",
            "[Step 31800] Loss: 3.2220\n",
            "[Step 31810] Loss: 3.1328\n",
            "[Step 31820] Loss: 3.5626\n",
            "[Step 31830] Loss: 3.3539\n",
            "[Step 31840] Loss: 3.4629\n",
            "[Step 31850] Loss: 3.4623\n",
            "[Step 31860] Loss: 3.4642\n",
            "[Step 31870] Loss: 3.3921\n",
            "[Step 31880] Loss: 3.5821\n",
            "[Step 31890] Loss: 3.4246\n",
            "[Step 31900] Loss: 3.8719\n",
            "[Step 31910] Loss: 3.4313\n",
            "[Step 31920] Loss: 3.4521\n",
            "[Step 31930] Loss: 3.2730\n",
            "[Step 31940] Loss: 3.1971\n",
            "[Step 31950] Loss: 3.2804\n",
            "[Step 31960] Loss: 3.4198\n",
            "[Step 31970] Loss: 3.4651\n",
            "[Step 31980] Loss: 3.4365\n",
            "[Step 31990] Loss: 3.5333\n",
            "[Step 32000] Loss: 3.4335\n",
            "[Step 32010] Loss: 3.6091\n",
            "[Step 32020] Loss: 3.7085\n",
            "[Step 32030] Loss: 3.4506\n",
            "[Step 32040] Loss: 3.3761\n",
            "[Step 32050] Loss: 3.3350\n",
            "[Step 32060] Loss: 3.6443\n",
            "[Step 32070] Loss: 3.4114\n",
            "[Step 32080] Loss: 3.7682\n",
            "[Step 32090] Loss: 3.4258\n",
            "[Step 32100] Loss: 3.2968\n",
            "[Step 32110] Loss: 3.1711\n",
            "[Step 32120] Loss: 3.6987\n",
            "[Step 32130] Loss: 3.1651\n",
            "[Step 32140] Loss: 3.3304\n",
            "[Step 32150] Loss: 3.6267\n",
            "[Step 32160] Loss: 3.4001\n",
            "[Step 32170] Loss: 3.2283\n",
            "[Step 32180] Loss: 3.1136\n",
            "[Step 32190] Loss: 3.3999\n",
            "ðŸ“˜ Epoch 17/20 - Avg Training Loss: 3.4572\n",
            "[Step 32200] Loss: 3.6023\n",
            "[Step 32210] Loss: 3.5301\n",
            "[Step 32220] Loss: 3.6833\n",
            "[Step 32230] Loss: 3.3059\n",
            "[Step 32240] Loss: 3.1822\n",
            "[Step 32250] Loss: 3.4973\n",
            "[Step 32260] Loss: 3.5919\n",
            "[Step 32270] Loss: 3.2321\n",
            "[Step 32280] Loss: 3.2753\n",
            "[Step 32290] Loss: 3.3671\n",
            "[Step 32300] Loss: 3.5579\n",
            "[Step 32310] Loss: 3.2189\n",
            "[Step 32320] Loss: 3.5075\n",
            "[Step 32330] Loss: 3.2298\n",
            "[Step 32340] Loss: 3.6058\n",
            "[Step 32350] Loss: 3.2887\n",
            "[Step 32360] Loss: 3.1894\n",
            "[Step 32370] Loss: 3.4682\n",
            "[Step 32380] Loss: 3.3208\n",
            "[Step 32390] Loss: 3.2753\n",
            "[Step 32400] Loss: 3.3876\n",
            "[Step 32410] Loss: 3.5475\n",
            "[Step 32420] Loss: 3.3431\n",
            "[Step 32430] Loss: 3.3812\n",
            "[Step 32440] Loss: 3.3888\n",
            "[Step 32450] Loss: 3.1927\n",
            "[Step 32460] Loss: 3.3987\n",
            "[Step 32470] Loss: 3.2108\n",
            "[Step 32480] Loss: 3.2621\n",
            "[Step 32490] Loss: 3.9967\n",
            "[Step 32500] Loss: 3.5747\n",
            "[Step 32510] Loss: 3.4275\n",
            "[Step 32520] Loss: 3.4579\n",
            "[Step 32530] Loss: 3.6627\n",
            "[Step 32540] Loss: 3.6224\n",
            "[Step 32550] Loss: 3.5730\n",
            "[Step 32560] Loss: 3.5189\n",
            "[Step 32570] Loss: 3.7212\n",
            "[Step 32580] Loss: 3.4257\n",
            "[Step 32590] Loss: 3.6209\n",
            "[Step 32600] Loss: 3.5806\n",
            "[Step 32610] Loss: 3.5594\n",
            "[Step 32620] Loss: 3.3392\n",
            "[Step 32630] Loss: 3.1361\n",
            "[Step 32640] Loss: 3.4300\n",
            "[Step 32650] Loss: 3.3234\n",
            "[Step 32660] Loss: 3.7329\n",
            "[Step 32670] Loss: 3.2783\n",
            "[Step 32680] Loss: 3.2960\n",
            "[Step 32690] Loss: 3.1349\n",
            "[Step 32700] Loss: 3.7871\n",
            "[Step 32710] Loss: 3.3073\n",
            "[Step 32720] Loss: 3.3831\n",
            "[Step 32730] Loss: 3.3235\n",
            "[Step 32740] Loss: 3.4230\n",
            "[Step 32750] Loss: 3.4603\n",
            "[Step 32760] Loss: 3.2636\n",
            "[Step 32770] Loss: 3.3310\n",
            "[Step 32780] Loss: 3.4462\n",
            "[Step 32790] Loss: 3.5165\n",
            "[Step 32800] Loss: 3.5175\n",
            "[Step 32810] Loss: 3.2599\n",
            "[Step 32820] Loss: 3.4644\n",
            "[Step 32830] Loss: 3.6385\n",
            "[Step 32840] Loss: 3.2706\n",
            "[Step 32850] Loss: 3.1738\n",
            "[Step 32860] Loss: 3.4369\n",
            "[Step 32870] Loss: 3.4074\n",
            "[Step 32880] Loss: 3.4636\n",
            "[Step 32890] Loss: 3.4806\n",
            "[Step 32900] Loss: 3.4069\n",
            "[Step 32910] Loss: 3.5284\n",
            "[Step 32920] Loss: 3.5607\n",
            "[Step 32930] Loss: 3.3317\n",
            "[Step 32940] Loss: 3.4486\n",
            "[Step 32950] Loss: 3.4687\n",
            "[Step 32960] Loss: 3.0255\n",
            "[Step 32970] Loss: 3.4836\n",
            "[Step 32980] Loss: 3.4879\n",
            "[Step 32990] Loss: 3.4402\n",
            "[Step 33000] Loss: 3.6541\n",
            "[Step 33010] Loss: 3.3782\n",
            "[Step 33020] Loss: 3.3047\n",
            "[Step 33030] Loss: 3.4984\n",
            "[Step 33040] Loss: 3.6884\n",
            "[Step 33050] Loss: 3.6272\n",
            "[Step 33060] Loss: 3.6778\n",
            "[Step 33070] Loss: 3.3011\n",
            "[Step 33080] Loss: 3.4223\n",
            "[Step 33090] Loss: 3.4974\n",
            "[Step 33100] Loss: 3.6898\n",
            "[Step 33110] Loss: 3.3771\n",
            "[Step 33120] Loss: 3.7254\n",
            "[Step 33130] Loss: 3.6773\n",
            "[Step 33140] Loss: 3.3629\n",
            "[Step 33150] Loss: 3.3816\n",
            "[Step 33160] Loss: 3.7814\n",
            "[Step 33170] Loss: 3.3980\n",
            "[Step 33180] Loss: 3.1054\n",
            "[Step 33190] Loss: 3.3456\n",
            "[Step 33200] Loss: 3.4666\n",
            "[Step 33210] Loss: 3.9120\n",
            "[Step 33220] Loss: 3.4082\n",
            "[Step 33230] Loss: 3.2721\n",
            "[Step 33240] Loss: 3.4952\n",
            "[Step 33250] Loss: 3.4803\n",
            "[Step 33260] Loss: 3.4540\n",
            "[Step 33270] Loss: 3.2834\n",
            "[Step 33280] Loss: 3.4212\n",
            "[Step 33290] Loss: 3.2170\n",
            "[Step 33300] Loss: 3.2821\n",
            "[Step 33310] Loss: 3.7168\n",
            "[Step 33320] Loss: 3.1217\n",
            "[Step 33330] Loss: 3.4681\n",
            "[Step 33340] Loss: 3.5399\n",
            "[Step 33350] Loss: 3.6624\n",
            "[Step 33360] Loss: 3.5743\n",
            "[Step 33370] Loss: 3.5243\n",
            "[Step 33380] Loss: 3.5306\n",
            "[Step 33390] Loss: 3.5035\n",
            "[Step 33400] Loss: 3.4180\n",
            "[Step 33410] Loss: 3.4943\n",
            "[Step 33420] Loss: 3.4392\n",
            "[Step 33430] Loss: 3.4007\n",
            "[Step 33440] Loss: 3.4099\n",
            "[Step 33450] Loss: 3.4021\n",
            "[Step 33460] Loss: 3.1059\n",
            "[Step 33470] Loss: 3.6322\n",
            "[Step 33480] Loss: 3.6135\n",
            "[Step 33490] Loss: 3.5997\n",
            "[Step 33500] Loss: 3.3755\n",
            "[Step 33510] Loss: 3.5244\n",
            "[Step 33520] Loss: 3.4316\n",
            "[Step 33530] Loss: 3.5234\n",
            "[Step 33540] Loss: 3.2348\n",
            "[Step 33550] Loss: 3.5787\n",
            "[Step 33560] Loss: 3.4555\n",
            "[Step 33570] Loss: 3.5577\n",
            "[Step 33580] Loss: 3.0748\n",
            "[Step 33590] Loss: 3.7569\n",
            "[Step 33600] Loss: 3.3114\n",
            "[Step 33610] Loss: 3.6004\n",
            "[Step 33620] Loss: 3.5282\n",
            "[Step 33630] Loss: 3.5398\n",
            "[Step 33640] Loss: 3.7100\n",
            "[Step 33650] Loss: 3.5805\n",
            "[Step 33660] Loss: 3.6687\n",
            "[Step 33670] Loss: 3.5349\n",
            "[Step 33680] Loss: 3.5483\n",
            "[Step 33690] Loss: 3.5035\n",
            "[Step 33700] Loss: 3.4568\n",
            "[Step 33710] Loss: 3.2739\n",
            "[Step 33720] Loss: 3.4891\n",
            "[Step 33730] Loss: 3.2758\n",
            "[Step 33740] Loss: 3.5214\n",
            "[Step 33750] Loss: 3.3399\n",
            "[Step 33760] Loss: 3.3731\n",
            "[Step 33770] Loss: 3.5393\n",
            "[Step 33780] Loss: 3.3723\n",
            "[Step 33790] Loss: 3.7665\n",
            "[Step 33800] Loss: 3.4881\n",
            "[Step 33810] Loss: 3.2292\n",
            "[Step 33820] Loss: 3.4247\n",
            "[Step 33830] Loss: 3.3638\n",
            "[Step 33840] Loss: 3.5837\n",
            "[Step 33850] Loss: 3.2857\n",
            "[Step 33860] Loss: 3.4029\n",
            "[Step 33870] Loss: 3.2131\n",
            "[Step 33880] Loss: 3.3445\n",
            "[Step 33890] Loss: 3.3635\n",
            "[Step 33900] Loss: 3.7227\n",
            "[Step 33910] Loss: 3.3936\n",
            "[Step 33920] Loss: 3.6713\n",
            "[Step 33930] Loss: 3.4912\n",
            "[Step 33940] Loss: 3.4437\n",
            "[Step 33950] Loss: 3.4517\n",
            "[Step 33960] Loss: 3.7127\n",
            "[Step 33970] Loss: 3.6047\n",
            "[Step 33980] Loss: 3.2434\n",
            "[Step 33990] Loss: 3.4060\n",
            "[Step 34000] Loss: 3.5633\n",
            "[Step 34010] Loss: 3.6127\n",
            "[Step 34020] Loss: 3.5842\n",
            "[Step 34030] Loss: 3.2512\n",
            "[Step 34040] Loss: 3.2638\n",
            "[Step 34050] Loss: 3.2439\n",
            "[Step 34060] Loss: 3.1908\n",
            "[Step 34070] Loss: 3.5925\n",
            "[Step 34080] Loss: 3.4175\n",
            "[Step 34090] Loss: 3.6218\n",
            "ðŸ“˜ Epoch 18/20 - Avg Training Loss: 3.4487\n",
            "[Step 34100] Loss: 3.1602\n",
            "[Step 34110] Loss: 3.3610\n",
            "[Step 34120] Loss: 3.5862\n",
            "[Step 34130] Loss: 3.2764\n",
            "[Step 34140] Loss: 3.7197\n",
            "[Step 34150] Loss: 3.7527\n",
            "[Step 34160] Loss: 3.4381\n",
            "[Step 34170] Loss: 3.2464\n",
            "[Step 34180] Loss: 3.5937\n",
            "[Step 34190] Loss: 3.9500\n",
            "[Step 34200] Loss: 3.4639\n",
            "[Step 34210] Loss: 3.4203\n",
            "[Step 34220] Loss: 3.6044\n",
            "[Step 34230] Loss: 3.5094\n",
            "[Step 34240] Loss: 3.1745\n",
            "[Step 34250] Loss: 3.6242\n",
            "[Step 34260] Loss: 3.4409\n",
            "[Step 34270] Loss: 3.6639\n",
            "[Step 34280] Loss: 3.7706\n",
            "[Step 34290] Loss: 3.4193\n",
            "[Step 34300] Loss: 3.3758\n",
            "[Step 34310] Loss: 3.2889\n",
            "[Step 34320] Loss: 3.3043\n",
            "[Step 34330] Loss: 3.4515\n",
            "[Step 34340] Loss: 3.4550\n",
            "[Step 34350] Loss: 3.7301\n",
            "[Step 34360] Loss: 3.2690\n",
            "[Step 34370] Loss: 3.4901\n",
            "[Step 34380] Loss: 3.6726\n",
            "[Step 34390] Loss: 3.6468\n",
            "[Step 34400] Loss: 3.4333\n",
            "[Step 34410] Loss: 3.2869\n",
            "[Step 34420] Loss: 3.3057\n",
            "[Step 34430] Loss: 3.2686\n",
            "[Step 34440] Loss: 3.5504\n",
            "[Step 34450] Loss: 3.2339\n",
            "[Step 34460] Loss: 3.5533\n",
            "[Step 34470] Loss: 3.4685\n",
            "[Step 34480] Loss: 3.4833\n",
            "[Step 34490] Loss: 3.1711\n",
            "[Step 34500] Loss: 3.6915\n",
            "[Step 34510] Loss: 3.3197\n",
            "[Step 34520] Loss: 3.5777\n",
            "[Step 34530] Loss: 3.4082\n",
            "[Step 34540] Loss: 3.5155\n",
            "[Step 34550] Loss: 3.2994\n",
            "[Step 34560] Loss: 3.5018\n",
            "[Step 34570] Loss: 3.3536\n",
            "[Step 34580] Loss: 3.2692\n",
            "[Step 34590] Loss: 3.4008\n",
            "[Step 34600] Loss: 3.5521\n",
            "[Step 34610] Loss: 3.3837\n",
            "[Step 34620] Loss: 3.6116\n",
            "[Step 34630] Loss: 3.5869\n",
            "[Step 34640] Loss: 3.3397\n",
            "[Step 34650] Loss: 3.1747\n",
            "[Step 34660] Loss: 3.4431\n",
            "[Step 34670] Loss: 3.3190\n",
            "[Step 34680] Loss: 3.4125\n",
            "[Step 34690] Loss: 3.3048\n",
            "[Step 34700] Loss: 3.4278\n",
            "[Step 34710] Loss: 3.3321\n",
            "[Step 34720] Loss: 3.6385\n",
            "[Step 34730] Loss: 3.4348\n",
            "[Step 34740] Loss: 3.3272\n",
            "[Step 34750] Loss: 3.3774\n",
            "[Step 34760] Loss: 3.8292\n",
            "[Step 34770] Loss: 3.5040\n",
            "[Step 34780] Loss: 3.5868\n",
            "[Step 34790] Loss: 3.2465\n",
            "[Step 34800] Loss: 3.4576\n",
            "[Step 34810] Loss: 3.4695\n",
            "[Step 34820] Loss: 3.5143\n",
            "[Step 34830] Loss: 3.3854\n",
            "[Step 34840] Loss: 3.2951\n",
            "[Step 34850] Loss: 3.6467\n",
            "[Step 34860] Loss: 3.6788\n",
            "[Step 34870] Loss: 3.7342\n",
            "[Step 34880] Loss: 3.1647\n",
            "[Step 34890] Loss: 3.2563\n",
            "[Step 34900] Loss: 3.6341\n",
            "[Step 34910] Loss: 3.3152\n",
            "[Step 34920] Loss: 3.7658\n",
            "[Step 34930] Loss: 3.3401\n",
            "[Step 34940] Loss: 3.7013\n",
            "[Step 34950] Loss: 3.3735\n",
            "[Step 34960] Loss: 3.6056\n",
            "[Step 34970] Loss: 3.3414\n",
            "[Step 34980] Loss: 3.3603\n",
            "[Step 34990] Loss: 3.8121\n",
            "[Step 35000] Loss: 3.3388\n",
            "[Step 35010] Loss: 3.2249\n",
            "[Step 35020] Loss: 3.3147\n",
            "[Step 35030] Loss: 3.3512\n",
            "[Step 35040] Loss: 3.4967\n",
            "[Step 35050] Loss: 3.2946\n",
            "[Step 35060] Loss: 3.3804\n",
            "[Step 35070] Loss: 3.2788\n",
            "[Step 35080] Loss: 3.5071\n",
            "[Step 35090] Loss: 3.5131\n",
            "[Step 35100] Loss: 3.5232\n",
            "[Step 35110] Loss: 3.5650\n",
            "[Step 35120] Loss: 3.8153\n",
            "[Step 35130] Loss: 3.6441\n",
            "[Step 35140] Loss: 3.1561\n",
            "[Step 35150] Loss: 3.5413\n",
            "[Step 35160] Loss: 3.4564\n",
            "[Step 35170] Loss: 3.3535\n",
            "[Step 35180] Loss: 3.3598\n",
            "[Step 35190] Loss: 3.4652\n",
            "[Step 35200] Loss: 3.4031\n",
            "[Step 35210] Loss: 3.3654\n",
            "[Step 35220] Loss: 3.1838\n",
            "[Step 35230] Loss: 3.3872\n",
            "[Step 35240] Loss: 3.3984\n",
            "[Step 35250] Loss: 3.2139\n",
            "[Step 35260] Loss: 3.2494\n",
            "[Step 35270] Loss: 3.3736\n",
            "[Step 35280] Loss: 3.5217\n",
            "[Step 35290] Loss: 3.5715\n",
            "[Step 35300] Loss: 3.3706\n",
            "[Step 35310] Loss: 3.1428\n",
            "[Step 35320] Loss: 3.5940\n",
            "[Step 35330] Loss: 3.0041\n",
            "[Step 35340] Loss: 3.7714\n",
            "[Step 35350] Loss: 3.1864\n",
            "[Step 35360] Loss: 3.5350\n",
            "[Step 35370] Loss: 3.5316\n",
            "[Step 35380] Loss: 3.4973\n",
            "[Step 35390] Loss: 3.4996\n",
            "[Step 35400] Loss: 3.6387\n",
            "[Step 35410] Loss: 3.6536\n",
            "[Step 35420] Loss: 3.3250\n",
            "[Step 35430] Loss: 3.3884\n",
            "[Step 35440] Loss: 3.3525\n",
            "[Step 35450] Loss: 3.3541\n",
            "[Step 35460] Loss: 3.6368\n",
            "[Step 35470] Loss: 3.4857\n",
            "[Step 35480] Loss: 3.5443\n",
            "[Step 35490] Loss: 3.1484\n",
            "[Step 35500] Loss: 3.6208\n",
            "[Step 35510] Loss: 3.5101\n",
            "[Step 35520] Loss: 3.2738\n",
            "[Step 35530] Loss: 3.5440\n",
            "[Step 35540] Loss: 3.5817\n",
            "[Step 35550] Loss: 3.3929\n",
            "[Step 35560] Loss: 3.5939\n",
            "[Step 35570] Loss: 3.6539\n",
            "[Step 35580] Loss: 3.4045\n",
            "[Step 35590] Loss: 3.3796\n",
            "[Step 35600] Loss: 3.6484\n",
            "[Step 35610] Loss: 3.6646\n",
            "[Step 35620] Loss: 3.4306\n",
            "[Step 35630] Loss: 3.5834\n",
            "[Step 35640] Loss: 3.2234\n",
            "[Step 35650] Loss: 3.2374\n",
            "[Step 35660] Loss: 3.2852\n",
            "[Step 35670] Loss: 3.5507\n",
            "[Step 35680] Loss: 3.4278\n",
            "[Step 35690] Loss: 3.4859\n",
            "[Step 35700] Loss: 3.4318\n",
            "[Step 35710] Loss: 3.5549\n",
            "[Step 35720] Loss: 3.4704\n",
            "[Step 35730] Loss: 3.4801\n",
            "[Step 35740] Loss: 3.2598\n",
            "[Step 35750] Loss: 3.2679\n",
            "[Step 35760] Loss: 3.6689\n",
            "[Step 35770] Loss: 3.3014\n",
            "[Step 35780] Loss: 3.2113\n",
            "[Step 35790] Loss: 3.5097\n",
            "[Step 35800] Loss: 3.2022\n",
            "[Step 35810] Loss: 3.3546\n",
            "[Step 35820] Loss: 3.3643\n",
            "[Step 35830] Loss: 3.6807\n",
            "[Step 35840] Loss: 3.4909\n",
            "[Step 35850] Loss: 3.3777\n",
            "[Step 35860] Loss: 3.3443\n",
            "[Step 35870] Loss: 3.3982\n",
            "[Step 35880] Loss: 3.4208\n",
            "[Step 35890] Loss: 3.4858\n",
            "[Step 35900] Loss: 3.5483\n",
            "[Step 35910] Loss: 3.1651\n",
            "[Step 35920] Loss: 3.4517\n",
            "[Step 35930] Loss: 3.4225\n",
            "[Step 35940] Loss: 3.5379\n",
            "[Step 35950] Loss: 3.4803\n",
            "[Step 35960] Loss: 3.5594\n",
            "[Step 35970] Loss: 3.4388\n",
            "[Step 35980] Loss: 3.5313\n",
            "ðŸ“˜ Epoch 19/20 - Avg Training Loss: 3.4452\n",
            "[Step 35990] Loss: 3.4275\n",
            "[Step 36000] Loss: 3.6098\n",
            "[Step 36010] Loss: 3.6675\n",
            "[Step 36020] Loss: 3.3491\n",
            "[Step 36030] Loss: 3.5194\n",
            "[Step 36040] Loss: 3.1990\n",
            "[Step 36050] Loss: 3.3450\n",
            "[Step 36060] Loss: 3.6303\n",
            "[Step 36070] Loss: 3.4345\n",
            "[Step 36080] Loss: 3.3051\n",
            "[Step 36090] Loss: 3.5929\n",
            "[Step 36100] Loss: 3.6991\n",
            "[Step 36110] Loss: 3.3424\n",
            "[Step 36120] Loss: 3.3505\n",
            "[Step 36130] Loss: 3.3194\n",
            "[Step 36140] Loss: 3.3121\n",
            "[Step 36150] Loss: 3.5608\n",
            "[Step 36160] Loss: 3.6885\n",
            "[Step 36170] Loss: 3.6322\n",
            "[Step 36180] Loss: 3.3152\n",
            "[Step 36190] Loss: 3.6214\n",
            "[Step 36200] Loss: 3.4403\n",
            "[Step 36210] Loss: 3.2504\n",
            "[Step 36220] Loss: 3.1032\n",
            "[Step 36230] Loss: 3.4557\n",
            "[Step 36240] Loss: 3.6291\n",
            "[Step 36250] Loss: 3.4583\n",
            "[Step 36260] Loss: 3.3400\n",
            "[Step 36270] Loss: 3.1059\n",
            "[Step 36280] Loss: 3.6276\n",
            "[Step 36290] Loss: 2.8889\n",
            "[Step 36300] Loss: 3.5057\n",
            "[Step 36310] Loss: 3.1373\n",
            "[Step 36320] Loss: 3.5122\n",
            "[Step 36330] Loss: 3.4786\n",
            "[Step 36340] Loss: 3.2856\n",
            "[Step 36350] Loss: 3.4848\n",
            "[Step 36360] Loss: 3.2315\n",
            "[Step 36370] Loss: 3.2214\n",
            "[Step 36380] Loss: 3.4131\n",
            "[Step 36390] Loss: 3.6269\n",
            "[Step 36400] Loss: 3.5164\n",
            "[Step 36410] Loss: 3.5809\n",
            "[Step 36420] Loss: 3.8500\n",
            "[Step 36430] Loss: 3.5753\n",
            "[Step 36440] Loss: 3.8826\n",
            "[Step 36450] Loss: 3.4392\n",
            "[Step 36460] Loss: 3.3691\n",
            "[Step 36470] Loss: 3.4976\n",
            "[Step 36480] Loss: 3.6886\n",
            "[Step 36490] Loss: 3.5083\n",
            "[Step 36500] Loss: 3.4876\n",
            "[Step 36510] Loss: 3.3669\n",
            "[Step 36520] Loss: 3.4564\n",
            "[Step 36530] Loss: 3.5788\n",
            "[Step 36540] Loss: 3.3217\n",
            "[Step 36550] Loss: 3.4268\n",
            "[Step 36560] Loss: 3.5342\n",
            "[Step 36570] Loss: 3.3662\n",
            "[Step 36580] Loss: 3.4390\n",
            "[Step 36590] Loss: 3.5138\n",
            "[Step 36600] Loss: 3.1662\n",
            "[Step 36610] Loss: 3.5881\n",
            "[Step 36620] Loss: 3.3693\n",
            "[Step 36630] Loss: 3.1416\n",
            "[Step 36640] Loss: 3.5866\n",
            "[Step 36650] Loss: 3.6324\n",
            "[Step 36660] Loss: 3.3698\n",
            "[Step 36670] Loss: 2.9948\n",
            "[Step 36680] Loss: 3.5860\n",
            "[Step 36690] Loss: 3.5271\n",
            "[Step 36700] Loss: 3.5428\n",
            "[Step 36710] Loss: 3.7238\n",
            "[Step 36720] Loss: 3.3201\n",
            "[Step 36730] Loss: 3.4135\n",
            "[Step 36740] Loss: 3.4140\n",
            "[Step 36750] Loss: 3.0130\n",
            "[Step 36760] Loss: 3.3548\n",
            "[Step 36770] Loss: 3.5923\n",
            "[Step 36780] Loss: 3.3721\n",
            "[Step 36790] Loss: 3.3767\n",
            "[Step 36800] Loss: 3.3674\n",
            "[Step 36810] Loss: 3.3579\n",
            "[Step 36820] Loss: 3.6615\n",
            "[Step 36830] Loss: 3.5117\n",
            "[Step 36840] Loss: 3.4413\n",
            "[Step 36850] Loss: 3.5942\n",
            "[Step 36860] Loss: 3.4831\n",
            "[Step 36870] Loss: 2.9492\n",
            "[Step 36880] Loss: 3.3228\n",
            "[Step 36890] Loss: 3.4668\n",
            "[Step 36900] Loss: 3.0880\n",
            "[Step 36910] Loss: 3.3642\n",
            "[Step 36920] Loss: 3.3837\n",
            "[Step 36930] Loss: 3.2778\n",
            "[Step 36940] Loss: 3.3527\n",
            "[Step 36950] Loss: 3.2655\n",
            "[Step 36960] Loss: 3.2392\n",
            "[Step 36970] Loss: 3.5987\n",
            "[Step 36980] Loss: 3.5528\n",
            "[Step 36990] Loss: 3.3071\n",
            "[Step 37000] Loss: 3.3716\n",
            "[Step 37010] Loss: 3.4606\n",
            "[Step 37020] Loss: 3.4815\n",
            "[Step 37030] Loss: 3.5505\n",
            "[Step 37040] Loss: 3.4115\n",
            "[Step 37050] Loss: 3.5620\n",
            "[Step 37060] Loss: 3.5218\n",
            "[Step 37070] Loss: 3.6993\n",
            "[Step 37080] Loss: 3.4105\n",
            "[Step 37090] Loss: 3.2016\n",
            "[Step 37100] Loss: 3.7004\n",
            "[Step 37110] Loss: 3.4125\n",
            "[Step 37120] Loss: 3.6450\n",
            "[Step 37130] Loss: 3.1733\n",
            "[Step 37140] Loss: 3.2824\n",
            "[Step 37150] Loss: 3.4228\n",
            "[Step 37160] Loss: 3.3000\n",
            "[Step 37170] Loss: 3.2739\n",
            "[Step 37180] Loss: 3.6544\n",
            "[Step 37190] Loss: 3.4837\n",
            "[Step 37200] Loss: 3.6030\n",
            "[Step 37210] Loss: 3.7716\n",
            "[Step 37220] Loss: 3.7086\n",
            "[Step 37230] Loss: 3.2821\n",
            "[Step 37240] Loss: 3.6626\n",
            "[Step 37250] Loss: 3.2241\n",
            "[Step 37260] Loss: 3.2580\n",
            "[Step 37270] Loss: 3.4901\n",
            "[Step 37280] Loss: 3.7170\n",
            "[Step 37290] Loss: 3.8126\n",
            "[Step 37300] Loss: 3.4685\n",
            "[Step 37310] Loss: 3.3048\n",
            "[Step 37320] Loss: 3.6620\n",
            "[Step 37330] Loss: 3.7036\n",
            "[Step 37340] Loss: 3.4282\n",
            "[Step 37350] Loss: 3.4000\n",
            "[Step 37360] Loss: 3.2675\n",
            "[Step 37370] Loss: 3.3843\n",
            "[Step 37380] Loss: 3.3935\n",
            "[Step 37390] Loss: 3.5219\n",
            "[Step 37400] Loss: 3.3395\n",
            "[Step 37410] Loss: 3.6558\n",
            "[Step 37420] Loss: 3.4867\n",
            "[Step 37430] Loss: 3.2798\n",
            "[Step 37440] Loss: 3.3696\n",
            "[Step 37450] Loss: 3.5416\n",
            "[Step 37460] Loss: 3.6787\n",
            "[Step 37470] Loss: 3.4855\n",
            "[Step 37480] Loss: 3.4174\n",
            "[Step 37490] Loss: 3.2997\n",
            "[Step 37500] Loss: 3.4980\n",
            "[Step 37510] Loss: 3.5405\n",
            "[Step 37520] Loss: 3.6584\n",
            "[Step 37530] Loss: 3.5191\n",
            "[Step 37540] Loss: 3.2024\n",
            "[Step 37550] Loss: 3.2720\n",
            "[Step 37560] Loss: 3.4049\n",
            "[Step 37570] Loss: 3.2397\n",
            "[Step 37580] Loss: 3.8512\n",
            "[Step 37590] Loss: 3.2620\n",
            "[Step 37600] Loss: 3.5292\n",
            "[Step 37610] Loss: 3.6182\n",
            "[Step 37620] Loss: 3.2097\n",
            "[Step 37630] Loss: 3.2036\n",
            "[Step 37640] Loss: 3.4592\n",
            "[Step 37650] Loss: 3.4104\n",
            "[Step 37660] Loss: 3.5617\n",
            "[Step 37670] Loss: 3.6371\n",
            "[Step 37680] Loss: 3.3644\n",
            "[Step 37690] Loss: 3.3167\n",
            "[Step 37700] Loss: 3.6269\n",
            "[Step 37710] Loss: 3.4281\n",
            "[Step 37720] Loss: 3.4233\n",
            "[Step 37730] Loss: 3.4138\n",
            "[Step 37740] Loss: 3.3291\n",
            "[Step 37750] Loss: 3.0699\n",
            "[Step 37760] Loss: 3.5139\n",
            "[Step 37770] Loss: 3.3976\n",
            "[Step 37780] Loss: 3.5132\n",
            "[Step 37790] Loss: 3.4045\n",
            "[Step 37800] Loss: 3.2907\n",
            "[Step 37810] Loss: 3.5733\n",
            "[Step 37820] Loss: 3.3629\n",
            "[Step 37830] Loss: 3.3480\n",
            "[Step 37840] Loss: 3.1752\n",
            "[Step 37850] Loss: 3.3656\n",
            "[Step 37860] Loss: 3.4113\n",
            "[Step 37870] Loss: 3.3229\n",
            "[Step 37880] Loss: 3.7230\n",
            "ðŸ“˜ Epoch 20/20 - Avg Training Loss: 3.4370\n",
            "âœ… Training complete\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=training_args[\"learning_rate\"],\n",
        "    weight_decay=training_args[\"weight_decay\"]\n",
        ")\n",
        "def lr_lambda(epoch):\n",
        "    warmup_epochs = 5\n",
        "    if epoch < warmup_epochs:\n",
        "        return float(epoch + 1) / warmup_epochs\n",
        "    return 1.0\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "epochs = training_args[\"num_train_epochs\"]\n",
        "logging_steps = training_args[\"logging_steps\"]\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % logging_steps == 0:\n",
        "            print(f\"[Step {global_step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"ðŸ“˜ Epoch {epoch+1}/{epochs} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "\n",
        "print(\"âœ… Training complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dDLLVqKXW2Mv",
        "outputId": "e4243d77-22c1-42d4-e374-8c33d901b642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 10] Loss: 4.0743\n",
            "[Step 20] Loss: 4.0034\n",
            "[Step 30] Loss: 4.1695\n",
            "[Step 40] Loss: 3.9216\n",
            "[Step 50] Loss: 3.7699\n",
            "[Step 60] Loss: 3.6231\n",
            "[Step 70] Loss: 4.1074\n",
            "[Step 80] Loss: 3.9075\n",
            "[Step 90] Loss: 3.7336\n",
            "[Step 100] Loss: 3.7646\n",
            "[Step 110] Loss: 3.7954\n",
            "[Step 120] Loss: 4.0164\n",
            "[Step 130] Loss: 3.4103\n",
            "[Step 140] Loss: 4.0810\n",
            "[Step 150] Loss: 3.8339\n",
            "[Step 160] Loss: 3.8368\n",
            "[Step 170] Loss: 4.1883\n",
            "[Step 180] Loss: 3.8612\n",
            "[Step 190] Loss: 3.6968\n",
            "[Step 200] Loss: 3.7308\n",
            "[Step 210] Loss: 4.0822\n",
            "[Step 220] Loss: 3.4873\n",
            "[Step 230] Loss: 3.7050\n",
            "[Step 240] Loss: 3.8901\n",
            "[Step 250] Loss: 3.8900\n",
            "[Step 260] Loss: 3.6682\n",
            "[Step 270] Loss: 3.9500\n",
            "[Step 280] Loss: 3.7188\n",
            "[Step 290] Loss: 3.5835\n",
            "[Step 300] Loss: 4.1595\n",
            "[Step 310] Loss: 4.1349\n",
            "[Step 320] Loss: 4.1262\n",
            "[Step 330] Loss: 3.9728\n",
            "[Step 340] Loss: 3.8384\n",
            "[Step 350] Loss: 3.9542\n",
            "[Step 360] Loss: 3.9212\n",
            "[Step 370] Loss: 3.9395\n",
            "[Step 380] Loss: 3.9816\n",
            "[Step 390] Loss: 3.9859\n",
            "[Step 400] Loss: 3.6065\n",
            "[Step 410] Loss: 3.7684\n",
            "[Step 420] Loss: 3.6544\n",
            "[Step 430] Loss: 3.8333\n",
            "[Step 440] Loss: 3.7759\n",
            "[Step 450] Loss: 3.5365\n",
            "[Step 460] Loss: 3.6963\n",
            "[Step 470] Loss: 3.7748\n",
            "[Step 480] Loss: 3.8427\n",
            "[Step 490] Loss: 3.8475\n",
            "[Step 500] Loss: 3.6311\n",
            "[Step 510] Loss: 3.7119\n",
            "[Step 520] Loss: 3.4137\n",
            "[Step 530] Loss: 3.8412\n",
            "[Step 540] Loss: 4.0420\n",
            "[Step 550] Loss: 3.6746\n",
            "[Step 560] Loss: 3.5773\n",
            "[Step 570] Loss: 4.0124\n",
            "[Step 580] Loss: 3.4993\n",
            "[Step 590] Loss: 3.9420\n",
            "[Step 600] Loss: 3.5694\n",
            "[Step 610] Loss: 4.0471\n",
            "[Step 620] Loss: 3.6034\n",
            "[Step 630] Loss: 3.6066\n",
            "[Step 640] Loss: 3.4144\n",
            "[Step 650] Loss: 3.6092\n",
            "[Step 660] Loss: 3.4868\n",
            "[Step 670] Loss: 3.7304\n",
            "[Step 680] Loss: 3.8741\n",
            "[Step 690] Loss: 3.7786\n",
            "[Step 700] Loss: 3.8494\n",
            "[Step 710] Loss: 3.6489\n",
            "[Step 720] Loss: 3.7856\n",
            "[Step 730] Loss: 3.5408\n",
            "[Step 740] Loss: 3.5336\n",
            "[Step 750] Loss: 3.5042\n",
            "[Step 760] Loss: 3.7438\n",
            "[Step 770] Loss: 3.2303\n",
            "[Step 780] Loss: 3.6949\n",
            "[Step 790] Loss: 3.6434\n",
            "[Step 800] Loss: 3.4955\n",
            "[Step 810] Loss: 3.8107\n",
            "[Step 820] Loss: 3.8749\n",
            "[Step 830] Loss: 3.2540\n",
            "[Step 840] Loss: 3.5014\n",
            "[Step 850] Loss: 3.8627\n",
            "[Step 860] Loss: 3.7088\n",
            "[Step 870] Loss: 3.6477\n",
            "[Step 880] Loss: 3.8223\n",
            "[Step 890] Loss: 3.3511\n",
            "[Step 900] Loss: 4.0702\n",
            "[Step 910] Loss: 3.2774\n",
            "[Step 920] Loss: 3.9005\n",
            "[Step 930] Loss: 3.2132\n",
            "[Step 940] Loss: 3.6148\n",
            "[Step 950] Loss: 3.5421\n",
            "[Step 960] Loss: 3.9690\n",
            "[Step 970] Loss: 3.4521\n",
            "[Step 980] Loss: 3.6928\n",
            "[Step 990] Loss: 3.4811\n",
            "[Step 1000] Loss: 3.9935\n",
            "[Step 1010] Loss: 4.1624\n",
            "[Step 1020] Loss: 3.6544\n",
            "[Step 1030] Loss: 3.3363\n",
            "[Step 1040] Loss: 3.5379\n",
            "[Step 1050] Loss: 3.7415\n",
            "[Step 1060] Loss: 3.8904\n",
            "[Step 1070] Loss: 3.5885\n",
            "[Step 1080] Loss: 3.7216\n",
            "[Step 1090] Loss: 3.6461\n",
            "[Step 1100] Loss: 3.5498\n",
            "[Step 1110] Loss: 3.5713\n",
            "[Step 1120] Loss: 3.7111\n",
            "[Step 1130] Loss: 3.5021\n",
            "[Step 1140] Loss: 3.9840\n",
            "[Step 1150] Loss: 3.2602\n",
            "[Step 1160] Loss: 3.5892\n",
            "[Step 1170] Loss: 3.6850\n",
            "[Step 1180] Loss: 3.4520\n",
            "[Step 1190] Loss: 3.8567\n",
            "[Step 1200] Loss: 3.4463\n",
            "[Step 1210] Loss: 3.6413\n",
            "[Step 1220] Loss: 4.0325\n",
            "[Step 1230] Loss: 3.8370\n",
            "[Step 1240] Loss: 4.1031\n",
            "[Step 1250] Loss: 3.5182\n",
            "[Step 1260] Loss: 3.9095\n",
            "[Step 1270] Loss: 3.3781\n",
            "[Step 1280] Loss: 3.6014\n",
            "[Step 1290] Loss: 3.4742\n",
            "[Step 1300] Loss: 3.2952\n",
            "[Step 1310] Loss: 3.3331\n",
            "[Step 1320] Loss: 3.6056\n",
            "[Step 1330] Loss: 3.6450\n",
            "[Step 1340] Loss: 3.2683\n",
            "[Step 1350] Loss: 3.2771\n",
            "[Step 1360] Loss: 4.1644\n",
            "[Step 1370] Loss: 3.8004\n",
            "[Step 1380] Loss: 3.8905\n",
            "[Step 1390] Loss: 3.6264\n",
            "[Step 1400] Loss: 3.5954\n",
            "[Step 1410] Loss: 3.5104\n",
            "[Step 1420] Loss: 3.2698\n",
            "[Step 1430] Loss: 3.6708\n",
            "[Step 1440] Loss: 3.4873\n",
            "[Step 1450] Loss: 3.8738\n",
            "[Step 1460] Loss: 3.5083\n",
            "[Step 1470] Loss: 3.2116\n",
            "[Step 1480] Loss: 3.8756\n",
            "[Step 1490] Loss: 3.5546\n",
            "[Step 1500] Loss: 3.7665\n",
            "[Step 1510] Loss: 3.4226\n",
            "[Step 1520] Loss: 3.4686\n",
            "[Step 1530] Loss: 3.6905\n",
            "[Step 1540] Loss: 3.7876\n",
            "[Step 1550] Loss: 3.2578\n",
            "[Step 1560] Loss: 3.6844\n",
            "[Step 1570] Loss: 3.5192\n",
            "[Step 1580] Loss: 3.4013\n",
            "[Step 1590] Loss: 3.2044\n",
            "[Step 1600] Loss: 3.5400\n",
            "[Step 1610] Loss: 3.4677\n",
            "[Step 1620] Loss: 3.5848\n",
            "[Step 1630] Loss: 3.7294\n",
            "[Step 1640] Loss: 3.0311\n",
            "[Step 1650] Loss: 3.7022\n",
            "[Step 1660] Loss: 4.3521\n",
            "[Step 1670] Loss: 3.6221\n",
            "[Step 1680] Loss: 3.7146\n",
            "[Step 1690] Loss: 3.4810\n",
            "[Step 1700] Loss: 3.8149\n",
            "[Step 1710] Loss: 3.3475\n",
            "[Step 1720] Loss: 3.7041\n",
            "[Step 1730] Loss: 3.2628\n",
            "[Step 1740] Loss: 3.8273\n",
            "[Step 1750] Loss: 3.7441\n",
            "[Step 1760] Loss: 3.4230\n",
            "[Step 1770] Loss: 3.7903\n",
            "[Step 1780] Loss: 3.8178\n",
            "[Step 1790] Loss: 3.3077\n",
            "[Step 1800] Loss: 3.3508\n",
            "[Step 1810] Loss: 3.1675\n",
            "[Step 1820] Loss: 3.6955\n",
            "[Step 1830] Loss: 3.5086\n",
            "[Step 1840] Loss: 3.6835\n",
            "[Step 1850] Loss: 3.2121\n",
            "[Step 1860] Loss: 3.6278\n",
            "[Step 1870] Loss: 3.8626\n",
            "[Step 1880] Loss: 3.6953\n",
            "[Step 1890] Loss: 3.4962\n",
            "ðŸ“˜ Epoch 21 - Avg Training Loss: 3.6771\n",
            "ðŸ“Š Final Validation â€” Loss: 3.5423 | Accuracy: 0.1538 | Precision: 0.1391\n",
            "[Step 1900] Loss: 3.3124\n",
            "[Step 1910] Loss: 3.6160\n",
            "[Step 1920] Loss: 3.8704\n",
            "[Step 1930] Loss: 3.5250\n",
            "[Step 1940] Loss: 3.2750\n",
            "[Step 1950] Loss: 3.6313\n",
            "[Step 1960] Loss: 3.5791\n",
            "[Step 1970] Loss: 3.4813\n",
            "[Step 1980] Loss: 3.5503\n",
            "[Step 1990] Loss: 3.6530\n",
            "[Step 2000] Loss: 3.4322\n",
            "[Step 2010] Loss: 3.4975\n",
            "[Step 2020] Loss: 3.6714\n",
            "[Step 2030] Loss: 3.4775\n",
            "[Step 2040] Loss: 3.2883\n",
            "[Step 2050] Loss: 3.2199\n",
            "[Step 2060] Loss: 3.6327\n",
            "[Step 2070] Loss: 3.7702\n",
            "[Step 2080] Loss: 3.5442\n",
            "[Step 2090] Loss: 3.4323\n",
            "[Step 2100] Loss: 3.8442\n",
            "[Step 2110] Loss: 3.5539\n",
            "[Step 2120] Loss: 3.6377\n",
            "[Step 2130] Loss: 3.4492\n",
            "[Step 2140] Loss: 3.2494\n",
            "[Step 2150] Loss: 3.3409\n",
            "[Step 2160] Loss: 3.3112\n",
            "[Step 2170] Loss: 3.7800\n",
            "[Step 2180] Loss: 3.5550\n",
            "[Step 2190] Loss: 3.2101\n",
            "[Step 2200] Loss: 3.3356\n",
            "[Step 2210] Loss: 3.4995\n",
            "[Step 2220] Loss: 3.2473\n",
            "[Step 2230] Loss: 3.6178\n",
            "[Step 2240] Loss: 3.5720\n",
            "[Step 2250] Loss: 3.8018\n",
            "[Step 2260] Loss: 3.2318\n",
            "[Step 2270] Loss: 3.8625\n",
            "[Step 2280] Loss: 3.3386\n",
            "[Step 2290] Loss: 3.5503\n",
            "[Step 2300] Loss: 3.1591\n",
            "[Step 2310] Loss: 3.4568\n",
            "[Step 2320] Loss: 3.8368\n",
            "[Step 2330] Loss: 3.0322\n",
            "[Step 2340] Loss: 3.5390\n",
            "[Step 2350] Loss: 3.9473\n",
            "[Step 2360] Loss: 3.5765\n",
            "[Step 2370] Loss: 3.1091\n",
            "[Step 2380] Loss: 3.7867\n",
            "[Step 2390] Loss: 3.2559\n",
            "[Step 2400] Loss: 3.6073\n",
            "[Step 2410] Loss: 3.7702\n",
            "[Step 2420] Loss: 3.5796\n",
            "[Step 2430] Loss: 3.4652\n",
            "[Step 2440] Loss: 3.3765\n",
            "[Step 2450] Loss: 3.4425\n",
            "[Step 2460] Loss: 3.5274\n",
            "[Step 2470] Loss: 3.3141\n",
            "[Step 2480] Loss: 2.9959\n",
            "[Step 2490] Loss: 3.4387\n",
            "[Step 2500] Loss: 3.5870\n",
            "[Step 2510] Loss: 3.9172\n",
            "[Step 2520] Loss: 3.5394\n",
            "[Step 2530] Loss: 3.5473\n",
            "[Step 2540] Loss: 3.5975\n",
            "[Step 2550] Loss: 3.6870\n",
            "[Step 2560] Loss: 3.1435\n",
            "[Step 2570] Loss: 3.2213\n",
            "[Step 2580] Loss: 3.5859\n",
            "[Step 2590] Loss: 3.6319\n",
            "[Step 2600] Loss: 3.3421\n",
            "[Step 2610] Loss: 3.4469\n",
            "[Step 2620] Loss: 2.9641\n",
            "[Step 2630] Loss: 3.7869\n",
            "[Step 2640] Loss: 3.7996\n",
            "[Step 2650] Loss: 3.5677\n",
            "[Step 2660] Loss: 3.3051\n",
            "[Step 2670] Loss: 3.6744\n",
            "[Step 2680] Loss: 3.5525\n",
            "[Step 2690] Loss: 3.6530\n",
            "[Step 2700] Loss: 3.7327\n",
            "[Step 2710] Loss: 3.1686\n",
            "[Step 2720] Loss: 3.3779\n",
            "[Step 2730] Loss: 3.4216\n",
            "[Step 2740] Loss: 3.4757\n",
            "[Step 2750] Loss: 3.2363\n",
            "[Step 2760] Loss: 3.4530\n",
            "[Step 2770] Loss: 3.5568\n",
            "[Step 2780] Loss: 3.4858\n",
            "[Step 2790] Loss: 3.0505\n",
            "[Step 2800] Loss: 3.0813\n",
            "[Step 2810] Loss: 3.3924\n",
            "[Step 2820] Loss: 3.6638\n",
            "[Step 2830] Loss: 3.3871\n",
            "[Step 2840] Loss: 3.4205\n",
            "[Step 2850] Loss: 3.4879\n",
            "[Step 2860] Loss: 3.0752\n",
            "[Step 2870] Loss: 3.4067\n",
            "[Step 2880] Loss: 3.6074\n",
            "[Step 2890] Loss: 3.5375\n",
            "[Step 2900] Loss: 3.4603\n",
            "[Step 2910] Loss: 3.7968\n",
            "[Step 2920] Loss: 3.5621\n",
            "[Step 2930] Loss: 3.4473\n",
            "[Step 2940] Loss: 3.1700\n",
            "[Step 2950] Loss: 3.8808\n",
            "[Step 2960] Loss: 3.7781\n",
            "[Step 2970] Loss: 4.0516\n",
            "[Step 2980] Loss: 3.4658\n",
            "[Step 2990] Loss: 3.4708\n",
            "[Step 3000] Loss: 3.4210\n",
            "[Step 3010] Loss: 3.4110\n",
            "[Step 3020] Loss: 3.3673\n",
            "[Step 3030] Loss: 3.5851\n",
            "[Step 3040] Loss: 3.5342\n",
            "[Step 3050] Loss: 3.4372\n",
            "[Step 3060] Loss: 3.2878\n",
            "[Step 3070] Loss: 3.4771\n",
            "[Step 3080] Loss: 3.4483\n",
            "[Step 3090] Loss: 3.4166\n",
            "[Step 3100] Loss: 3.6344\n",
            "[Step 3110] Loss: 3.5043\n",
            "[Step 3120] Loss: 3.2641\n",
            "[Step 3130] Loss: 3.5614\n",
            "[Step 3140] Loss: 3.4032\n",
            "[Step 3150] Loss: 3.6398\n",
            "[Step 3160] Loss: 3.4385\n",
            "[Step 3170] Loss: 3.1374\n",
            "[Step 3180] Loss: 3.5117\n",
            "[Step 3190] Loss: 3.1878\n",
            "[Step 3200] Loss: 3.1312\n",
            "[Step 3210] Loss: 3.7162\n",
            "[Step 3220] Loss: 3.5903\n",
            "[Step 3230] Loss: 3.5860\n",
            "[Step 3240] Loss: 3.4421\n",
            "[Step 3250] Loss: 3.3401\n",
            "[Step 3260] Loss: 3.6434\n",
            "[Step 3270] Loss: 3.4725\n",
            "[Step 3280] Loss: 3.4196\n",
            "[Step 3290] Loss: 2.9970\n",
            "[Step 3300] Loss: 3.4020\n",
            "[Step 3310] Loss: 3.1340\n",
            "[Step 3320] Loss: 3.4272\n",
            "[Step 3330] Loss: 3.4437\n",
            "[Step 3340] Loss: 3.4524\n",
            "[Step 3350] Loss: 3.6093\n",
            "[Step 3360] Loss: 3.5923\n",
            "[Step 3370] Loss: 3.7138\n",
            "[Step 3380] Loss: 3.2341\n",
            "[Step 3390] Loss: 3.7419\n",
            "[Step 3400] Loss: 3.6034\n",
            "[Step 3410] Loss: 3.2502\n",
            "[Step 3420] Loss: 3.5149\n",
            "[Step 3430] Loss: 3.3105\n",
            "[Step 3440] Loss: 3.3832\n",
            "[Step 3450] Loss: 3.4983\n",
            "[Step 3460] Loss: 3.5515\n",
            "[Step 3470] Loss: 3.5083\n",
            "[Step 3480] Loss: 3.2751\n",
            "[Step 3490] Loss: 3.3391\n",
            "[Step 3500] Loss: 3.0878\n",
            "[Step 3510] Loss: 3.6720\n",
            "[Step 3520] Loss: 3.3229\n",
            "[Step 3530] Loss: 3.5586\n",
            "[Step 3540] Loss: 3.3059\n",
            "[Step 3550] Loss: 3.4036\n",
            "[Step 3560] Loss: 3.5297\n",
            "[Step 3570] Loss: 3.0545\n",
            "[Step 3580] Loss: 3.2221\n",
            "[Step 3590] Loss: 3.3216\n",
            "[Step 3600] Loss: 3.4714\n",
            "[Step 3610] Loss: 3.0575\n",
            "[Step 3620] Loss: 3.4510\n",
            "[Step 3630] Loss: 3.5349\n",
            "[Step 3640] Loss: 3.6611\n",
            "[Step 3650] Loss: 3.8330\n",
            "[Step 3660] Loss: 3.5311\n",
            "[Step 3670] Loss: 3.6605\n",
            "[Step 3680] Loss: 3.6631\n",
            "[Step 3690] Loss: 3.4639\n",
            "[Step 3700] Loss: 3.5190\n",
            "[Step 3710] Loss: 3.0210\n",
            "[Step 3720] Loss: 3.6653\n",
            "[Step 3730] Loss: 2.9403\n",
            "[Step 3740] Loss: 3.6054\n",
            "[Step 3750] Loss: 3.0981\n",
            "[Step 3760] Loss: 3.5692\n",
            "[Step 3770] Loss: 3.3043\n",
            "[Step 3780] Loss: 3.3336\n",
            "ðŸ“˜ Epoch 22 - Avg Training Loss: 3.4537\n",
            "ðŸ“Š Final Validation â€” Loss: 3.4892 | Accuracy: 0.1685 | Precision: 0.1813\n",
            "[Step 3790] Loss: 3.3431\n",
            "[Step 3800] Loss: 3.2238\n",
            "[Step 3810] Loss: 3.2679\n",
            "[Step 3820] Loss: 3.0740\n",
            "[Step 3830] Loss: 3.2459\n",
            "[Step 3840] Loss: 2.8024\n",
            "[Step 3850] Loss: 3.0466\n",
            "[Step 3860] Loss: 3.4248\n",
            "[Step 3870] Loss: 3.2537\n",
            "[Step 3880] Loss: 3.1719\n",
            "[Step 3890] Loss: 2.8158\n",
            "[Step 3900] Loss: 3.2738\n",
            "[Step 3910] Loss: 3.3880\n",
            "[Step 3920] Loss: 2.8305\n",
            "[Step 3930] Loss: 2.7932\n",
            "[Step 3940] Loss: 3.4105\n",
            "[Step 3950] Loss: 2.9999\n",
            "[Step 3960] Loss: 3.7181\n",
            "[Step 3970] Loss: 3.8442\n",
            "[Step 3980] Loss: 3.6121\n",
            "[Step 3990] Loss: 3.6717\n",
            "[Step 4000] Loss: 3.2127\n",
            "[Step 4010] Loss: 3.4301\n",
            "[Step 4020] Loss: 3.9080\n",
            "[Step 4030] Loss: 3.0392\n",
            "[Step 4040] Loss: 2.9129\n",
            "[Step 4050] Loss: 2.9215\n",
            "[Step 4060] Loss: 3.5237\n",
            "[Step 4070] Loss: 3.5225\n",
            "[Step 4080] Loss: 3.1536\n",
            "[Step 4090] Loss: 3.2013\n",
            "[Step 4100] Loss: 3.0095\n",
            "[Step 4110] Loss: 3.2195\n",
            "[Step 4120] Loss: 3.0920\n",
            "[Step 4130] Loss: 3.2635\n",
            "[Step 4140] Loss: 3.3508\n",
            "[Step 4150] Loss: 3.0125\n",
            "[Step 4160] Loss: 3.4051\n",
            "[Step 4170] Loss: 3.1244\n",
            "[Step 4180] Loss: 3.5473\n",
            "[Step 4190] Loss: 3.2779\n",
            "[Step 4200] Loss: 2.8952\n",
            "[Step 4210] Loss: 3.4246\n",
            "[Step 4220] Loss: 3.1271\n",
            "[Step 4230] Loss: 3.0875\n",
            "[Step 4240] Loss: 3.2230\n",
            "[Step 4250] Loss: 3.5308\n",
            "[Step 4260] Loss: 3.8872\n",
            "[Step 4270] Loss: 3.6268\n",
            "[Step 4280] Loss: 3.0792\n",
            "[Step 4290] Loss: 3.2103\n",
            "[Step 4300] Loss: 3.3167\n",
            "[Step 4310] Loss: 3.6261\n",
            "[Step 4320] Loss: 2.8570\n",
            "[Step 4330] Loss: 3.4886\n",
            "[Step 4340] Loss: 3.2943\n",
            "[Step 4350] Loss: 2.9965\n",
            "[Step 4360] Loss: 3.7451\n",
            "[Step 4370] Loss: 3.0582\n",
            "[Step 4380] Loss: 3.1741\n",
            "[Step 4390] Loss: 3.1905\n",
            "[Step 4400] Loss: 3.2182\n",
            "[Step 4410] Loss: 3.3010\n",
            "[Step 4420] Loss: 2.6374\n",
            "[Step 4430] Loss: 2.8663\n",
            "[Step 4440] Loss: 2.9272\n",
            "[Step 4450] Loss: 3.1129\n",
            "[Step 4460] Loss: 3.1683\n",
            "[Step 4470] Loss: 3.3664\n",
            "[Step 4480] Loss: 2.8903\n",
            "[Step 4490] Loss: 2.8529\n",
            "[Step 4500] Loss: 3.2137\n",
            "[Step 4510] Loss: 3.3203\n",
            "[Step 4520] Loss: 3.3427\n",
            "[Step 4530] Loss: 3.2876\n",
            "[Step 4540] Loss: 3.5542\n",
            "[Step 4550] Loss: 2.9814\n",
            "[Step 4560] Loss: 3.7617\n",
            "[Step 4570] Loss: 3.3676\n",
            "[Step 4580] Loss: 3.4151\n",
            "[Step 4590] Loss: 3.3051\n",
            "[Step 4600] Loss: 3.0691\n",
            "[Step 4610] Loss: 3.6879\n",
            "[Step 4620] Loss: 3.0543\n",
            "[Step 4630] Loss: 3.2459\n",
            "[Step 4640] Loss: 3.1519\n",
            "[Step 4650] Loss: 3.2557\n",
            "[Step 4660] Loss: 3.3281\n",
            "[Step 4670] Loss: 2.8394\n",
            "[Step 4680] Loss: 3.0419\n",
            "[Step 4690] Loss: 3.8236\n",
            "[Step 4700] Loss: 3.2014\n",
            "[Step 4710] Loss: 3.4590\n",
            "[Step 4720] Loss: 2.9496\n",
            "[Step 4730] Loss: 3.1922\n",
            "[Step 4740] Loss: 3.5912\n",
            "[Step 4750] Loss: 3.2070\n",
            "[Step 4760] Loss: 3.2641\n",
            "[Step 4770] Loss: 3.4795\n",
            "[Step 4780] Loss: 3.0792\n",
            "[Step 4790] Loss: 3.8085\n",
            "[Step 4800] Loss: 3.2367\n",
            "[Step 4810] Loss: 3.1580\n",
            "[Step 4820] Loss: 3.1279\n",
            "[Step 4830] Loss: 3.7018\n",
            "[Step 4840] Loss: 3.3140\n",
            "[Step 4850] Loss: 3.1523\n",
            "[Step 4860] Loss: 3.1448\n",
            "[Step 4870] Loss: 3.1608\n",
            "[Step 4880] Loss: 2.8903\n",
            "[Step 4890] Loss: 3.5197\n",
            "[Step 4900] Loss: 3.0302\n",
            "[Step 4910] Loss: 3.4942\n",
            "[Step 4920] Loss: 2.7762\n",
            "[Step 4930] Loss: 3.4839\n",
            "[Step 4940] Loss: 3.2295\n",
            "[Step 4950] Loss: 2.9666\n",
            "[Step 4960] Loss: 3.1252\n",
            "[Step 4970] Loss: 3.7007\n",
            "[Step 4980] Loss: 3.2764\n",
            "[Step 4990] Loss: 3.2645\n",
            "[Step 5000] Loss: 3.2182\n",
            "[Step 5010] Loss: 3.6776\n",
            "[Step 5020] Loss: 3.2085\n",
            "[Step 5030] Loss: 3.1781\n",
            "[Step 5040] Loss: 2.9295\n",
            "[Step 5050] Loss: 3.4700\n",
            "[Step 5060] Loss: 3.3677\n",
            "[Step 5070] Loss: 2.8846\n",
            "[Step 5080] Loss: 3.2964\n",
            "[Step 5090] Loss: 3.5394\n",
            "[Step 5100] Loss: 3.2609\n",
            "[Step 5110] Loss: 3.4196\n",
            "[Step 5120] Loss: 3.7230\n",
            "[Step 5130] Loss: 3.1676\n",
            "[Step 5140] Loss: 3.2788\n",
            "[Step 5150] Loss: 3.1945\n",
            "[Step 5160] Loss: 3.6974\n",
            "[Step 5170] Loss: 3.2710\n",
            "[Step 5180] Loss: 2.9659\n",
            "[Step 5190] Loss: 3.5910\n",
            "[Step 5200] Loss: 3.3888\n",
            "[Step 5210] Loss: 3.5862\n",
            "[Step 5220] Loss: 3.2217\n",
            "[Step 5230] Loss: 3.1281\n",
            "[Step 5240] Loss: 2.8845\n",
            "[Step 5250] Loss: 3.3346\n",
            "[Step 5260] Loss: 3.1386\n",
            "[Step 5270] Loss: 3.3357\n",
            "[Step 5280] Loss: 3.5323\n",
            "[Step 5290] Loss: 3.0725\n",
            "[Step 5300] Loss: 2.6778\n",
            "[Step 5310] Loss: 3.1016\n",
            "[Step 5320] Loss: 3.3105\n",
            "[Step 5330] Loss: 3.5467\n",
            "[Step 5340] Loss: 3.2902\n",
            "[Step 5350] Loss: 3.4104\n",
            "[Step 5360] Loss: 3.2534\n",
            "[Step 5370] Loss: 3.2271\n",
            "[Step 5380] Loss: 3.6512\n",
            "[Step 5390] Loss: 3.9170\n",
            "[Step 5400] Loss: 3.4289\n",
            "[Step 5410] Loss: 3.5021\n",
            "[Step 5420] Loss: 3.7240\n",
            "[Step 5430] Loss: 3.2478\n",
            "[Step 5440] Loss: 2.9662\n",
            "[Step 5450] Loss: 3.2147\n",
            "[Step 5460] Loss: 3.2854\n",
            "[Step 5470] Loss: 3.3306\n",
            "[Step 5480] Loss: 3.4884\n",
            "[Step 5490] Loss: 3.4268\n",
            "[Step 5500] Loss: 3.7067\n",
            "[Step 5510] Loss: 3.0069\n",
            "[Step 5520] Loss: 3.5458\n",
            "[Step 5530] Loss: 3.3411\n",
            "[Step 5540] Loss: 3.0934\n",
            "[Step 5550] Loss: 3.3084\n",
            "[Step 5560] Loss: 3.3511\n",
            "[Step 5570] Loss: 2.8297\n",
            "[Step 5580] Loss: 3.1795\n",
            "[Step 5590] Loss: 3.1926\n",
            "[Step 5600] Loss: 3.7418\n",
            "[Step 5610] Loss: 3.0478\n",
            "[Step 5620] Loss: 2.8193\n",
            "[Step 5630] Loss: 3.4368\n",
            "[Step 5640] Loss: 3.0889\n",
            "[Step 5650] Loss: 2.9557\n",
            "[Step 5660] Loss: 3.5080\n",
            "[Step 5670] Loss: 3.1732\n",
            "[Step 5680] Loss: 3.1939\n",
            "ðŸ“˜ Epoch 23 - Avg Training Loss: 3.2771\n",
            "ðŸ“Š Final Validation â€” Loss: 3.3914 | Accuracy: 0.1842 | Precision: 0.1840\n",
            "[Step 5690] Loss: 2.9669\n",
            "[Step 5700] Loss: 3.2173\n",
            "[Step 5710] Loss: 3.1228\n",
            "[Step 5720] Loss: 3.1921\n",
            "[Step 5730] Loss: 3.6599\n",
            "[Step 5740] Loss: 2.5910\n",
            "[Step 5750] Loss: 2.9642\n",
            "[Step 5760] Loss: 3.2277\n",
            "[Step 5770] Loss: 3.1457\n",
            "[Step 5780] Loss: 3.2875\n",
            "[Step 5790] Loss: 3.0209\n",
            "[Step 5800] Loss: 2.9186\n",
            "[Step 5810] Loss: 3.0331\n",
            "[Step 5820] Loss: 2.7908\n",
            "[Step 5830] Loss: 3.2324\n",
            "[Step 5840] Loss: 3.2626\n",
            "[Step 5850] Loss: 3.0760\n",
            "[Step 5860] Loss: 2.6812\n",
            "[Step 5870] Loss: 2.9762\n",
            "[Step 5880] Loss: 3.2991\n",
            "[Step 5890] Loss: 2.9782\n",
            "[Step 5900] Loss: 2.8305\n",
            "[Step 5910] Loss: 3.5943\n",
            "[Step 5920] Loss: 2.6532\n",
            "[Step 5930] Loss: 3.2971\n",
            "[Step 5940] Loss: 3.2454\n",
            "[Step 5950] Loss: 3.1293\n",
            "[Step 5960] Loss: 2.5574\n",
            "[Step 5970] Loss: 3.5276\n",
            "[Step 5980] Loss: 2.8291\n",
            "[Step 5990] Loss: 2.8936\n",
            "[Step 6000] Loss: 3.0171\n",
            "[Step 6010] Loss: 3.2212\n",
            "[Step 6020] Loss: 2.8722\n",
            "[Step 6030] Loss: 2.6060\n",
            "[Step 6040] Loss: 2.8061\n",
            "[Step 6050] Loss: 2.8835\n",
            "[Step 6060] Loss: 3.2246\n",
            "[Step 6070] Loss: 2.7246\n",
            "[Step 6080] Loss: 2.9329\n",
            "[Step 6090] Loss: 3.0434\n",
            "[Step 6100] Loss: 3.1788\n",
            "[Step 6110] Loss: 3.4411\n",
            "[Step 6120] Loss: 3.0936\n",
            "[Step 6130] Loss: 2.9404\n",
            "[Step 6140] Loss: 3.1768\n",
            "[Step 6150] Loss: 2.9781\n",
            "[Step 6160] Loss: 3.0603\n",
            "[Step 6170] Loss: 3.1036\n",
            "[Step 6180] Loss: 3.0509\n",
            "[Step 6190] Loss: 2.9524\n",
            "[Step 6200] Loss: 3.2178\n",
            "[Step 6210] Loss: 3.1647\n",
            "[Step 6220] Loss: 3.1105\n",
            "[Step 6230] Loss: 2.9609\n",
            "[Step 6240] Loss: 3.2651\n",
            "[Step 6250] Loss: 3.3442\n",
            "[Step 6260] Loss: 3.0880\n",
            "[Step 6270] Loss: 3.3156\n",
            "[Step 6280] Loss: 3.1587\n",
            "[Step 6290] Loss: 3.0345\n",
            "[Step 6300] Loss: 3.2851\n",
            "[Step 6310] Loss: 3.1029\n",
            "[Step 6320] Loss: 3.2567\n",
            "[Step 6330] Loss: 3.2325\n",
            "[Step 6340] Loss: 3.3424\n",
            "[Step 6350] Loss: 3.1156\n",
            "[Step 6360] Loss: 3.4080\n",
            "[Step 6370] Loss: 3.2119\n",
            "[Step 6380] Loss: 3.2094\n",
            "[Step 6390] Loss: 3.3266\n",
            "[Step 6400] Loss: 3.2652\n",
            "[Step 6410] Loss: 2.7832\n",
            "[Step 6420] Loss: 3.0803\n",
            "[Step 6430] Loss: 2.7998\n",
            "[Step 6440] Loss: 2.7986\n",
            "[Step 6450] Loss: 2.9314\n",
            "[Step 6460] Loss: 3.2806\n",
            "[Step 6470] Loss: 3.2845\n",
            "[Step 6480] Loss: 2.8044\n",
            "[Step 6490] Loss: 3.2304\n",
            "[Step 6500] Loss: 3.4844\n",
            "[Step 6510] Loss: 2.7453\n",
            "[Step 6520] Loss: 3.1522\n",
            "[Step 6530] Loss: 2.9196\n",
            "[Step 6540] Loss: 2.5604\n",
            "[Step 6550] Loss: 3.5298\n",
            "[Step 6560] Loss: 3.1455\n",
            "[Step 6570] Loss: 3.0417\n",
            "[Step 6580] Loss: 3.1854\n",
            "[Step 6590] Loss: 3.4944\n",
            "[Step 6600] Loss: 2.7209\n",
            "[Step 6610] Loss: 3.2468\n",
            "[Step 6620] Loss: 3.3293\n",
            "[Step 6630] Loss: 2.9948\n",
            "[Step 6640] Loss: 2.7747\n",
            "[Step 6650] Loss: 3.3569\n",
            "[Step 6660] Loss: 2.7917\n",
            "[Step 6670] Loss: 3.2832\n",
            "[Step 6680] Loss: 2.9414\n",
            "[Step 6690] Loss: 3.0695\n",
            "[Step 6700] Loss: 3.0637\n",
            "[Step 6710] Loss: 3.2962\n",
            "[Step 6720] Loss: 2.7885\n",
            "[Step 6730] Loss: 2.9549\n",
            "[Step 6740] Loss: 3.2724\n",
            "[Step 6750] Loss: 3.3462\n",
            "[Step 6760] Loss: 3.0077\n",
            "[Step 6770] Loss: 3.0543\n",
            "[Step 6780] Loss: 2.9479\n",
            "[Step 6790] Loss: 3.1722\n",
            "[Step 6800] Loss: 3.3243\n",
            "[Step 6810] Loss: 3.1024\n",
            "[Step 6820] Loss: 3.2780\n",
            "[Step 6830] Loss: 2.8479\n",
            "[Step 6840] Loss: 3.1821\n",
            "[Step 6850] Loss: 3.4939\n",
            "[Step 6860] Loss: 3.3118\n",
            "[Step 6870] Loss: 3.1704\n",
            "[Step 6880] Loss: 2.9395\n",
            "[Step 6890] Loss: 2.9112\n",
            "[Step 6900] Loss: 3.5621\n",
            "[Step 6910] Loss: 3.5551\n",
            "[Step 6920] Loss: 2.7771\n",
            "[Step 6930] Loss: 3.6653\n",
            "[Step 6940] Loss: 2.9729\n",
            "[Step 6950] Loss: 3.3204\n",
            "[Step 6960] Loss: 2.5387\n",
            "[Step 6970] Loss: 3.1986\n",
            "[Step 6980] Loss: 2.6803\n",
            "[Step 6990] Loss: 2.8419\n",
            "[Step 7000] Loss: 3.5328\n",
            "[Step 7010] Loss: 3.0189\n",
            "[Step 7020] Loss: 3.0420\n",
            "[Step 7030] Loss: 3.2285\n",
            "[Step 7040] Loss: 4.0641\n",
            "[Step 7050] Loss: 3.2805\n",
            "[Step 7060] Loss: 3.1793\n",
            "[Step 7070] Loss: 3.0219\n",
            "[Step 7080] Loss: 3.1128\n",
            "[Step 7090] Loss: 2.7450\n",
            "[Step 7100] Loss: 2.9768\n",
            "[Step 7110] Loss: 3.3551\n",
            "[Step 7120] Loss: 3.6710\n",
            "[Step 7130] Loss: 3.3421\n",
            "[Step 7140] Loss: 2.8853\n",
            "[Step 7150] Loss: 3.1176\n",
            "[Step 7160] Loss: 3.1733\n",
            "[Step 7170] Loss: 2.7090\n",
            "[Step 7180] Loss: 2.8900\n",
            "[Step 7190] Loss: 3.1912\n",
            "[Step 7200] Loss: 3.2729\n",
            "[Step 7210] Loss: 3.1826\n",
            "[Step 7220] Loss: 3.3175\n",
            "[Step 7230] Loss: 2.9785\n",
            "[Step 7240] Loss: 3.1649\n",
            "[Step 7250] Loss: 2.9462\n",
            "[Step 7260] Loss: 3.7119\n",
            "[Step 7270] Loss: 2.7749\n",
            "[Step 7280] Loss: 3.1589\n",
            "[Step 7290] Loss: 2.6694\n",
            "[Step 7300] Loss: 3.4080\n",
            "[Step 7310] Loss: 2.9678\n",
            "[Step 7320] Loss: 3.6321\n",
            "[Step 7330] Loss: 3.0805\n",
            "[Step 7340] Loss: 2.9244\n",
            "[Step 7350] Loss: 3.4393\n",
            "[Step 7360] Loss: 2.9300\n",
            "[Step 7370] Loss: 3.0936\n",
            "[Step 7380] Loss: 3.3202\n",
            "[Step 7390] Loss: 3.1203\n",
            "[Step 7400] Loss: 2.9408\n",
            "[Step 7410] Loss: 3.4277\n",
            "[Step 7420] Loss: 3.0355\n",
            "[Step 7430] Loss: 2.9828\n",
            "[Step 7440] Loss: 3.1724\n",
            "[Step 7450] Loss: 3.0148\n",
            "[Step 7460] Loss: 2.6645\n",
            "[Step 7470] Loss: 3.6098\n",
            "[Step 7480] Loss: 3.0427\n",
            "[Step 7490] Loss: 3.2634\n",
            "[Step 7500] Loss: 2.8779\n",
            "[Step 7510] Loss: 2.9311\n",
            "[Step 7520] Loss: 3.1304\n",
            "[Step 7530] Loss: 2.9927\n",
            "[Step 7540] Loss: 3.2289\n",
            "[Step 7550] Loss: 2.8637\n",
            "[Step 7560] Loss: 3.3610\n",
            "[Step 7570] Loss: 2.7423\n",
            "ðŸ“˜ Epoch 24 - Avg Training Loss: 3.1050\n",
            "ðŸ“Š Final Validation â€” Loss: 3.1272 | Accuracy: 0.2347 | Precision: 0.2305\n",
            "[Step 7580] Loss: 2.8462\n",
            "[Step 7590] Loss: 3.1369\n",
            "[Step 7600] Loss: 3.4008\n",
            "[Step 7610] Loss: 2.9929\n",
            "[Step 7620] Loss: 2.5630\n",
            "[Step 7630] Loss: 3.0249\n",
            "[Step 7640] Loss: 2.8296\n",
            "[Step 7650] Loss: 3.0453\n",
            "[Step 7660] Loss: 3.2399\n",
            "[Step 7670] Loss: 3.3460\n",
            "[Step 7680] Loss: 3.0892\n",
            "[Step 7690] Loss: 2.6106\n",
            "[Step 7700] Loss: 3.0293\n",
            "[Step 7710] Loss: 2.8866\n",
            "[Step 7720] Loss: 3.2534\n",
            "[Step 7730] Loss: 2.4297\n",
            "[Step 7740] Loss: 2.7061\n",
            "[Step 7750] Loss: 3.3691\n",
            "[Step 7760] Loss: 2.6282\n",
            "[Step 7770] Loss: 3.1210\n",
            "[Step 7780] Loss: 3.1953\n",
            "[Step 7790] Loss: 2.8757\n",
            "[Step 7800] Loss: 2.9764\n",
            "[Step 7810] Loss: 2.7894\n",
            "[Step 7820] Loss: 2.4312\n",
            "[Step 7830] Loss: 3.1870\n",
            "[Step 7840] Loss: 3.2124\n",
            "[Step 7850] Loss: 3.1642\n",
            "[Step 7860] Loss: 2.9082\n",
            "[Step 7870] Loss: 2.8606\n",
            "[Step 7880] Loss: 3.1820\n",
            "[Step 7890] Loss: 3.1454\n",
            "[Step 7900] Loss: 2.6876\n",
            "[Step 7910] Loss: 3.1276\n",
            "[Step 7920] Loss: 2.9093\n",
            "[Step 7930] Loss: 2.9039\n",
            "[Step 7940] Loss: 3.0238\n",
            "[Step 7950] Loss: 2.9702\n",
            "[Step 7960] Loss: 3.0089\n",
            "[Step 7970] Loss: 2.8988\n",
            "[Step 7980] Loss: 3.0426\n",
            "[Step 7990] Loss: 3.2293\n",
            "[Step 8000] Loss: 2.9302\n",
            "[Step 8010] Loss: 2.4631\n",
            "[Step 8020] Loss: 2.6552\n",
            "[Step 8030] Loss: 3.0080\n",
            "[Step 8040] Loss: 2.9683\n",
            "[Step 8050] Loss: 2.8695\n",
            "[Step 8060] Loss: 2.9039\n",
            "[Step 8070] Loss: 3.2537\n",
            "[Step 8080] Loss: 3.0764\n",
            "[Step 8090] Loss: 3.1634\n",
            "[Step 8100] Loss: 2.9067\n",
            "[Step 8110] Loss: 2.9030\n",
            "[Step 8120] Loss: 2.7445\n",
            "[Step 8130] Loss: 2.5808\n",
            "[Step 8140] Loss: 2.9687\n",
            "[Step 8150] Loss: 2.4437\n",
            "[Step 8160] Loss: 2.9645\n",
            "[Step 8170] Loss: 2.8672\n",
            "[Step 8180] Loss: 2.9252\n",
            "[Step 8190] Loss: 3.2170\n",
            "[Step 8200] Loss: 2.8052\n",
            "[Step 8210] Loss: 3.0531\n",
            "[Step 8220] Loss: 3.1713\n",
            "[Step 8230] Loss: 3.1072\n",
            "[Step 8240] Loss: 2.8921\n",
            "[Step 8250] Loss: 2.7831\n",
            "[Step 8260] Loss: 3.1591\n",
            "[Step 8270] Loss: 2.7476\n",
            "[Step 8280] Loss: 2.8929\n",
            "[Step 8290] Loss: 2.8253\n",
            "[Step 8300] Loss: 3.0122\n",
            "[Step 8310] Loss: 2.7981\n",
            "[Step 8320] Loss: 3.1438\n",
            "[Step 8330] Loss: 3.2751\n",
            "[Step 8340] Loss: 2.4663\n",
            "[Step 8350] Loss: 2.7460\n",
            "[Step 8360] Loss: 2.9345\n",
            "[Step 8370] Loss: 3.1536\n",
            "[Step 8380] Loss: 3.2491\n",
            "[Step 8390] Loss: 3.3097\n",
            "[Step 8400] Loss: 2.9818\n",
            "[Step 8410] Loss: 3.1576\n",
            "[Step 8420] Loss: 2.7746\n",
            "[Step 8430] Loss: 2.7494\n",
            "[Step 8440] Loss: 2.9400\n",
            "[Step 8450] Loss: 2.8440\n",
            "[Step 8460] Loss: 3.2076\n",
            "[Step 8470] Loss: 2.6806\n",
            "[Step 8480] Loss: 3.2060\n",
            "[Step 8490] Loss: 3.0694\n",
            "[Step 8500] Loss: 2.4569\n",
            "[Step 8510] Loss: 3.0838\n",
            "[Step 8520] Loss: 3.2827\n",
            "[Step 8530] Loss: 3.1598\n",
            "[Step 8540] Loss: 2.9118\n",
            "[Step 8550] Loss: 3.2355\n",
            "[Step 8560] Loss: 3.2962\n",
            "[Step 8570] Loss: 2.6399\n",
            "[Step 8580] Loss: 3.3890\n",
            "[Step 8590] Loss: 2.9994\n",
            "[Step 8600] Loss: 2.7220\n",
            "[Step 8610] Loss: 2.8828\n",
            "[Step 8620] Loss: 3.1004\n",
            "[Step 8630] Loss: 2.5948\n",
            "[Step 8640] Loss: 2.6307\n",
            "[Step 8650] Loss: 3.3341\n",
            "[Step 8660] Loss: 2.8060\n",
            "[Step 8670] Loss: 3.4422\n",
            "[Step 8680] Loss: 2.7065\n",
            "[Step 8690] Loss: 3.0575\n",
            "[Step 8700] Loss: 2.9087\n",
            "[Step 8710] Loss: 3.7252\n",
            "[Step 8720] Loss: 2.6675\n",
            "[Step 8730] Loss: 2.9661\n",
            "[Step 8740] Loss: 2.7670\n",
            "[Step 8750] Loss: 2.9480\n",
            "[Step 8760] Loss: 3.0841\n",
            "[Step 8770] Loss: 3.1630\n",
            "[Step 8780] Loss: 3.0077\n",
            "[Step 8790] Loss: 3.1229\n",
            "[Step 8800] Loss: 2.9406\n",
            "[Step 8810] Loss: 2.7917\n",
            "[Step 8820] Loss: 3.0192\n",
            "[Step 8830] Loss: 2.8696\n",
            "[Step 8840] Loss: 2.8284\n",
            "[Step 8850] Loss: 2.8334\n",
            "[Step 8860] Loss: 2.9724\n",
            "[Step 8870] Loss: 3.0749\n",
            "[Step 8880] Loss: 2.7057\n",
            "[Step 8890] Loss: 2.9785\n",
            "[Step 8900] Loss: 2.8796\n",
            "[Step 8910] Loss: 3.2450\n",
            "[Step 8920] Loss: 2.5989\n",
            "[Step 8930] Loss: 2.9700\n",
            "[Step 8940] Loss: 2.6596\n",
            "[Step 8950] Loss: 2.7569\n",
            "[Step 8960] Loss: 3.0711\n",
            "[Step 8970] Loss: 2.9646\n",
            "[Step 8980] Loss: 2.7982\n",
            "[Step 8990] Loss: 2.7086\n",
            "[Step 9000] Loss: 2.8930\n",
            "[Step 9010] Loss: 3.0949\n",
            "[Step 9020] Loss: 2.6160\n",
            "[Step 9030] Loss: 2.4104\n",
            "[Step 9040] Loss: 2.9898\n",
            "[Step 9050] Loss: 2.9296\n",
            "[Step 9060] Loss: 2.6543\n",
            "[Step 9070] Loss: 3.1785\n",
            "[Step 9080] Loss: 2.7903\n",
            "[Step 9090] Loss: 2.7500\n",
            "[Step 9100] Loss: 2.7970\n",
            "[Step 9110] Loss: 3.1940\n",
            "[Step 9120] Loss: 3.3416\n",
            "[Step 9130] Loss: 3.0963\n",
            "[Step 9140] Loss: 3.2310\n",
            "[Step 9150] Loss: 2.6088\n",
            "[Step 9160] Loss: 3.0331\n",
            "[Step 9170] Loss: 3.0898\n",
            "[Step 9180] Loss: 3.3008\n",
            "[Step 9190] Loss: 2.6413\n",
            "[Step 9200] Loss: 3.0451\n",
            "[Step 9210] Loss: 3.2670\n",
            "[Step 9220] Loss: 3.0319\n",
            "[Step 9230] Loss: 3.4238\n",
            "[Step 9240] Loss: 3.0915\n",
            "[Step 9250] Loss: 2.9547\n",
            "[Step 9260] Loss: 3.0882\n",
            "[Step 9270] Loss: 3.4197\n",
            "[Step 9280] Loss: 2.6316\n",
            "[Step 9290] Loss: 3.0971\n",
            "[Step 9300] Loss: 2.3684\n",
            "[Step 9310] Loss: 3.0528\n",
            "[Step 9320] Loss: 3.3201\n",
            "[Step 9330] Loss: 2.4033\n",
            "[Step 9340] Loss: 2.4743\n",
            "[Step 9350] Loss: 2.6143\n",
            "[Step 9360] Loss: 2.8142\n",
            "[Step 9370] Loss: 3.3565\n",
            "[Step 9380] Loss: 2.8255\n",
            "[Step 9390] Loss: 3.2900\n",
            "[Step 9400] Loss: 3.1141\n",
            "[Step 9410] Loss: 2.4584\n",
            "[Step 9420] Loss: 2.8910\n",
            "[Step 9430] Loss: 3.2149\n",
            "[Step 9440] Loss: 2.7894\n",
            "[Step 9450] Loss: 3.2861\n",
            "[Step 9460] Loss: 2.6300\n",
            "[Step 9470] Loss: 2.6984\n",
            "ðŸ“˜ Epoch 25 - Avg Training Loss: 2.9629\n",
            "ðŸ“Š Final Validation â€” Loss: 2.9400 | Accuracy: 0.2695 | Precision: 0.2434\n",
            "âœ… Continued training complete\n"
          ]
        }
      ],
      "source": [
        "new_training_args = {\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"num_additional_epochs\": 5,\n",
        "    \"logging_steps\": 10,\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=new_training_args[\"learning_rate\"],\n",
        "    weight_decay=new_training_args[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=new_training_args[\"num_additional_epochs\"])\n",
        "\n",
        "starting_epoch = 20\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(starting_epoch, starting_epoch + new_training_args[\"num_additional_epochs\"]):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % new_training_args[\"logging_steps\"] == 0:\n",
        "            print(f\"[Step {global_step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"ðŸ“˜ Epoch {epoch+1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    evaluate_val(model, val_loader, criterion, device)\n",
        "\n",
        "print(\"âœ… Continued training complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3LgBYTRMcuZN",
        "outputId": "3fe49bcf-a759-46d6-9445-c02dae879d26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 10] Loss: 3.2470\n",
            "[Step 20] Loss: 3.7849\n",
            "[Step 30] Loss: 3.0097\n",
            "[Step 40] Loss: 4.0659\n",
            "[Step 50] Loss: 3.4907\n",
            "[Step 60] Loss: 3.2783\n",
            "[Step 70] Loss: 3.2306\n",
            "[Step 80] Loss: 3.1643\n",
            "[Step 90] Loss: 2.8836\n",
            "[Step 100] Loss: 3.5572\n",
            "[Step 110] Loss: 3.3544\n",
            "[Step 120] Loss: 3.6359\n",
            "[Step 130] Loss: 3.4556\n",
            "[Step 140] Loss: 3.0833\n",
            "[Step 150] Loss: 3.0657\n",
            "[Step 160] Loss: 3.4813\n",
            "[Step 170] Loss: 3.7157\n",
            "[Step 180] Loss: 3.0730\n",
            "[Step 190] Loss: 2.7763\n",
            "[Step 200] Loss: 3.4802\n",
            "[Step 210] Loss: 3.9062\n",
            "[Step 220] Loss: 3.6892\n",
            "[Step 230] Loss: 3.9088\n",
            "[Step 240] Loss: 3.3388\n",
            "[Step 250] Loss: 3.1946\n",
            "[Step 260] Loss: 3.3498\n",
            "[Step 270] Loss: 3.3107\n",
            "[Step 280] Loss: 3.2720\n",
            "[Step 290] Loss: 4.0614\n",
            "[Step 300] Loss: 2.9693\n",
            "[Step 310] Loss: 2.8540\n",
            "[Step 320] Loss: 2.9629\n",
            "[Step 330] Loss: 2.6533\n",
            "[Step 340] Loss: 3.2079\n",
            "[Step 350] Loss: 3.5346\n",
            "[Step 360] Loss: 3.2922\n",
            "[Step 370] Loss: 3.2377\n",
            "[Step 380] Loss: 3.4169\n",
            "[Step 390] Loss: 3.4910\n",
            "[Step 400] Loss: 3.0242\n",
            "[Step 410] Loss: 3.3152\n",
            "[Step 420] Loss: 3.3835\n",
            "[Step 430] Loss: 2.8923\n",
            "[Step 440] Loss: 3.0968\n",
            "[Step 450] Loss: 3.3161\n",
            "[Step 460] Loss: 4.0769\n",
            "[Step 470] Loss: 2.9320\n",
            "[Step 480] Loss: 3.5467\n",
            "[Step 490] Loss: 3.2451\n",
            "[Step 500] Loss: 3.1752\n",
            "[Step 510] Loss: 3.2296\n",
            "[Step 520] Loss: 3.3581\n",
            "[Step 530] Loss: 2.9606\n",
            "[Step 540] Loss: 3.7791\n",
            "[Step 550] Loss: 2.8196\n",
            "[Step 560] Loss: 3.4339\n",
            "[Step 570] Loss: 3.3879\n",
            "[Step 580] Loss: 2.8161\n",
            "[Step 590] Loss: 3.0079\n",
            "[Step 600] Loss: 3.1040\n",
            "[Step 610] Loss: 2.9966\n",
            "[Step 620] Loss: 3.5325\n",
            "[Step 630] Loss: 3.1497\n",
            "[Step 640] Loss: 3.6958\n",
            "[Step 650] Loss: 3.6218\n",
            "[Step 660] Loss: 3.0667\n",
            "[Step 670] Loss: 2.9055\n",
            "[Step 680] Loss: 3.3432\n",
            "[Step 690] Loss: 3.2479\n",
            "[Step 700] Loss: 3.4072\n",
            "[Step 710] Loss: 3.1827\n",
            "[Step 720] Loss: 3.3023\n",
            "[Step 730] Loss: 3.4132\n",
            "[Step 740] Loss: 3.0786\n",
            "[Step 750] Loss: 3.0496\n",
            "[Step 760] Loss: 3.2528\n",
            "[Step 770] Loss: 3.3984\n",
            "[Step 780] Loss: 3.2315\n",
            "[Step 790] Loss: 2.8059\n",
            "[Step 800] Loss: 2.9767\n",
            "[Step 810] Loss: 3.5519\n",
            "[Step 820] Loss: 3.5012\n",
            "[Step 830] Loss: 2.9864\n",
            "[Step 840] Loss: 3.5041\n",
            "[Step 850] Loss: 2.7463\n",
            "[Step 860] Loss: 3.2662\n",
            "[Step 870] Loss: 3.3290\n",
            "[Step 880] Loss: 2.8619\n",
            "[Step 890] Loss: 3.2909\n",
            "[Step 900] Loss: 3.0341\n",
            "[Step 910] Loss: 3.2094\n",
            "[Step 920] Loss: 3.3843\n",
            "[Step 930] Loss: 3.2246\n",
            "[Step 940] Loss: 3.6497\n",
            "[Step 950] Loss: 3.4227\n",
            "[Step 960] Loss: 3.4669\n",
            "[Step 970] Loss: 3.4491\n",
            "[Step 980] Loss: 3.1778\n",
            "[Step 990] Loss: 2.8469\n",
            "[Step 1000] Loss: 3.4496\n",
            "[Step 1010] Loss: 3.3572\n",
            "[Step 1020] Loss: 3.4023\n",
            "[Step 1030] Loss: 3.2321\n",
            "[Step 1040] Loss: 3.0016\n",
            "[Step 1050] Loss: 3.5185\n",
            "[Step 1060] Loss: 3.0297\n",
            "[Step 1070] Loss: 3.1331\n",
            "[Step 1080] Loss: 3.0546\n",
            "[Step 1090] Loss: 2.5727\n",
            "[Step 1100] Loss: 4.0305\n",
            "[Step 1110] Loss: 3.2929\n",
            "[Step 1120] Loss: 3.0735\n",
            "[Step 1130] Loss: 3.2704\n",
            "[Step 1140] Loss: 3.3278\n",
            "[Step 1150] Loss: 3.3961\n",
            "[Step 1160] Loss: 3.2067\n",
            "[Step 1170] Loss: 2.9609\n",
            "[Step 1180] Loss: 2.9744\n",
            "[Step 1190] Loss: 3.3091\n",
            "[Step 1200] Loss: 3.0792\n",
            "[Step 1210] Loss: 3.1699\n",
            "[Step 1220] Loss: 3.4042\n",
            "[Step 1230] Loss: 2.7379\n",
            "[Step 1240] Loss: 3.4806\n",
            "[Step 1250] Loss: 3.5235\n",
            "[Step 1260] Loss: 2.7112\n",
            "[Step 1270] Loss: 3.3209\n",
            "[Step 1280] Loss: 3.5040\n",
            "[Step 1290] Loss: 3.1399\n",
            "[Step 1300] Loss: 3.0425\n",
            "[Step 1310] Loss: 3.2596\n",
            "[Step 1320] Loss: 3.0077\n",
            "[Step 1330] Loss: 3.1905\n",
            "[Step 1340] Loss: 3.1910\n",
            "[Step 1350] Loss: 3.0253\n",
            "[Step 1360] Loss: 3.2466\n",
            "[Step 1370] Loss: 3.0586\n",
            "[Step 1380] Loss: 3.6270\n",
            "[Step 1390] Loss: 3.4969\n",
            "[Step 1400] Loss: 3.3519\n",
            "[Step 1410] Loss: 2.7881\n",
            "[Step 1420] Loss: 3.4374\n",
            "[Step 1430] Loss: 3.1973\n",
            "[Step 1440] Loss: 3.3172\n",
            "[Step 1450] Loss: 3.1480\n",
            "[Step 1460] Loss: 3.1791\n",
            "[Step 1470] Loss: 3.4373\n",
            "[Step 1480] Loss: 2.9119\n",
            "[Step 1490] Loss: 3.1688\n",
            "[Step 1500] Loss: 3.2775\n",
            "[Step 1510] Loss: 2.7440\n",
            "[Step 1520] Loss: 3.5074\n",
            "[Step 1530] Loss: 3.4458\n",
            "[Step 1540] Loss: 2.5306\n",
            "[Step 1550] Loss: 2.7562\n",
            "[Step 1560] Loss: 3.4073\n",
            "[Step 1570] Loss: 3.0205\n",
            "[Step 1580] Loss: 3.4484\n",
            "[Step 1590] Loss: 3.4324\n",
            "[Step 1600] Loss: 2.7711\n",
            "[Step 1610] Loss: 3.2566\n",
            "[Step 1620] Loss: 3.3298\n",
            "[Step 1630] Loss: 3.4291\n",
            "[Step 1640] Loss: 3.3850\n",
            "[Step 1650] Loss: 3.1153\n",
            "[Step 1660] Loss: 3.0285\n",
            "[Step 1670] Loss: 3.5600\n",
            "[Step 1680] Loss: 3.5064\n",
            "[Step 1690] Loss: 3.5017\n",
            "[Step 1700] Loss: 3.1797\n",
            "[Step 1710] Loss: 3.0847\n",
            "[Step 1720] Loss: 3.2224\n",
            "[Step 1730] Loss: 3.2498\n",
            "[Step 1740] Loss: 2.8426\n",
            "[Step 1750] Loss: 3.5231\n",
            "[Step 1760] Loss: 3.9608\n",
            "[Step 1770] Loss: 2.7639\n",
            "[Step 1780] Loss: 3.1745\n",
            "[Step 1790] Loss: 3.3958\n",
            "[Step 1800] Loss: 3.1897\n",
            "[Step 1810] Loss: 2.7373\n",
            "[Step 1820] Loss: 3.1564\n",
            "[Step 1830] Loss: 3.9154\n",
            "[Step 1840] Loss: 3.2379\n",
            "[Step 1850] Loss: 3.1438\n",
            "[Step 1860] Loss: 3.7474\n",
            "[Step 1870] Loss: 3.0644\n",
            "[Step 1880] Loss: 3.4601\n",
            "[Step 1890] Loss: 3.8292\n",
            "ðŸ“˜ Epoch 26 - Avg Training Loss: 3.2358\n",
            "ðŸ“Š Final Validation â€” Loss: 3.6604 | Accuracy: 0.1667 | Precision: 0.2185\n",
            "[Step 1900] Loss: 3.1441\n",
            "[Step 1910] Loss: 3.7002\n",
            "[Step 1920] Loss: 3.0970\n",
            "[Step 1930] Loss: 3.0175\n",
            "[Step 1940] Loss: 3.8647\n",
            "[Step 1950] Loss: 3.3006\n",
            "[Step 1960] Loss: 3.2507\n",
            "[Step 1970] Loss: 3.7121\n",
            "[Step 1980] Loss: 3.3600\n",
            "[Step 1990] Loss: 2.7788\n",
            "[Step 2000] Loss: 2.8380\n",
            "[Step 2010] Loss: 3.4508\n",
            "[Step 2020] Loss: 2.8883\n",
            "[Step 2030] Loss: 2.8233\n",
            "[Step 2040] Loss: 3.1474\n",
            "[Step 2050] Loss: 3.2695\n",
            "[Step 2060] Loss: 3.3327\n",
            "[Step 2070] Loss: 3.0599\n",
            "[Step 2080] Loss: 3.7055\n",
            "[Step 2090] Loss: 3.4796\n",
            "[Step 2100] Loss: 2.8894\n",
            "[Step 2110] Loss: 2.9363\n",
            "[Step 2120] Loss: 3.1558\n",
            "[Step 2130] Loss: 3.1330\n",
            "[Step 2140] Loss: 2.9453\n",
            "[Step 2150] Loss: 3.3111\n",
            "[Step 2160] Loss: 3.6924\n",
            "[Step 2170] Loss: 3.1298\n",
            "[Step 2180] Loss: 3.1405\n",
            "[Step 2190] Loss: 3.0416\n",
            "[Step 2200] Loss: 3.3573\n",
            "[Step 2210] Loss: 3.1999\n",
            "[Step 2220] Loss: 3.1639\n",
            "[Step 2230] Loss: 2.9318\n",
            "[Step 2240] Loss: 3.6278\n",
            "[Step 2250] Loss: 3.6229\n",
            "[Step 2260] Loss: 3.0363\n",
            "[Step 2270] Loss: 2.9537\n",
            "[Step 2280] Loss: 3.1104\n",
            "[Step 2290] Loss: 2.8329\n",
            "[Step 2300] Loss: 2.9780\n",
            "[Step 2310] Loss: 2.6122\n",
            "[Step 2320] Loss: 2.9279\n",
            "[Step 2330] Loss: 3.3887\n",
            "[Step 2340] Loss: 3.2451\n",
            "[Step 2350] Loss: 3.3283\n",
            "[Step 2360] Loss: 3.1498\n",
            "[Step 2370] Loss: 3.3633\n",
            "[Step 2380] Loss: 2.9712\n",
            "[Step 2390] Loss: 3.0120\n",
            "[Step 2400] Loss: 3.2809\n",
            "[Step 2410] Loss: 3.1839\n",
            "[Step 2420] Loss: 3.1099\n",
            "[Step 2430] Loss: 3.1687\n",
            "[Step 2440] Loss: 3.3882\n",
            "[Step 2450] Loss: 2.8666\n",
            "[Step 2460] Loss: 3.1772\n",
            "[Step 2470] Loss: 3.4051\n",
            "[Step 2480] Loss: 3.4223\n",
            "[Step 2490] Loss: 3.0621\n",
            "[Step 2500] Loss: 3.5141\n",
            "[Step 2510] Loss: 3.4257\n",
            "[Step 2520] Loss: 3.1812\n",
            "[Step 2530] Loss: 3.3801\n",
            "[Step 2540] Loss: 3.0926\n",
            "[Step 2550] Loss: 2.8971\n",
            "[Step 2560] Loss: 3.2741\n",
            "[Step 2570] Loss: 3.0096\n",
            "[Step 2580] Loss: 3.4385\n",
            "[Step 2590] Loss: 3.4467\n",
            "[Step 2600] Loss: 3.4166\n",
            "[Step 2610] Loss: 2.4802\n",
            "[Step 2620] Loss: 3.1628\n",
            "[Step 2630] Loss: 3.3322\n",
            "[Step 2640] Loss: 3.0313\n",
            "[Step 2650] Loss: 2.6562\n",
            "[Step 2660] Loss: 3.3508\n",
            "[Step 2670] Loss: 3.0958\n",
            "[Step 2680] Loss: 3.0089\n",
            "[Step 2690] Loss: 2.9945\n",
            "[Step 2700] Loss: 2.9528\n",
            "[Step 2710] Loss: 3.3207\n",
            "[Step 2720] Loss: 3.1437\n",
            "[Step 2730] Loss: 2.6989\n",
            "[Step 2740] Loss: 3.1799\n",
            "[Step 2750] Loss: 3.0826\n",
            "[Step 2760] Loss: 3.4080\n",
            "[Step 2770] Loss: 2.7089\n",
            "[Step 2780] Loss: 3.3147\n",
            "[Step 2790] Loss: 3.2615\n",
            "[Step 2800] Loss: 2.9079\n",
            "[Step 2810] Loss: 3.7879\n",
            "[Step 2820] Loss: 2.9644\n",
            "[Step 2830] Loss: 3.1550\n",
            "[Step 2840] Loss: 3.4047\n",
            "[Step 2850] Loss: 2.7867\n",
            "[Step 2860] Loss: 3.1175\n",
            "[Step 2870] Loss: 2.7697\n",
            "[Step 2880] Loss: 3.1821\n",
            "[Step 2890] Loss: 3.0776\n",
            "[Step 2900] Loss: 3.8077\n",
            "[Step 2910] Loss: 3.3786\n",
            "[Step 2920] Loss: 3.0641\n",
            "[Step 2930] Loss: 3.2616\n",
            "[Step 2940] Loss: 3.0193\n",
            "[Step 2950] Loss: 3.4202\n",
            "[Step 2960] Loss: 3.4612\n",
            "[Step 2970] Loss: 2.9159\n",
            "[Step 2980] Loss: 3.3377\n",
            "[Step 2990] Loss: 2.6538\n",
            "[Step 3000] Loss: 3.3817\n",
            "[Step 3010] Loss: 3.3696\n",
            "[Step 3020] Loss: 3.4531\n",
            "[Step 3030] Loss: 3.7423\n",
            "[Step 3040] Loss: 3.0319\n",
            "[Step 3050] Loss: 3.1312\n",
            "[Step 3060] Loss: 2.6391\n",
            "[Step 3070] Loss: 2.9078\n",
            "[Step 3080] Loss: 2.9900\n",
            "[Step 3090] Loss: 2.8690\n",
            "[Step 3100] Loss: 2.3592\n",
            "[Step 3110] Loss: 2.6669\n",
            "[Step 3120] Loss: 3.0891\n",
            "[Step 3130] Loss: 2.8513\n",
            "[Step 3140] Loss: 3.2001\n",
            "[Step 3150] Loss: 3.2230\n",
            "[Step 3160] Loss: 3.5519\n",
            "[Step 3170] Loss: 3.2609\n",
            "[Step 3180] Loss: 3.3131\n",
            "[Step 3190] Loss: 3.1159\n",
            "[Step 3200] Loss: 3.6810\n",
            "[Step 3210] Loss: 2.8935\n",
            "[Step 3220] Loss: 2.9521\n",
            "[Step 3230] Loss: 2.8421\n",
            "[Step 3240] Loss: 3.7140\n",
            "[Step 3250] Loss: 3.0591\n",
            "[Step 3260] Loss: 3.0445\n",
            "[Step 3270] Loss: 3.4627\n",
            "[Step 3280] Loss: 3.2040\n",
            "[Step 3290] Loss: 3.6848\n",
            "[Step 3300] Loss: 3.3576\n",
            "[Step 3310] Loss: 3.1433\n",
            "[Step 3320] Loss: 3.2550\n",
            "[Step 3330] Loss: 3.7674\n",
            "[Step 3340] Loss: 3.6698\n",
            "[Step 3350] Loss: 3.6103\n",
            "[Step 3360] Loss: 2.9574\n",
            "[Step 3370] Loss: 3.2309\n",
            "[Step 3380] Loss: 3.1437\n",
            "[Step 3390] Loss: 3.4562\n",
            "[Step 3400] Loss: 3.4469\n",
            "[Step 3410] Loss: 3.1810\n",
            "[Step 3420] Loss: 3.3545\n",
            "[Step 3430] Loss: 2.7391\n",
            "[Step 3440] Loss: 2.9150\n",
            "[Step 3450] Loss: 2.9820\n",
            "[Step 3460] Loss: 2.8149\n",
            "[Step 3470] Loss: 3.2334\n",
            "[Step 3480] Loss: 2.9242\n",
            "[Step 3490] Loss: 3.0792\n",
            "[Step 3500] Loss: 3.2850\n",
            "[Step 3510] Loss: 3.1799\n",
            "[Step 3520] Loss: 3.1942\n",
            "[Step 3530] Loss: 2.6383\n",
            "[Step 3540] Loss: 3.1548\n",
            "[Step 3550] Loss: 3.2440\n",
            "[Step 3560] Loss: 3.1387\n",
            "[Step 3570] Loss: 3.4400\n",
            "[Step 3580] Loss: 3.1537\n",
            "[Step 3590] Loss: 3.0982\n",
            "[Step 3600] Loss: 3.0421\n",
            "[Step 3610] Loss: 3.2069\n",
            "[Step 3620] Loss: 2.9708\n",
            "[Step 3630] Loss: 3.4355\n",
            "[Step 3640] Loss: 3.6373\n",
            "[Step 3650] Loss: 2.7177\n",
            "[Step 3660] Loss: 3.0331\n",
            "[Step 3670] Loss: 2.8540\n",
            "[Step 3680] Loss: 3.3960\n",
            "[Step 3690] Loss: 3.1547\n",
            "[Step 3700] Loss: 3.3991\n",
            "[Step 3710] Loss: 3.0197\n",
            "[Step 3720] Loss: 2.7590\n",
            "[Step 3730] Loss: 3.3743\n",
            "[Step 3740] Loss: 3.2100\n",
            "[Step 3750] Loss: 3.3517\n",
            "[Step 3760] Loss: 2.8487\n",
            "[Step 3770] Loss: 3.0648\n",
            "[Step 3780] Loss: 3.0131\n",
            "ðŸ“˜ Epoch 27 - Avg Training Loss: 3.1741\n",
            "ðŸ“Š Final Validation â€” Loss: 3.3930 | Accuracy: 0.1821 | Precision: 0.2120\n",
            "[Step 3790] Loss: 3.5001\n",
            "[Step 3800] Loss: 3.6086\n",
            "[Step 3810] Loss: 3.0038\n",
            "[Step 3820] Loss: 2.6730\n",
            "[Step 3830] Loss: 2.8280\n",
            "[Step 3840] Loss: 3.3986\n",
            "[Step 3850] Loss: 2.9895\n",
            "[Step 3860] Loss: 2.9968\n",
            "[Step 3870] Loss: 3.1270\n",
            "[Step 3880] Loss: 3.3291\n",
            "[Step 3890] Loss: 2.4503\n",
            "[Step 3900] Loss: 3.5369\n",
            "[Step 3910] Loss: 2.9957\n",
            "[Step 3920] Loss: 2.9511\n",
            "[Step 3930] Loss: 2.5266\n",
            "[Step 3940] Loss: 2.6565\n",
            "[Step 3950] Loss: 2.9366\n",
            "[Step 3960] Loss: 3.1465\n",
            "[Step 3970] Loss: 3.2419\n",
            "[Step 3980] Loss: 3.2549\n",
            "[Step 3990] Loss: 3.0032\n",
            "[Step 4000] Loss: 2.9064\n",
            "[Step 4010] Loss: 3.0211\n",
            "[Step 4020] Loss: 2.7858\n",
            "[Step 4030] Loss: 2.7867\n",
            "[Step 4040] Loss: 3.2030\n",
            "[Step 4050] Loss: 3.0430\n",
            "[Step 4060] Loss: 3.0844\n",
            "[Step 4070] Loss: 3.3436\n",
            "[Step 4080] Loss: 2.7980\n",
            "[Step 4090] Loss: 2.7504\n",
            "[Step 4100] Loss: 2.8346\n",
            "[Step 4110] Loss: 3.1287\n",
            "[Step 4120] Loss: 3.4146\n",
            "[Step 4130] Loss: 3.1779\n",
            "[Step 4140] Loss: 2.7260\n",
            "[Step 4150] Loss: 2.9338\n",
            "[Step 4160] Loss: 3.0048\n",
            "[Step 4170] Loss: 2.9508\n",
            "[Step 4180] Loss: 3.1381\n",
            "[Step 4190] Loss: 3.5490\n",
            "[Step 4200] Loss: 3.1662\n",
            "[Step 4210] Loss: 2.9718\n",
            "[Step 4220] Loss: 3.0582\n",
            "[Step 4230] Loss: 2.9490\n",
            "[Step 4240] Loss: 3.1731\n",
            "[Step 4250] Loss: 3.3874\n",
            "[Step 4260] Loss: 3.6794\n",
            "[Step 4270] Loss: 3.3287\n",
            "[Step 4280] Loss: 3.2533\n",
            "[Step 4290] Loss: 3.1948\n",
            "[Step 4300] Loss: 3.0816\n",
            "[Step 4310] Loss: 3.3006\n",
            "[Step 4320] Loss: 3.1651\n",
            "[Step 4330] Loss: 3.2606\n",
            "[Step 4340] Loss: 3.3175\n",
            "[Step 4350] Loss: 2.9725\n",
            "[Step 4360] Loss: 3.2405\n",
            "[Step 4370] Loss: 2.7782\n",
            "[Step 4380] Loss: 3.1434\n",
            "[Step 4390] Loss: 3.5126\n",
            "[Step 4400] Loss: 2.9593\n",
            "[Step 4410] Loss: 2.7748\n",
            "[Step 4420] Loss: 2.7723\n",
            "[Step 4430] Loss: 2.9632\n",
            "[Step 4440] Loss: 2.9786\n",
            "[Step 4450] Loss: 3.1362\n",
            "[Step 4460] Loss: 2.9605\n",
            "[Step 4470] Loss: 3.4671\n",
            "[Step 4480] Loss: 3.0311\n",
            "[Step 4490] Loss: 3.7198\n",
            "[Step 4500] Loss: 2.8550\n",
            "[Step 4510] Loss: 3.3269\n",
            "[Step 4520] Loss: 3.0729\n",
            "[Step 4530] Loss: 2.9895\n",
            "[Step 4540] Loss: 2.9956\n",
            "[Step 4550] Loss: 3.7651\n",
            "[Step 4560] Loss: 2.8166\n",
            "[Step 4570] Loss: 3.0331\n",
            "[Step 4580] Loss: 3.4540\n",
            "[Step 4590] Loss: 3.0826\n",
            "[Step 4600] Loss: 3.8333\n",
            "[Step 4610] Loss: 3.9477\n",
            "[Step 4620] Loss: 2.9531\n",
            "[Step 4630] Loss: 2.7628\n",
            "[Step 4640] Loss: 3.0946\n",
            "[Step 4650] Loss: 2.9545\n",
            "[Step 4660] Loss: 2.8834\n",
            "[Step 4670] Loss: 2.9867\n",
            "[Step 4680] Loss: 3.5066\n",
            "[Step 4690] Loss: 3.0708\n",
            "[Step 4700] Loss: 2.8574\n",
            "[Step 4710] Loss: 3.2562\n",
            "[Step 4720] Loss: 2.7794\n",
            "[Step 4730] Loss: 3.1007\n",
            "[Step 4740] Loss: 3.1798\n",
            "[Step 4750] Loss: 3.3064\n",
            "[Step 4760] Loss: 2.7494\n",
            "[Step 4770] Loss: 3.3820\n",
            "[Step 4780] Loss: 3.0054\n",
            "[Step 4790] Loss: 3.2370\n",
            "[Step 4800] Loss: 2.9703\n",
            "[Step 4810] Loss: 2.9446\n",
            "[Step 4820] Loss: 2.9894\n",
            "[Step 4830] Loss: 3.4687\n",
            "[Step 4840] Loss: 3.5113\n",
            "[Step 4850] Loss: 3.4153\n",
            "[Step 4860] Loss: 3.0677\n",
            "[Step 4870] Loss: 3.7436\n",
            "[Step 4880] Loss: 2.8704\n",
            "[Step 4890] Loss: 3.6159\n",
            "[Step 4900] Loss: 3.3810\n",
            "[Step 4910] Loss: 3.0976\n",
            "[Step 4920] Loss: 3.3138\n",
            "[Step 4930] Loss: 3.5209\n",
            "[Step 4940] Loss: 3.1583\n",
            "[Step 4950] Loss: 3.5821\n",
            "[Step 4960] Loss: 2.9426\n",
            "[Step 4970] Loss: 2.5658\n",
            "[Step 4980] Loss: 3.5271\n",
            "[Step 4990] Loss: 3.5356\n",
            "[Step 5000] Loss: 3.0238\n",
            "[Step 5010] Loss: 3.4799\n",
            "[Step 5020] Loss: 3.2236\n",
            "[Step 5030] Loss: 3.1380\n",
            "[Step 5040] Loss: 3.0075\n",
            "[Step 5050] Loss: 3.0609\n",
            "[Step 5060] Loss: 2.9592\n",
            "[Step 5070] Loss: 3.4582\n",
            "[Step 5080] Loss: 3.0702\n",
            "[Step 5090] Loss: 2.8091\n",
            "[Step 5100] Loss: 3.4353\n",
            "[Step 5110] Loss: 2.8192\n",
            "[Step 5120] Loss: 2.5044\n",
            "[Step 5130] Loss: 2.7006\n",
            "[Step 5140] Loss: 3.4116\n",
            "[Step 5150] Loss: 2.8452\n",
            "[Step 5160] Loss: 3.3372\n",
            "[Step 5170] Loss: 3.2385\n",
            "[Step 5180] Loss: 3.8487\n",
            "[Step 5190] Loss: 3.0453\n",
            "[Step 5200] Loss: 2.9792\n",
            "[Step 5210] Loss: 3.0851\n",
            "[Step 5220] Loss: 3.0595\n",
            "[Step 5230] Loss: 3.1423\n",
            "[Step 5240] Loss: 3.1266\n",
            "[Step 5250] Loss: 3.2385\n",
            "[Step 5260] Loss: 3.4737\n",
            "[Step 5270] Loss: 3.1335\n",
            "[Step 5280] Loss: 3.0598\n",
            "[Step 5290] Loss: 3.2551\n",
            "[Step 5300] Loss: 3.0084\n",
            "[Step 5310] Loss: 2.8852\n",
            "[Step 5320] Loss: 3.5995\n",
            "[Step 5330] Loss: 2.9107\n",
            "[Step 5340] Loss: 2.7176\n",
            "[Step 5350] Loss: 3.1186\n",
            "[Step 5360] Loss: 2.9472\n",
            "[Step 5370] Loss: 2.9001\n",
            "[Step 5380] Loss: 3.2401\n",
            "[Step 5390] Loss: 3.1562\n",
            "[Step 5400] Loss: 3.7763\n",
            "[Step 5410] Loss: 2.9133\n",
            "[Step 5420] Loss: 3.4249\n",
            "[Step 5430] Loss: 3.3485\n",
            "[Step 5440] Loss: 2.9869\n",
            "[Step 5450] Loss: 3.2300\n",
            "[Step 5460] Loss: 3.0681\n",
            "[Step 5470] Loss: 2.8562\n",
            "[Step 5480] Loss: 3.2120\n",
            "[Step 5490] Loss: 3.0079\n",
            "[Step 5500] Loss: 3.3328\n",
            "[Step 5510] Loss: 3.0032\n",
            "[Step 5520] Loss: 3.3722\n",
            "[Step 5530] Loss: 2.9073\n",
            "[Step 5540] Loss: 3.0329\n",
            "[Step 5550] Loss: 2.9899\n",
            "[Step 5560] Loss: 2.4025\n",
            "[Step 5570] Loss: 2.6092\n",
            "[Step 5580] Loss: 3.5355\n",
            "[Step 5590] Loss: 2.8676\n",
            "[Step 5600] Loss: 3.2688\n",
            "[Step 5610] Loss: 3.4448\n",
            "[Step 5620] Loss: 3.1540\n",
            "[Step 5630] Loss: 2.9025\n",
            "[Step 5640] Loss: 2.4295\n",
            "[Step 5650] Loss: 2.6377\n",
            "[Step 5660] Loss: 3.2784\n",
            "[Step 5670] Loss: 2.9001\n",
            "[Step 5680] Loss: 3.0614\n",
            "ðŸ“˜ Epoch 28 - Avg Training Loss: 3.0994\n",
            "ðŸ“Š Final Validation â€” Loss: 3.3256 | Accuracy: 0.2009 | Precision: 0.2354\n",
            "[Step 5690] Loss: 2.9520\n",
            "[Step 5700] Loss: 3.2157\n",
            "[Step 5710] Loss: 3.1660\n",
            "[Step 5720] Loss: 2.9534\n",
            "[Step 5730] Loss: 3.2966\n",
            "[Step 5740] Loss: 2.9906\n",
            "[Step 5750] Loss: 2.8504\n",
            "[Step 5760] Loss: 3.2188\n",
            "[Step 5770] Loss: 2.9431\n",
            "[Step 5780] Loss: 2.7350\n",
            "[Step 5790] Loss: 2.9455\n",
            "[Step 5800] Loss: 3.0066\n",
            "[Step 5810] Loss: 2.8993\n",
            "[Step 5820] Loss: 2.6862\n",
            "[Step 5830] Loss: 3.0046\n",
            "[Step 5840] Loss: 3.7522\n",
            "[Step 5850] Loss: 2.6804\n",
            "[Step 5860] Loss: 3.0238\n",
            "[Step 5870] Loss: 2.6650\n",
            "[Step 5880] Loss: 3.1102\n",
            "[Step 5890] Loss: 2.8975\n",
            "[Step 5900] Loss: 2.8572\n",
            "[Step 5910] Loss: 3.1079\n",
            "[Step 5920] Loss: 2.8845\n",
            "[Step 5930] Loss: 2.7089\n",
            "[Step 5940] Loss: 2.8762\n",
            "[Step 5950] Loss: 2.9127\n",
            "[Step 5960] Loss: 3.3021\n",
            "[Step 5970] Loss: 2.8247\n",
            "[Step 5980] Loss: 2.8486\n",
            "[Step 5990] Loss: 2.6831\n",
            "[Step 6000] Loss: 3.4139\n",
            "[Step 6010] Loss: 3.3004\n",
            "[Step 6020] Loss: 3.1359\n",
            "[Step 6030] Loss: 3.1319\n",
            "[Step 6040] Loss: 3.0794\n",
            "[Step 6050] Loss: 3.2910\n",
            "[Step 6060] Loss: 3.3132\n",
            "[Step 6070] Loss: 3.2100\n",
            "[Step 6080] Loss: 2.9403\n",
            "[Step 6090] Loss: 3.0558\n",
            "[Step 6100] Loss: 3.0361\n",
            "[Step 6110] Loss: 3.3933\n",
            "[Step 6120] Loss: 3.1524\n",
            "[Step 6130] Loss: 2.4889\n",
            "[Step 6140] Loss: 2.4709\n",
            "[Step 6150] Loss: 3.3860\n",
            "[Step 6160] Loss: 2.8803\n",
            "[Step 6170] Loss: 2.8330\n",
            "[Step 6180] Loss: 3.2359\n",
            "[Step 6190] Loss: 2.8596\n",
            "[Step 6200] Loss: 2.8947\n",
            "[Step 6210] Loss: 2.7804\n",
            "[Step 6220] Loss: 3.1839\n",
            "[Step 6230] Loss: 2.5138\n",
            "[Step 6240] Loss: 2.9478\n",
            "[Step 6250] Loss: 2.9236\n",
            "[Step 6260] Loss: 2.8560\n",
            "[Step 6270] Loss: 3.0027\n",
            "[Step 6280] Loss: 3.6766\n",
            "[Step 6290] Loss: 2.6800\n",
            "[Step 6300] Loss: 2.3928\n",
            "[Step 6310] Loss: 2.5544\n",
            "[Step 6320] Loss: 3.2987\n",
            "[Step 6330] Loss: 3.0541\n",
            "[Step 6340] Loss: 3.3781\n",
            "[Step 6350] Loss: 3.0587\n",
            "[Step 6360] Loss: 3.2024\n",
            "[Step 6370] Loss: 3.3810\n",
            "[Step 6380] Loss: 3.5010\n",
            "[Step 6390] Loss: 2.9707\n",
            "[Step 6400] Loss: 3.1178\n",
            "[Step 6410] Loss: 3.3783\n",
            "[Step 6420] Loss: 2.5564\n",
            "[Step 6430] Loss: 2.7732\n",
            "[Step 6440] Loss: 3.1796\n",
            "[Step 6450] Loss: 3.1751\n",
            "[Step 6460] Loss: 3.1658\n",
            "[Step 6470] Loss: 3.0490\n",
            "[Step 6480] Loss: 3.3032\n",
            "[Step 6490] Loss: 2.8042\n",
            "[Step 6500] Loss: 3.2147\n",
            "[Step 6510] Loss: 3.3464\n",
            "[Step 6520] Loss: 3.1593\n",
            "[Step 6530] Loss: 2.8665\n",
            "[Step 6540] Loss: 2.9465\n",
            "[Step 6550] Loss: 2.9580\n",
            "[Step 6560] Loss: 3.3101\n",
            "[Step 6570] Loss: 3.3254\n",
            "[Step 6580] Loss: 3.1550\n",
            "[Step 6590] Loss: 3.0625\n",
            "[Step 6600] Loss: 3.0881\n",
            "[Step 6610] Loss: 2.9364\n",
            "[Step 6620] Loss: 3.2058\n",
            "[Step 6630] Loss: 3.0477\n",
            "[Step 6640] Loss: 2.5654\n",
            "[Step 6650] Loss: 2.9748\n",
            "[Step 6660] Loss: 3.0054\n",
            "[Step 6670] Loss: 3.2037\n",
            "[Step 6680] Loss: 2.8438\n",
            "[Step 6690] Loss: 3.2489\n",
            "[Step 6700] Loss: 3.4769\n",
            "[Step 6710] Loss: 3.0934\n",
            "[Step 6720] Loss: 3.1802\n",
            "[Step 6730] Loss: 2.6369\n",
            "[Step 6740] Loss: 2.4791\n",
            "[Step 6750] Loss: 2.8038\n",
            "[Step 6760] Loss: 3.1476\n",
            "[Step 6770] Loss: 3.0065\n",
            "[Step 6780] Loss: 2.9255\n",
            "[Step 6790] Loss: 3.0525\n",
            "[Step 6800] Loss: 2.9615\n",
            "[Step 6810] Loss: 3.0135\n",
            "[Step 6820] Loss: 2.8340\n",
            "[Step 6830] Loss: 3.3414\n",
            "[Step 6840] Loss: 3.0452\n",
            "[Step 6850] Loss: 2.9288\n",
            "[Step 6860] Loss: 2.8864\n",
            "[Step 6870] Loss: 2.6680\n",
            "[Step 6880] Loss: 2.4374\n",
            "[Step 6890] Loss: 2.8583\n",
            "[Step 6900] Loss: 3.4538\n",
            "[Step 6910] Loss: 2.8971\n",
            "[Step 6920] Loss: 3.4077\n",
            "[Step 6930] Loss: 3.6165\n",
            "[Step 6940] Loss: 3.1787\n",
            "[Step 6950] Loss: 3.1064\n",
            "[Step 6960] Loss: 2.6720\n",
            "[Step 6970] Loss: 2.7288\n",
            "[Step 6980] Loss: 3.1448\n",
            "[Step 6990] Loss: 2.9661\n",
            "[Step 7000] Loss: 3.1072\n",
            "[Step 7010] Loss: 2.4937\n",
            "[Step 7020] Loss: 3.5690\n",
            "[Step 7030] Loss: 2.8001\n",
            "[Step 7040] Loss: 3.0821\n",
            "[Step 7050] Loss: 3.6097\n",
            "[Step 7060] Loss: 2.7656\n",
            "[Step 7070] Loss: 3.0685\n",
            "[Step 7080] Loss: 2.7861\n",
            "[Step 7090] Loss: 2.5999\n",
            "[Step 7100] Loss: 2.8966\n",
            "[Step 7110] Loss: 2.7547\n",
            "[Step 7120] Loss: 3.2630\n",
            "[Step 7130] Loss: 3.2522\n",
            "[Step 7140] Loss: 3.2820\n",
            "[Step 7150] Loss: 3.0761\n",
            "[Step 7160] Loss: 3.1144\n",
            "[Step 7170] Loss: 2.9440\n",
            "[Step 7180] Loss: 3.3026\n",
            "[Step 7190] Loss: 3.0739\n",
            "[Step 7200] Loss: 3.0471\n",
            "[Step 7210] Loss: 3.1113\n",
            "[Step 7220] Loss: 3.0449\n",
            "[Step 7230] Loss: 2.9581\n",
            "[Step 7240] Loss: 3.1710\n",
            "[Step 7250] Loss: 3.7106\n",
            "[Step 7260] Loss: 3.3127\n",
            "[Step 7270] Loss: 2.8120\n",
            "[Step 7280] Loss: 2.8251\n",
            "[Step 7290] Loss: 3.1710\n",
            "[Step 7300] Loss: 3.1907\n",
            "[Step 7310] Loss: 3.1932\n",
            "[Step 7320] Loss: 2.9377\n",
            "[Step 7330] Loss: 2.9917\n",
            "[Step 7340] Loss: 2.5869\n",
            "[Step 7350] Loss: 3.0449\n",
            "[Step 7360] Loss: 3.0606\n",
            "[Step 7370] Loss: 2.9558\n",
            "[Step 7380] Loss: 2.9303\n",
            "[Step 7390] Loss: 3.0441\n",
            "[Step 7400] Loss: 3.2034\n",
            "[Step 7410] Loss: 3.1274\n",
            "[Step 7420] Loss: 3.0027\n",
            "[Step 7430] Loss: 3.0223\n",
            "[Step 7440] Loss: 2.9349\n",
            "[Step 7450] Loss: 3.1501\n",
            "[Step 7460] Loss: 3.4283\n",
            "[Step 7470] Loss: 3.3592\n",
            "[Step 7480] Loss: 2.9958\n",
            "[Step 7490] Loss: 3.1648\n",
            "[Step 7500] Loss: 2.9434\n",
            "[Step 7510] Loss: 3.0192\n",
            "[Step 7520] Loss: 2.7639\n",
            "[Step 7530] Loss: 3.5683\n",
            "[Step 7540] Loss: 3.4104\n",
            "[Step 7550] Loss: 3.0568\n",
            "[Step 7560] Loss: 2.5160\n",
            "[Step 7570] Loss: 2.8818\n",
            "ðŸ“˜ Epoch 29 - Avg Training Loss: 3.0205\n",
            "ðŸ“Š Final Validation â€” Loss: 3.1153 | Accuracy: 0.2409 | Precision: 0.2596\n",
            "[Step 7580] Loss: 2.6194\n",
            "[Step 7590] Loss: 3.2258\n",
            "[Step 7600] Loss: 2.9244\n",
            "[Step 7610] Loss: 2.8755\n",
            "[Step 7620] Loss: 2.9766\n",
            "[Step 7630] Loss: 2.8862\n",
            "[Step 7640] Loss: 3.1930\n",
            "[Step 7650] Loss: 2.5273\n",
            "[Step 7660] Loss: 3.0594\n",
            "[Step 7670] Loss: 2.5072\n",
            "[Step 7680] Loss: 3.6535\n",
            "[Step 7690] Loss: 2.7809\n",
            "[Step 7700] Loss: 2.5774\n",
            "[Step 7710] Loss: 2.8542\n",
            "[Step 7720] Loss: 2.6776\n",
            "[Step 7730] Loss: 3.2151\n",
            "[Step 7740] Loss: 3.4169\n",
            "[Step 7750] Loss: 2.6497\n",
            "[Step 7760] Loss: 2.8490\n",
            "[Step 7770] Loss: 2.5830\n",
            "[Step 7780] Loss: 2.5356\n",
            "[Step 7790] Loss: 2.9005\n",
            "[Step 7800] Loss: 2.7640\n",
            "[Step 7810] Loss: 2.5593\n",
            "[Step 7820] Loss: 3.0574\n",
            "[Step 7830] Loss: 2.8674\n",
            "[Step 7840] Loss: 2.7246\n",
            "[Step 7850] Loss: 2.6848\n",
            "[Step 7860] Loss: 3.2895\n",
            "[Step 7870] Loss: 3.0487\n",
            "[Step 7880] Loss: 2.9727\n",
            "[Step 7890] Loss: 2.6065\n",
            "[Step 7900] Loss: 3.0092\n",
            "[Step 7910] Loss: 2.8621\n",
            "[Step 7920] Loss: 2.9012\n",
            "[Step 7930] Loss: 3.0907\n",
            "[Step 7940] Loss: 3.3854\n",
            "[Step 7950] Loss: 3.5151\n",
            "[Step 7960] Loss: 3.1308\n",
            "[Step 7970] Loss: 2.6286\n",
            "[Step 7980] Loss: 2.9683\n",
            "[Step 7990] Loss: 2.8464\n",
            "[Step 8000] Loss: 2.8490\n",
            "[Step 8010] Loss: 2.8479\n",
            "[Step 8020] Loss: 2.5999\n",
            "[Step 8030] Loss: 2.8397\n",
            "[Step 8040] Loss: 2.8678\n",
            "[Step 8050] Loss: 3.3487\n",
            "[Step 8060] Loss: 2.9720\n",
            "[Step 8070] Loss: 3.1657\n",
            "[Step 8080] Loss: 2.9715\n",
            "[Step 8090] Loss: 3.3614\n",
            "[Step 8100] Loss: 3.1738\n",
            "[Step 8110] Loss: 2.7050\n",
            "[Step 8120] Loss: 2.7696\n",
            "[Step 8130] Loss: 2.3203\n",
            "[Step 8140] Loss: 2.8800\n",
            "[Step 8150] Loss: 2.5709\n",
            "[Step 8160] Loss: 2.9468\n",
            "[Step 8170] Loss: 2.8097\n",
            "[Step 8180] Loss: 2.9398\n",
            "[Step 8190] Loss: 2.9311\n",
            "[Step 8200] Loss: 2.8875\n",
            "[Step 8210] Loss: 3.0478\n",
            "[Step 8220] Loss: 2.4174\n",
            "[Step 8230] Loss: 3.2620\n",
            "[Step 8240] Loss: 2.8347\n",
            "[Step 8250] Loss: 2.9661\n",
            "[Step 8260] Loss: 2.7185\n",
            "[Step 8270] Loss: 3.0030\n",
            "[Step 8280] Loss: 2.8418\n",
            "[Step 8290] Loss: 2.8201\n",
            "[Step 8300] Loss: 2.9183\n",
            "[Step 8310] Loss: 2.8435\n",
            "[Step 8320] Loss: 3.1562\n",
            "[Step 8330] Loss: 2.8088\n",
            "[Step 8340] Loss: 2.9426\n",
            "[Step 8350] Loss: 2.8171\n",
            "[Step 8360] Loss: 2.8056\n",
            "[Step 8370] Loss: 2.8747\n",
            "[Step 8380] Loss: 3.2281\n",
            "[Step 8390] Loss: 2.6863\n",
            "[Step 8400] Loss: 2.5051\n",
            "[Step 8410] Loss: 2.5979\n",
            "[Step 8420] Loss: 3.1156\n",
            "[Step 8430] Loss: 2.8844\n",
            "[Step 8440] Loss: 2.5606\n",
            "[Step 8450] Loss: 3.0496\n",
            "[Step 8460] Loss: 2.5299\n",
            "[Step 8470] Loss: 2.9979\n",
            "[Step 8480] Loss: 3.3658\n",
            "[Step 8490] Loss: 2.9288\n",
            "[Step 8500] Loss: 3.2152\n",
            "[Step 8510] Loss: 2.9678\n",
            "[Step 8520] Loss: 2.6672\n",
            "[Step 8530] Loss: 3.2827\n",
            "[Step 8540] Loss: 3.6270\n",
            "[Step 8550] Loss: 2.7882\n",
            "[Step 8560] Loss: 2.4438\n",
            "[Step 8570] Loss: 2.7801\n",
            "[Step 8580] Loss: 3.4057\n",
            "[Step 8590] Loss: 2.9567\n",
            "[Step 8600] Loss: 2.8360\n",
            "[Step 8610] Loss: 3.0723\n",
            "[Step 8620] Loss: 3.4751\n",
            "[Step 8630] Loss: 3.1473\n",
            "[Step 8640] Loss: 2.9556\n",
            "[Step 8650] Loss: 2.8855\n",
            "[Step 8660] Loss: 3.3109\n",
            "[Step 8670] Loss: 3.3182\n",
            "[Step 8680] Loss: 3.1089\n",
            "[Step 8690] Loss: 2.7663\n",
            "[Step 8700] Loss: 2.9784\n",
            "[Step 8710] Loss: 2.7299\n",
            "[Step 8720] Loss: 3.1994\n",
            "[Step 8730] Loss: 2.6162\n",
            "[Step 8740] Loss: 2.8473\n",
            "[Step 8750] Loss: 2.5114\n",
            "[Step 8760] Loss: 2.3997\n",
            "[Step 8770] Loss: 3.2662\n",
            "[Step 8780] Loss: 3.2141\n",
            "[Step 8790] Loss: 2.6864\n",
            "[Step 8800] Loss: 3.0156\n",
            "[Step 8810] Loss: 2.7330\n",
            "[Step 8820] Loss: 2.6428\n",
            "[Step 8830] Loss: 2.9503\n",
            "[Step 8840] Loss: 2.8078\n",
            "[Step 8850] Loss: 2.8987\n",
            "[Step 8860] Loss: 3.0098\n",
            "[Step 8870] Loss: 3.1816\n",
            "[Step 8880] Loss: 3.1785\n",
            "[Step 8890] Loss: 2.7960\n",
            "[Step 8900] Loss: 3.0425\n",
            "[Step 8910] Loss: 2.6745\n",
            "[Step 8920] Loss: 3.2261\n",
            "[Step 8930] Loss: 3.0357\n",
            "[Step 8940] Loss: 2.4844\n",
            "[Step 8950] Loss: 2.7640\n",
            "[Step 8960] Loss: 3.2193\n",
            "[Step 8970] Loss: 3.0033\n",
            "[Step 8980] Loss: 3.0251\n",
            "[Step 8990] Loss: 3.2263\n",
            "[Step 9000] Loss: 2.8319\n",
            "[Step 9010] Loss: 3.0698\n",
            "[Step 9020] Loss: 3.4218\n",
            "[Step 9030] Loss: 2.9970\n",
            "[Step 9040] Loss: 2.6314\n",
            "[Step 9050] Loss: 2.8070\n",
            "[Step 9060] Loss: 3.1007\n",
            "[Step 9070] Loss: 2.7317\n",
            "[Step 9080] Loss: 3.3541\n",
            "[Step 9090] Loss: 2.8103\n",
            "[Step 9100] Loss: 2.7318\n",
            "[Step 9110] Loss: 2.8144\n",
            "[Step 9120] Loss: 2.9604\n",
            "[Step 9130] Loss: 3.1364\n",
            "[Step 9140] Loss: 2.8570\n",
            "[Step 9150] Loss: 2.8566\n",
            "[Step 9160] Loss: 2.6929\n",
            "[Step 9170] Loss: 3.0241\n",
            "[Step 9180] Loss: 3.3976\n",
            "[Step 9190] Loss: 3.4306\n",
            "[Step 9200] Loss: 2.8595\n",
            "[Step 9210] Loss: 2.2839\n",
            "[Step 9220] Loss: 2.8952\n",
            "[Step 9230] Loss: 3.1507\n",
            "[Step 9240] Loss: 2.4645\n",
            "[Step 9250] Loss: 3.0851\n",
            "[Step 9260] Loss: 2.8284\n",
            "[Step 9270] Loss: 2.7401\n",
            "[Step 9280] Loss: 2.8068\n",
            "[Step 9290] Loss: 3.1903\n",
            "[Step 9300] Loss: 2.7202\n",
            "[Step 9310] Loss: 3.0243\n",
            "[Step 9320] Loss: 3.1200\n",
            "[Step 9330] Loss: 2.9718\n",
            "[Step 9340] Loss: 3.0597\n",
            "[Step 9350] Loss: 2.7806\n",
            "[Step 9360] Loss: 2.9068\n",
            "[Step 9370] Loss: 2.4376\n",
            "[Step 9380] Loss: 3.2889\n",
            "[Step 9390] Loss: 2.8124\n",
            "[Step 9400] Loss: 2.9430\n",
            "[Step 9410] Loss: 3.3529\n",
            "[Step 9420] Loss: 2.8754\n",
            "[Step 9430] Loss: 3.1232\n",
            "[Step 9440] Loss: 2.5008\n",
            "[Step 9450] Loss: 2.8201\n",
            "[Step 9460] Loss: 2.7484\n",
            "[Step 9470] Loss: 3.2192\n",
            "ðŸ“˜ Epoch 30 - Avg Training Loss: 2.9316\n",
            "ðŸ“Š Final Validation â€” Loss: 3.0411 | Accuracy: 0.2593 | Precision: 0.2880\n",
            "[Step 9480] Loss: 2.9617\n",
            "[Step 9490] Loss: 3.0956\n",
            "[Step 9500] Loss: 2.6954\n",
            "[Step 9510] Loss: 2.8256\n",
            "[Step 9520] Loss: 2.7648\n",
            "[Step 9530] Loss: 2.9192\n",
            "[Step 9540] Loss: 3.4082\n",
            "[Step 9550] Loss: 3.0615\n",
            "[Step 9560] Loss: 3.0333\n",
            "[Step 9570] Loss: 2.6859\n",
            "[Step 9580] Loss: 2.8243\n",
            "[Step 9590] Loss: 3.0974\n",
            "[Step 9600] Loss: 2.6918\n",
            "[Step 9610] Loss: 3.0299\n",
            "[Step 9620] Loss: 2.4201\n",
            "[Step 9630] Loss: 2.6474\n",
            "[Step 9640] Loss: 2.7761\n",
            "[Step 9650] Loss: 2.5495\n",
            "[Step 9660] Loss: 3.5331\n",
            "[Step 9670] Loss: 2.7397\n",
            "[Step 9680] Loss: 2.7059\n",
            "[Step 9690] Loss: 2.6986\n",
            "[Step 9700] Loss: 2.1128\n",
            "[Step 9710] Loss: 2.8740\n",
            "[Step 9720] Loss: 2.9276\n",
            "[Step 9730] Loss: 3.0504\n",
            "[Step 9740] Loss: 3.3778\n",
            "[Step 9750] Loss: 2.9099\n",
            "[Step 9760] Loss: 2.8981\n",
            "[Step 9770] Loss: 3.2703\n",
            "[Step 9780] Loss: 2.9066\n",
            "[Step 9790] Loss: 3.0583\n",
            "[Step 9800] Loss: 2.4922\n",
            "[Step 9810] Loss: 2.6059\n",
            "[Step 9820] Loss: 3.0239\n",
            "[Step 9830] Loss: 2.6010\n",
            "[Step 9840] Loss: 3.2644\n",
            "[Step 9850] Loss: 2.4917\n",
            "[Step 9860] Loss: 2.4548\n",
            "[Step 9870] Loss: 3.1177\n",
            "[Step 9880] Loss: 2.3875\n",
            "[Step 9890] Loss: 2.5516\n",
            "[Step 9900] Loss: 2.5238\n",
            "[Step 9910] Loss: 2.9385\n",
            "[Step 9920] Loss: 3.3397\n",
            "[Step 9930] Loss: 3.0647\n",
            "[Step 9940] Loss: 2.8854\n",
            "[Step 9950] Loss: 3.0228\n",
            "[Step 9960] Loss: 2.7923\n",
            "[Step 9970] Loss: 3.2111\n",
            "[Step 9980] Loss: 2.6648\n",
            "[Step 9990] Loss: 3.4519\n",
            "[Step 10000] Loss: 3.1129\n",
            "[Step 10010] Loss: 2.9882\n",
            "[Step 10020] Loss: 2.6612\n",
            "[Step 10030] Loss: 2.4597\n",
            "[Step 10040] Loss: 2.7347\n",
            "[Step 10050] Loss: 3.1291\n",
            "[Step 10060] Loss: 3.0525\n",
            "[Step 10070] Loss: 2.8030\n",
            "[Step 10080] Loss: 2.5317\n",
            "[Step 10090] Loss: 2.6459\n",
            "[Step 10100] Loss: 3.0535\n",
            "[Step 10110] Loss: 3.1522\n",
            "[Step 10120] Loss: 3.1210\n",
            "[Step 10130] Loss: 2.7456\n",
            "[Step 10140] Loss: 3.0937\n",
            "[Step 10150] Loss: 3.5679\n",
            "[Step 10160] Loss: 2.5412\n",
            "[Step 10170] Loss: 3.0259\n",
            "[Step 10180] Loss: 3.3276\n",
            "[Step 10190] Loss: 2.8332\n",
            "[Step 10200] Loss: 3.3228\n",
            "[Step 10210] Loss: 2.9688\n",
            "[Step 10220] Loss: 3.0738\n",
            "[Step 10230] Loss: 2.4504\n",
            "[Step 10240] Loss: 3.2104\n",
            "[Step 10250] Loss: 2.8946\n",
            "[Step 10260] Loss: 3.1705\n",
            "[Step 10270] Loss: 3.2266\n",
            "[Step 10280] Loss: 3.0686\n",
            "[Step 10290] Loss: 2.5240\n",
            "[Step 10300] Loss: 2.9246\n",
            "[Step 10310] Loss: 2.5780\n",
            "[Step 10320] Loss: 2.4574\n",
            "[Step 10330] Loss: 3.1448\n",
            "[Step 10340] Loss: 2.3587\n",
            "[Step 10350] Loss: 2.7586\n",
            "[Step 10360] Loss: 2.8465\n",
            "[Step 10370] Loss: 3.4406\n",
            "[Step 10380] Loss: 2.8759\n",
            "[Step 10390] Loss: 2.6742\n",
            "[Step 10400] Loss: 2.8260\n",
            "[Step 10410] Loss: 2.3051\n",
            "[Step 10420] Loss: 2.7315\n",
            "[Step 10430] Loss: 2.8422\n",
            "[Step 10440] Loss: 2.6461\n",
            "[Step 10450] Loss: 2.6431\n",
            "[Step 10460] Loss: 2.8971\n",
            "[Step 10470] Loss: 3.1166\n",
            "[Step 10480] Loss: 3.3239\n",
            "[Step 10490] Loss: 2.6954\n",
            "[Step 10500] Loss: 2.4344\n",
            "[Step 10510] Loss: 2.5345\n",
            "[Step 10520] Loss: 2.7893\n",
            "[Step 10530] Loss: 2.8380\n",
            "[Step 10540] Loss: 2.8283\n",
            "[Step 10550] Loss: 3.0725\n",
            "[Step 10560] Loss: 3.3031\n",
            "[Step 10570] Loss: 2.7110\n",
            "[Step 10580] Loss: 3.0720\n",
            "[Step 10590] Loss: 2.9651\n",
            "[Step 10600] Loss: 3.0689\n",
            "[Step 10610] Loss: 3.2571\n",
            "[Step 10620] Loss: 2.6146\n",
            "[Step 10630] Loss: 3.2516\n",
            "[Step 10640] Loss: 2.5668\n",
            "[Step 10650] Loss: 2.6484\n",
            "[Step 10660] Loss: 2.8615\n",
            "[Step 10670] Loss: 2.7609\n",
            "[Step 10680] Loss: 2.9600\n",
            "[Step 10690] Loss: 3.0630\n",
            "[Step 10700] Loss: 3.1641\n",
            "[Step 10710] Loss: 2.4232\n",
            "[Step 10720] Loss: 2.8984\n",
            "[Step 10730] Loss: 3.3894\n",
            "[Step 10740] Loss: 2.5060\n",
            "[Step 10750] Loss: 3.2065\n",
            "[Step 10760] Loss: 2.7551\n",
            "[Step 10770] Loss: 2.8211\n",
            "[Step 10780] Loss: 3.2834\n",
            "[Step 10790] Loss: 2.6020\n",
            "[Step 10800] Loss: 3.1364\n",
            "[Step 10810] Loss: 2.9562\n",
            "[Step 10820] Loss: 2.4879\n",
            "[Step 10830] Loss: 3.1260\n",
            "[Step 10840] Loss: 2.7508\n",
            "[Step 10850] Loss: 2.8007\n",
            "[Step 10860] Loss: 2.9813\n",
            "[Step 10870] Loss: 3.1932\n",
            "[Step 10880] Loss: 2.7896\n",
            "[Step 10890] Loss: 2.5915\n",
            "[Step 10900] Loss: 2.9986\n",
            "[Step 10910] Loss: 2.9192\n",
            "[Step 10920] Loss: 3.0557\n",
            "[Step 10930] Loss: 2.6891\n",
            "[Step 10940] Loss: 3.0071\n",
            "[Step 10950] Loss: 3.4752\n",
            "[Step 10960] Loss: 3.1988\n",
            "[Step 10970] Loss: 2.8238\n",
            "[Step 10980] Loss: 2.8716\n",
            "[Step 10990] Loss: 2.7276\n",
            "[Step 11000] Loss: 3.0046\n",
            "[Step 11010] Loss: 2.7173\n",
            "[Step 11020] Loss: 2.7411\n",
            "[Step 11030] Loss: 3.0922\n",
            "[Step 11040] Loss: 1.9865\n",
            "[Step 11050] Loss: 3.2669\n",
            "[Step 11060] Loss: 3.1892\n",
            "[Step 11070] Loss: 2.5054\n",
            "[Step 11080] Loss: 3.0891\n",
            "[Step 11090] Loss: 3.0900\n",
            "[Step 11100] Loss: 2.9058\n",
            "[Step 11110] Loss: 2.7917\n",
            "[Step 11120] Loss: 3.2740\n",
            "[Step 11130] Loss: 2.7420\n",
            "[Step 11140] Loss: 2.8667\n",
            "[Step 11150] Loss: 2.6703\n",
            "[Step 11160] Loss: 2.3648\n",
            "[Step 11170] Loss: 2.7180\n",
            "[Step 11180] Loss: 3.3235\n",
            "[Step 11190] Loss: 2.7092\n",
            "[Step 11200] Loss: 2.3911\n",
            "[Step 11210] Loss: 2.9140\n",
            "[Step 11220] Loss: 2.7736\n",
            "[Step 11230] Loss: 3.1529\n",
            "[Step 11240] Loss: 2.5744\n",
            "[Step 11250] Loss: 2.7035\n",
            "[Step 11260] Loss: 2.6878\n",
            "[Step 11270] Loss: 2.7775\n",
            "[Step 11280] Loss: 3.4722\n",
            "[Step 11290] Loss: 2.8182\n",
            "[Step 11300] Loss: 2.6878\n",
            "[Step 11310] Loss: 3.0841\n",
            "[Step 11320] Loss: 3.1359\n",
            "[Step 11330] Loss: 2.5821\n",
            "[Step 11340] Loss: 2.9421\n",
            "[Step 11350] Loss: 2.7031\n",
            "[Step 11360] Loss: 3.1431\n",
            "ðŸ“˜ Epoch 31 - Avg Training Loss: 2.8487\n",
            "ðŸ“Š Final Validation â€” Loss: 2.8454 | Accuracy: 0.2996 | Precision: 0.2955\n",
            "[Step 11370] Loss: 3.0763\n",
            "[Step 11380] Loss: 2.5569\n",
            "[Step 11390] Loss: 2.5323\n",
            "[Step 11400] Loss: 3.3031\n",
            "[Step 11410] Loss: 2.6233\n",
            "[Step 11420] Loss: 2.6587\n",
            "[Step 11430] Loss: 2.8621\n",
            "[Step 11440] Loss: 3.0253\n",
            "[Step 11450] Loss: 2.7574\n",
            "[Step 11460] Loss: 3.1463\n",
            "[Step 11470] Loss: 2.5540\n",
            "[Step 11480] Loss: 2.6523\n",
            "[Step 11490] Loss: 3.1985\n",
            "[Step 11500] Loss: 2.8768\n",
            "[Step 11510] Loss: 3.3088\n",
            "[Step 11520] Loss: 2.7690\n",
            "[Step 11530] Loss: 2.6315\n",
            "[Step 11540] Loss: 3.3442\n",
            "[Step 11550] Loss: 2.7817\n",
            "[Step 11560] Loss: 2.5693\n",
            "[Step 11570] Loss: 3.4104\n",
            "[Step 11580] Loss: 2.3929\n",
            "[Step 11590] Loss: 2.4262\n",
            "[Step 11600] Loss: 3.0575\n",
            "[Step 11610] Loss: 2.6063\n",
            "[Step 11620] Loss: 2.6382\n",
            "[Step 11630] Loss: 2.6365\n",
            "[Step 11640] Loss: 2.7143\n",
            "[Step 11650] Loss: 2.7408\n",
            "[Step 11660] Loss: 3.0670\n",
            "[Step 11670] Loss: 2.9762\n",
            "[Step 11680] Loss: 2.8783\n",
            "[Step 11690] Loss: 2.6180\n",
            "[Step 11700] Loss: 2.7015\n",
            "[Step 11710] Loss: 2.6658\n",
            "[Step 11720] Loss: 2.6576\n",
            "[Step 11730] Loss: 2.6201\n",
            "[Step 11740] Loss: 2.5313\n",
            "[Step 11750] Loss: 2.8710\n",
            "[Step 11760] Loss: 2.9917\n",
            "[Step 11770] Loss: 2.7688\n",
            "[Step 11780] Loss: 2.2780\n",
            "[Step 11790] Loss: 3.0088\n",
            "[Step 11800] Loss: 2.9336\n",
            "[Step 11810] Loss: 3.0133\n",
            "[Step 11820] Loss: 2.5671\n",
            "[Step 11830] Loss: 2.9210\n",
            "[Step 11840] Loss: 2.8434\n",
            "[Step 11850] Loss: 2.2538\n",
            "[Step 11860] Loss: 2.9524\n",
            "[Step 11870] Loss: 2.4430\n",
            "[Step 11880] Loss: 2.9450\n",
            "[Step 11890] Loss: 2.9974\n",
            "[Step 11900] Loss: 2.7564\n",
            "[Step 11910] Loss: 2.4926\n",
            "[Step 11920] Loss: 3.0139\n",
            "[Step 11930] Loss: 2.8059\n",
            "[Step 11940] Loss: 2.7809\n",
            "[Step 11950] Loss: 2.6416\n",
            "[Step 11960] Loss: 2.4967\n",
            "[Step 11970] Loss: 2.6497\n",
            "[Step 11980] Loss: 2.8843\n",
            "[Step 11990] Loss: 2.6054\n",
            "[Step 12000] Loss: 3.1012\n",
            "[Step 12010] Loss: 2.7995\n",
            "[Step 12020] Loss: 2.9791\n",
            "[Step 12030] Loss: 2.5185\n",
            "[Step 12040] Loss: 2.7668\n",
            "[Step 12050] Loss: 2.9258\n",
            "[Step 12060] Loss: 2.3363\n",
            "[Step 12070] Loss: 2.9402\n",
            "[Step 12080] Loss: 2.1918\n",
            "[Step 12090] Loss: 2.6702\n",
            "[Step 12100] Loss: 2.4115\n",
            "[Step 12110] Loss: 2.4703\n",
            "[Step 12120] Loss: 3.0528\n",
            "[Step 12130] Loss: 2.9982\n",
            "[Step 12140] Loss: 2.8763\n",
            "[Step 12150] Loss: 2.8574\n",
            "[Step 12160] Loss: 2.5873\n",
            "[Step 12170] Loss: 2.8329\n",
            "[Step 12180] Loss: 2.5965\n",
            "[Step 12190] Loss: 3.0243\n",
            "[Step 12200] Loss: 2.9943\n",
            "[Step 12210] Loss: 3.1318\n",
            "[Step 12220] Loss: 2.8838\n",
            "[Step 12230] Loss: 2.7705\n",
            "[Step 12240] Loss: 2.6182\n",
            "[Step 12250] Loss: 2.7898\n",
            "[Step 12260] Loss: 3.0485\n",
            "[Step 12270] Loss: 3.0150\n",
            "[Step 12280] Loss: 2.6001\n",
            "[Step 12290] Loss: 2.3411\n",
            "[Step 12300] Loss: 2.9038\n",
            "[Step 12310] Loss: 2.6568\n",
            "[Step 12320] Loss: 3.2643\n",
            "[Step 12330] Loss: 2.6915\n",
            "[Step 12340] Loss: 2.5626\n",
            "[Step 12350] Loss: 2.2510\n",
            "[Step 12360] Loss: 2.5193\n",
            "[Step 12370] Loss: 2.4891\n",
            "[Step 12380] Loss: 2.3781\n",
            "[Step 12390] Loss: 2.9131\n",
            "[Step 12400] Loss: 2.5514\n",
            "[Step 12410] Loss: 2.7498\n",
            "[Step 12420] Loss: 2.2478\n",
            "[Step 12430] Loss: 2.0114\n",
            "[Step 12440] Loss: 2.4543\n",
            "[Step 12450] Loss: 3.1523\n",
            "[Step 12460] Loss: 2.6075\n",
            "[Step 12470] Loss: 2.8250\n",
            "[Step 12480] Loss: 2.3992\n",
            "[Step 12490] Loss: 2.5687\n",
            "[Step 12500] Loss: 3.2902\n",
            "[Step 12510] Loss: 3.0516\n",
            "[Step 12520] Loss: 2.8471\n",
            "[Step 12530] Loss: 3.0416\n",
            "[Step 12540] Loss: 2.7509\n",
            "[Step 12550] Loss: 2.5368\n",
            "[Step 12560] Loss: 2.8318\n",
            "[Step 12570] Loss: 2.3282\n",
            "[Step 12580] Loss: 3.2304\n",
            "[Step 12590] Loss: 2.3972\n",
            "[Step 12600] Loss: 2.6271\n",
            "[Step 12610] Loss: 2.4202\n",
            "[Step 12620] Loss: 2.7410\n",
            "[Step 12630] Loss: 3.2741\n",
            "[Step 12640] Loss: 2.8656\n",
            "[Step 12650] Loss: 2.3873\n",
            "[Step 12660] Loss: 2.9682\n",
            "[Step 12670] Loss: 2.6801\n",
            "[Step 12680] Loss: 2.6832\n",
            "[Step 12690] Loss: 3.1442\n",
            "[Step 12700] Loss: 2.4288\n",
            "[Step 12710] Loss: 2.9609\n",
            "[Step 12720] Loss: 2.4149\n",
            "[Step 12730] Loss: 2.8480\n",
            "[Step 12740] Loss: 3.2508\n",
            "[Step 12750] Loss: 2.2772\n",
            "[Step 12760] Loss: 2.8233\n",
            "[Step 12770] Loss: 2.4065\n",
            "[Step 12780] Loss: 2.0665\n",
            "[Step 12790] Loss: 2.6523\n",
            "[Step 12800] Loss: 3.1016\n",
            "[Step 12810] Loss: 2.4120\n",
            "[Step 12820] Loss: 3.2581\n",
            "[Step 12830] Loss: 3.0053\n",
            "[Step 12840] Loss: 2.9152\n",
            "[Step 12850] Loss: 2.5977\n",
            "[Step 12860] Loss: 2.8082\n",
            "[Step 12870] Loss: 2.8952\n",
            "[Step 12880] Loss: 2.7761\n",
            "[Step 12890] Loss: 3.1594\n",
            "[Step 12900] Loss: 2.3342\n",
            "[Step 12910] Loss: 2.6815\n",
            "[Step 12920] Loss: 2.8204\n",
            "[Step 12930] Loss: 2.9951\n",
            "[Step 12940] Loss: 2.5455\n",
            "[Step 12950] Loss: 2.6257\n",
            "[Step 12960] Loss: 2.5084\n",
            "[Step 12970] Loss: 2.6334\n",
            "[Step 12980] Loss: 2.6627\n",
            "[Step 12990] Loss: 2.8130\n",
            "[Step 13000] Loss: 2.8099\n",
            "[Step 13010] Loss: 2.8643\n",
            "[Step 13020] Loss: 2.9483\n",
            "[Step 13030] Loss: 2.6649\n",
            "[Step 13040] Loss: 2.4431\n",
            "[Step 13050] Loss: 2.5925\n",
            "[Step 13060] Loss: 2.4437\n",
            "[Step 13070] Loss: 2.9893\n",
            "[Step 13080] Loss: 2.5046\n",
            "[Step 13090] Loss: 2.6472\n",
            "[Step 13100] Loss: 2.7164\n",
            "[Step 13110] Loss: 2.4291\n",
            "[Step 13120] Loss: 2.5477\n",
            "[Step 13130] Loss: 3.0183\n",
            "[Step 13140] Loss: 2.1645\n",
            "[Step 13150] Loss: 3.0939\n",
            "[Step 13160] Loss: 2.3612\n",
            "[Step 13170] Loss: 2.7802\n",
            "[Step 13180] Loss: 2.6620\n",
            "[Step 13190] Loss: 3.0000\n",
            "[Step 13200] Loss: 2.7945\n",
            "[Step 13210] Loss: 2.3082\n",
            "[Step 13220] Loss: 3.3779\n",
            "[Step 13230] Loss: 2.7845\n",
            "[Step 13240] Loss: 2.8804\n",
            "[Step 13250] Loss: 2.5507\n",
            "ðŸ“˜ Epoch 32 - Avg Training Loss: 2.7549\n",
            "ðŸ“Š Final Validation â€” Loss: 2.8422 | Accuracy: 0.2981 | Precision: 0.3012\n",
            "[Step 13260] Loss: 2.6022\n",
            "[Step 13270] Loss: 2.9151\n",
            "[Step 13280] Loss: 2.8740\n",
            "[Step 13290] Loss: 2.5841\n",
            "[Step 13300] Loss: 2.5841\n",
            "[Step 13310] Loss: 2.4660\n",
            "[Step 13320] Loss: 2.8594\n",
            "[Step 13330] Loss: 2.4136\n",
            "[Step 13340] Loss: 2.4389\n",
            "[Step 13350] Loss: 2.9932\n",
            "[Step 13360] Loss: 2.8433\n",
            "[Step 13370] Loss: 2.9364\n",
            "[Step 13380] Loss: 2.7075\n",
            "[Step 13390] Loss: 3.2203\n",
            "[Step 13400] Loss: 2.4782\n",
            "[Step 13410] Loss: 2.9723\n",
            "[Step 13420] Loss: 2.3903\n",
            "[Step 13430] Loss: 2.9851\n",
            "[Step 13440] Loss: 2.9430\n",
            "[Step 13450] Loss: 2.8781\n",
            "[Step 13460] Loss: 2.6408\n",
            "[Step 13470] Loss: 2.9290\n",
            "[Step 13480] Loss: 2.5378\n",
            "[Step 13490] Loss: 2.4355\n",
            "[Step 13500] Loss: 2.6247\n",
            "[Step 13510] Loss: 3.1214\n",
            "[Step 13520] Loss: 2.4748\n",
            "[Step 13530] Loss: 2.4503\n",
            "[Step 13540] Loss: 2.8055\n",
            "[Step 13550] Loss: 2.8633\n",
            "[Step 13560] Loss: 2.5661\n",
            "[Step 13570] Loss: 2.9483\n",
            "[Step 13580] Loss: 3.1823\n",
            "[Step 13590] Loss: 2.9481\n",
            "[Step 13600] Loss: 3.0219\n",
            "[Step 13610] Loss: 3.1088\n",
            "[Step 13620] Loss: 2.5974\n",
            "[Step 13630] Loss: 2.7644\n",
            "[Step 13640] Loss: 3.3054\n",
            "[Step 13650] Loss: 2.4850\n",
            "[Step 13660] Loss: 2.5164\n",
            "[Step 13670] Loss: 2.4448\n",
            "[Step 13680] Loss: 2.9178\n",
            "[Step 13690] Loss: 2.6740\n",
            "[Step 13700] Loss: 2.3050\n",
            "[Step 13710] Loss: 3.4660\n",
            "[Step 13720] Loss: 2.8992\n",
            "[Step 13730] Loss: 2.6115\n",
            "[Step 13740] Loss: 2.6709\n",
            "[Step 13750] Loss: 2.8076\n",
            "[Step 13760] Loss: 2.5336\n",
            "[Step 13770] Loss: 2.5350\n",
            "[Step 13780] Loss: 2.5922\n",
            "[Step 13790] Loss: 2.6420\n",
            "[Step 13800] Loss: 2.1377\n",
            "[Step 13810] Loss: 2.7982\n",
            "[Step 13820] Loss: 2.0538\n",
            "[Step 13830] Loss: 2.8045\n",
            "[Step 13840] Loss: 2.6476\n",
            "[Step 13850] Loss: 2.9021\n",
            "[Step 13860] Loss: 2.5232\n",
            "[Step 13870] Loss: 2.5624\n",
            "[Step 13880] Loss: 2.4500\n",
            "[Step 13890] Loss: 3.0248\n",
            "[Step 13900] Loss: 2.4678\n",
            "[Step 13910] Loss: 2.5999\n",
            "[Step 13920] Loss: 2.3908\n",
            "[Step 13930] Loss: 2.9340\n",
            "[Step 13940] Loss: 2.5799\n",
            "[Step 13950] Loss: 2.7886\n",
            "[Step 13960] Loss: 2.5603\n",
            "[Step 13970] Loss: 2.7008\n",
            "[Step 13980] Loss: 2.8659\n",
            "[Step 13990] Loss: 2.5527\n",
            "[Step 14000] Loss: 2.4377\n",
            "[Step 14010] Loss: 2.5302\n",
            "[Step 14020] Loss: 2.7232\n",
            "[Step 14030] Loss: 2.5959\n",
            "[Step 14040] Loss: 2.1346\n",
            "[Step 14050] Loss: 2.2795\n",
            "[Step 14060] Loss: 2.4687\n",
            "[Step 14070] Loss: 2.7066\n",
            "[Step 14080] Loss: 2.6432\n",
            "[Step 14090] Loss: 2.3974\n",
            "[Step 14100] Loss: 2.4018\n",
            "[Step 14110] Loss: 2.3191\n",
            "[Step 14120] Loss: 2.7852\n",
            "[Step 14130] Loss: 3.2317\n",
            "[Step 14140] Loss: 2.7845\n",
            "[Step 14150] Loss: 2.8384\n",
            "[Step 14160] Loss: 2.7907\n",
            "[Step 14170] Loss: 2.4715\n",
            "[Step 14180] Loss: 2.7874\n",
            "[Step 14190] Loss: 3.3592\n",
            "[Step 14200] Loss: 2.5363\n",
            "[Step 14210] Loss: 2.8790\n",
            "[Step 14220] Loss: 2.6937\n",
            "[Step 14230] Loss: 2.6425\n",
            "[Step 14240] Loss: 2.6211\n",
            "[Step 14250] Loss: 2.5785\n",
            "[Step 14260] Loss: 2.6415\n",
            "[Step 14270] Loss: 2.9322\n",
            "[Step 14280] Loss: 2.5885\n",
            "[Step 14290] Loss: 2.6035\n",
            "[Step 14300] Loss: 2.7746\n",
            "[Step 14310] Loss: 2.9283\n",
            "[Step 14320] Loss: 2.6337\n",
            "[Step 14330] Loss: 2.8038\n",
            "[Step 14340] Loss: 2.6979\n",
            "[Step 14350] Loss: 2.5062\n",
            "[Step 14360] Loss: 2.2385\n",
            "[Step 14370] Loss: 2.5951\n",
            "[Step 14380] Loss: 2.3763\n",
            "[Step 14390] Loss: 2.6635\n",
            "[Step 14400] Loss: 2.7217\n",
            "[Step 14410] Loss: 3.0044\n",
            "[Step 14420] Loss: 2.1674\n",
            "[Step 14430] Loss: 2.4397\n",
            "[Step 14440] Loss: 3.3212\n",
            "[Step 14450] Loss: 2.7819\n",
            "[Step 14460] Loss: 2.3674\n",
            "[Step 14470] Loss: 2.1801\n",
            "[Step 14480] Loss: 2.9293\n",
            "[Step 14490] Loss: 2.9349\n",
            "[Step 14500] Loss: 2.6490\n",
            "[Step 14510] Loss: 2.5782\n",
            "[Step 14520] Loss: 2.8835\n",
            "[Step 14530] Loss: 2.8200\n",
            "[Step 14540] Loss: 2.5617\n",
            "[Step 14550] Loss: 2.8957\n",
            "[Step 14560] Loss: 3.1112\n",
            "[Step 14570] Loss: 2.7273\n",
            "[Step 14580] Loss: 2.3730\n",
            "[Step 14590] Loss: 2.8298\n",
            "[Step 14600] Loss: 2.6906\n",
            "[Step 14610] Loss: 2.2127\n",
            "[Step 14620] Loss: 2.4816\n",
            "[Step 14630] Loss: 3.1553\n",
            "[Step 14640] Loss: 2.9321\n",
            "[Step 14650] Loss: 2.9431\n",
            "[Step 14660] Loss: 2.6327\n",
            "[Step 14670] Loss: 2.2873\n",
            "[Step 14680] Loss: 2.6357\n",
            "[Step 14690] Loss: 2.5575\n",
            "[Step 14700] Loss: 2.2782\n",
            "[Step 14710] Loss: 3.0429\n",
            "[Step 14720] Loss: 2.7590\n",
            "[Step 14730] Loss: 2.6331\n",
            "[Step 14740] Loss: 2.7988\n",
            "[Step 14750] Loss: 2.6948\n",
            "[Step 14760] Loss: 2.7813\n",
            "[Step 14770] Loss: 2.3739\n",
            "[Step 14780] Loss: 3.4550\n",
            "[Step 14790] Loss: 2.9835\n",
            "[Step 14800] Loss: 2.3738\n",
            "[Step 14810] Loss: 3.0624\n",
            "[Step 14820] Loss: 2.2964\n",
            "[Step 14830] Loss: 2.5154\n",
            "[Step 14840] Loss: 2.1421\n",
            "[Step 14850] Loss: 2.6602\n",
            "[Step 14860] Loss: 2.7978\n",
            "[Step 14870] Loss: 2.8178\n",
            "[Step 14880] Loss: 2.3818\n",
            "[Step 14890] Loss: 2.6355\n",
            "[Step 14900] Loss: 2.9985\n",
            "[Step 14910] Loss: 3.1291\n",
            "[Step 14920] Loss: 2.9973\n",
            "[Step 14930] Loss: 2.7450\n",
            "[Step 14940] Loss: 3.0105\n",
            "[Step 14950] Loss: 2.6832\n",
            "[Step 14960] Loss: 3.2137\n",
            "[Step 14970] Loss: 2.7854\n",
            "[Step 14980] Loss: 2.7867\n",
            "[Step 14990] Loss: 2.5483\n",
            "[Step 15000] Loss: 2.3773\n",
            "[Step 15010] Loss: 2.7274\n",
            "[Step 15020] Loss: 2.4770\n",
            "[Step 15030] Loss: 2.2611\n",
            "[Step 15040] Loss: 2.2565\n",
            "[Step 15050] Loss: 2.3169\n",
            "[Step 15060] Loss: 2.7434\n",
            "[Step 15070] Loss: 2.7916\n",
            "[Step 15080] Loss: 2.5798\n",
            "[Step 15090] Loss: 2.7336\n",
            "[Step 15100] Loss: 2.5753\n",
            "[Step 15110] Loss: 2.5452\n",
            "[Step 15120] Loss: 2.4664\n",
            "[Step 15130] Loss: 2.3648\n",
            "[Step 15140] Loss: 2.6702\n",
            "[Step 15150] Loss: 2.9122\n",
            "ðŸ“˜ Epoch 33 - Avg Training Loss: 2.6690\n",
            "ðŸ“Š Final Validation â€” Loss: 2.7113 | Accuracy: 0.3304 | Precision: 0.3127\n",
            "[Step 15160] Loss: 2.1485\n",
            "[Step 15170] Loss: 2.5189\n",
            "[Step 15180] Loss: 2.4415\n",
            "[Step 15190] Loss: 2.1932\n",
            "[Step 15200] Loss: 2.3447\n",
            "[Step 15210] Loss: 2.5692\n",
            "[Step 15220] Loss: 2.5361\n",
            "[Step 15230] Loss: 2.5328\n",
            "[Step 15240] Loss: 2.2978\n",
            "[Step 15250] Loss: 2.6926\n",
            "[Step 15260] Loss: 2.3329\n",
            "[Step 15270] Loss: 2.8602\n",
            "[Step 15280] Loss: 2.6094\n",
            "[Step 15290] Loss: 3.0098\n",
            "[Step 15300] Loss: 2.4970\n",
            "[Step 15310] Loss: 2.7374\n",
            "[Step 15320] Loss: 2.5689\n",
            "[Step 15330] Loss: 2.1556\n",
            "[Step 15340] Loss: 2.9274\n",
            "[Step 15350] Loss: 2.6080\n",
            "[Step 15360] Loss: 2.5509\n",
            "[Step 15370] Loss: 2.6609\n",
            "[Step 15380] Loss: 2.2487\n",
            "[Step 15390] Loss: 2.7047\n",
            "[Step 15400] Loss: 2.5965\n",
            "[Step 15410] Loss: 2.6464\n",
            "[Step 15420] Loss: 2.7339\n",
            "[Step 15430] Loss: 2.2491\n",
            "[Step 15440] Loss: 2.6906\n",
            "[Step 15450] Loss: 3.0548\n",
            "[Step 15460] Loss: 2.3796\n",
            "[Step 15470] Loss: 2.3020\n",
            "[Step 15480] Loss: 2.7833\n",
            "[Step 15490] Loss: 2.7980\n",
            "[Step 15500] Loss: 2.5122\n",
            "[Step 15510] Loss: 2.9201\n",
            "[Step 15520] Loss: 2.5031\n",
            "[Step 15530] Loss: 2.5515\n",
            "[Step 15540] Loss: 2.6756\n",
            "[Step 15550] Loss: 2.8122\n",
            "[Step 15560] Loss: 2.5972\n",
            "[Step 15570] Loss: 2.5058\n",
            "[Step 15580] Loss: 2.6861\n",
            "[Step 15590] Loss: 2.4034\n",
            "[Step 15600] Loss: 2.3580\n",
            "[Step 15610] Loss: 2.4507\n",
            "[Step 15620] Loss: 2.3527\n",
            "[Step 15630] Loss: 2.3510\n",
            "[Step 15640] Loss: 2.6334\n",
            "[Step 15650] Loss: 2.2881\n",
            "[Step 15660] Loss: 2.5811\n",
            "[Step 15670] Loss: 2.3486\n",
            "[Step 15680] Loss: 2.4190\n",
            "[Step 15690] Loss: 2.6661\n",
            "[Step 15700] Loss: 2.3693\n",
            "[Step 15710] Loss: 2.3844\n",
            "[Step 15720] Loss: 2.9916\n",
            "[Step 15730] Loss: 2.5827\n",
            "[Step 15740] Loss: 3.0006\n",
            "[Step 15750] Loss: 2.7572\n",
            "[Step 15760] Loss: 2.5596\n",
            "[Step 15770] Loss: 2.9458\n",
            "[Step 15780] Loss: 3.1725\n",
            "[Step 15790] Loss: 2.4088\n",
            "[Step 15800] Loss: 2.3290\n",
            "[Step 15810] Loss: 2.6005\n",
            "[Step 15820] Loss: 2.6693\n",
            "[Step 15830] Loss: 2.7426\n",
            "[Step 15840] Loss: 2.7855\n",
            "[Step 15850] Loss: 2.7734\n",
            "[Step 15860] Loss: 2.4151\n",
            "[Step 15870] Loss: 3.0895\n",
            "[Step 15880] Loss: 2.4008\n",
            "[Step 15890] Loss: 3.2942\n",
            "[Step 15900] Loss: 2.6945\n",
            "[Step 15910] Loss: 2.9165\n",
            "[Step 15920] Loss: 2.7666\n",
            "[Step 15930] Loss: 2.6765\n",
            "[Step 15940] Loss: 3.1947\n",
            "[Step 15950] Loss: 2.3851\n",
            "[Step 15960] Loss: 2.7927\n",
            "[Step 15970] Loss: 2.3639\n",
            "[Step 15980] Loss: 2.2206\n",
            "[Step 15990] Loss: 2.5824\n",
            "[Step 16000] Loss: 2.5675\n",
            "[Step 16010] Loss: 3.1178\n",
            "[Step 16020] Loss: 2.2766\n",
            "[Step 16030] Loss: 2.3615\n",
            "[Step 16040] Loss: 2.9075\n",
            "[Step 16050] Loss: 2.3069\n",
            "[Step 16060] Loss: 2.7884\n",
            "[Step 16070] Loss: 2.5045\n",
            "[Step 16080] Loss: 2.6622\n",
            "[Step 16090] Loss: 2.7463\n",
            "[Step 16100] Loss: 3.0647\n",
            "[Step 16110] Loss: 2.1448\n",
            "[Step 16120] Loss: 2.5018\n",
            "[Step 16130] Loss: 2.4822\n",
            "[Step 16140] Loss: 1.9071\n",
            "[Step 16150] Loss: 2.0149\n",
            "[Step 16160] Loss: 2.3332\n",
            "[Step 16170] Loss: 2.5440\n",
            "[Step 16180] Loss: 2.4361\n",
            "[Step 16190] Loss: 2.5119\n",
            "[Step 16200] Loss: 2.3746\n",
            "[Step 16210] Loss: 2.8868\n",
            "[Step 16220] Loss: 2.2112\n",
            "[Step 16230] Loss: 2.9615\n",
            "[Step 16240] Loss: 2.4567\n",
            "[Step 16250] Loss: 2.9278\n",
            "[Step 16260] Loss: 2.3988\n",
            "[Step 16270] Loss: 2.9626\n",
            "[Step 16280] Loss: 2.6296\n",
            "[Step 16290] Loss: 2.4894\n",
            "[Step 16300] Loss: 2.3507\n",
            "[Step 16310] Loss: 2.6296\n",
            "[Step 16320] Loss: 2.6200\n",
            "[Step 16330] Loss: 2.1874\n",
            "[Step 16340] Loss: 2.8492\n",
            "[Step 16350] Loss: 2.2613\n",
            "[Step 16360] Loss: 2.7507\n",
            "[Step 16370] Loss: 1.9684\n",
            "[Step 16380] Loss: 2.6637\n",
            "[Step 16390] Loss: 2.5495\n",
            "[Step 16400] Loss: 2.5729\n",
            "[Step 16410] Loss: 2.3892\n",
            "[Step 16420] Loss: 2.3713\n",
            "[Step 16430] Loss: 2.7067\n",
            "[Step 16440] Loss: 2.6156\n",
            "[Step 16450] Loss: 2.3800\n",
            "[Step 16460] Loss: 2.3593\n",
            "[Step 16470] Loss: 2.9095\n",
            "[Step 16480] Loss: 2.9462\n",
            "[Step 16490] Loss: 3.0408\n",
            "[Step 16500] Loss: 2.6213\n",
            "[Step 16510] Loss: 2.8668\n",
            "[Step 16520] Loss: 2.2051\n",
            "[Step 16530] Loss: 2.2086\n",
            "[Step 16540] Loss: 3.0088\n",
            "[Step 16550] Loss: 2.9804\n",
            "[Step 16560] Loss: 2.6226\n",
            "[Step 16570] Loss: 2.8226\n",
            "[Step 16580] Loss: 2.7097\n",
            "[Step 16590] Loss: 2.7635\n",
            "[Step 16600] Loss: 2.3299\n",
            "[Step 16610] Loss: 2.2169\n",
            "[Step 16620] Loss: 3.1038\n",
            "[Step 16630] Loss: 2.6527\n",
            "[Step 16640] Loss: 2.4118\n",
            "[Step 16650] Loss: 2.9530\n",
            "[Step 16660] Loss: 3.0523\n",
            "[Step 16670] Loss: 2.6818\n",
            "[Step 16680] Loss: 2.6363\n",
            "[Step 16690] Loss: 2.3315\n",
            "[Step 16700] Loss: 2.4300\n",
            "[Step 16710] Loss: 2.1486\n",
            "[Step 16720] Loss: 2.1490\n",
            "[Step 16730] Loss: 2.5597\n",
            "[Step 16740] Loss: 2.3758\n",
            "[Step 16750] Loss: 2.9184\n",
            "[Step 16760] Loss: 2.2346\n",
            "[Step 16770] Loss: 2.5368\n",
            "[Step 16780] Loss: 2.6925\n",
            "[Step 16790] Loss: 2.5158\n",
            "[Step 16800] Loss: 2.9576\n",
            "[Step 16810] Loss: 2.5019\n",
            "[Step 16820] Loss: 2.5186\n",
            "[Step 16830] Loss: 2.3943\n",
            "[Step 16840] Loss: 2.5644\n",
            "[Step 16850] Loss: 2.7164\n",
            "[Step 16860] Loss: 2.5507\n",
            "[Step 16870] Loss: 2.1068\n",
            "[Step 16880] Loss: 2.4720\n",
            "[Step 16890] Loss: 2.5506\n",
            "[Step 16900] Loss: 2.8593\n",
            "[Step 16910] Loss: 2.4969\n",
            "[Step 16920] Loss: 2.4735\n",
            "[Step 16930] Loss: 2.7211\n",
            "[Step 16940] Loss: 2.0218\n",
            "[Step 16950] Loss: 2.4485\n",
            "[Step 16960] Loss: 2.8888\n",
            "[Step 16970] Loss: 2.3218\n",
            "[Step 16980] Loss: 2.4404\n",
            "[Step 16990] Loss: 2.7177\n",
            "[Step 17000] Loss: 2.7598\n",
            "[Step 17010] Loss: 2.6103\n",
            "[Step 17020] Loss: 2.9189\n",
            "[Step 17030] Loss: 2.4123\n",
            "[Step 17040] Loss: 2.8935\n",
            "ðŸ“˜ Epoch 34 - Avg Training Loss: 2.6058\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6693 | Accuracy: 0.3355 | Precision: 0.3210\n",
            "[Step 17050] Loss: 2.8476\n",
            "[Step 17060] Loss: 3.0073\n",
            "[Step 17070] Loss: 2.6935\n",
            "[Step 17080] Loss: 3.0478\n",
            "[Step 17090] Loss: 2.3431\n",
            "[Step 17100] Loss: 2.4303\n",
            "[Step 17110] Loss: 2.6032\n",
            "[Step 17120] Loss: 2.5959\n",
            "[Step 17130] Loss: 2.4291\n",
            "[Step 17140] Loss: 2.4080\n",
            "[Step 17150] Loss: 2.8933\n",
            "[Step 17160] Loss: 2.2865\n",
            "[Step 17170] Loss: 2.6965\n",
            "[Step 17180] Loss: 2.7598\n",
            "[Step 17190] Loss: 2.2891\n",
            "[Step 17200] Loss: 2.9000\n",
            "[Step 17210] Loss: 2.7594\n",
            "[Step 17220] Loss: 2.3401\n",
            "[Step 17230] Loss: 2.7210\n",
            "[Step 17240] Loss: 2.8338\n",
            "[Step 17250] Loss: 2.4262\n",
            "[Step 17260] Loss: 2.0081\n",
            "[Step 17270] Loss: 2.4784\n",
            "[Step 17280] Loss: 2.7168\n",
            "[Step 17290] Loss: 2.4395\n",
            "[Step 17300] Loss: 2.5510\n",
            "[Step 17310] Loss: 2.2941\n",
            "[Step 17320] Loss: 3.2113\n",
            "[Step 17330] Loss: 2.6541\n",
            "[Step 17340] Loss: 3.0831\n",
            "[Step 17350] Loss: 2.8305\n",
            "[Step 17360] Loss: 2.6881\n",
            "[Step 17370] Loss: 3.1294\n",
            "[Step 17380] Loss: 2.4136\n",
            "[Step 17390] Loss: 2.3512\n",
            "[Step 17400] Loss: 2.5624\n",
            "[Step 17410] Loss: 3.0741\n",
            "[Step 17420] Loss: 2.6739\n",
            "[Step 17430] Loss: 2.5146\n",
            "[Step 17440] Loss: 2.5283\n",
            "[Step 17450] Loss: 2.4120\n",
            "[Step 17460] Loss: 3.0244\n",
            "[Step 17470] Loss: 2.6679\n",
            "[Step 17480] Loss: 2.7218\n",
            "[Step 17490] Loss: 2.3884\n",
            "[Step 17500] Loss: 2.5360\n",
            "[Step 17510] Loss: 2.1318\n",
            "[Step 17520] Loss: 2.0627\n",
            "[Step 17530] Loss: 2.7021\n",
            "[Step 17540] Loss: 2.5920\n",
            "[Step 17550] Loss: 2.9737\n",
            "[Step 17560] Loss: 2.3760\n",
            "[Step 17570] Loss: 2.2405\n",
            "[Step 17580] Loss: 2.6050\n",
            "[Step 17590] Loss: 3.0367\n",
            "[Step 17600] Loss: 2.7160\n",
            "[Step 17610] Loss: 2.4931\n",
            "[Step 17620] Loss: 2.4132\n",
            "[Step 17630] Loss: 2.6535\n",
            "[Step 17640] Loss: 2.4215\n",
            "[Step 17650] Loss: 2.7914\n",
            "[Step 17660] Loss: 2.5021\n",
            "[Step 17670] Loss: 2.3717\n",
            "[Step 17680] Loss: 2.8251\n",
            "[Step 17690] Loss: 2.8145\n",
            "[Step 17700] Loss: 2.6891\n",
            "[Step 17710] Loss: 2.5659\n",
            "[Step 17720] Loss: 2.1868\n",
            "[Step 17730] Loss: 2.2717\n",
            "[Step 17740] Loss: 2.4291\n",
            "[Step 17750] Loss: 2.4143\n",
            "[Step 17760] Loss: 2.3926\n",
            "[Step 17770] Loss: 2.2759\n",
            "[Step 17780] Loss: 2.9629\n",
            "[Step 17790] Loss: 2.2938\n",
            "[Step 17800] Loss: 3.1270\n",
            "[Step 17810] Loss: 2.8019\n",
            "[Step 17820] Loss: 1.9844\n",
            "[Step 17830] Loss: 2.8176\n",
            "[Step 17840] Loss: 2.3499\n",
            "[Step 17850] Loss: 2.7813\n",
            "[Step 17860] Loss: 2.5702\n",
            "[Step 17870] Loss: 2.6446\n",
            "[Step 17880] Loss: 2.3162\n",
            "[Step 17890] Loss: 2.8335\n",
            "[Step 17900] Loss: 2.6332\n",
            "[Step 17910] Loss: 2.5312\n",
            "[Step 17920] Loss: 2.2958\n",
            "[Step 17930] Loss: 2.6089\n",
            "[Step 17940] Loss: 3.0329\n",
            "[Step 17950] Loss: 2.5980\n",
            "[Step 17960] Loss: 2.4965\n",
            "[Step 17970] Loss: 2.9114\n",
            "[Step 17980] Loss: 2.6527\n",
            "[Step 17990] Loss: 2.6027\n",
            "[Step 18000] Loss: 2.5246\n",
            "[Step 18010] Loss: 2.5319\n",
            "[Step 18020] Loss: 2.6407\n",
            "[Step 18030] Loss: 2.3894\n",
            "[Step 18040] Loss: 2.6860\n",
            "[Step 18050] Loss: 2.6287\n",
            "[Step 18060] Loss: 2.1144\n",
            "[Step 18070] Loss: 2.7988\n",
            "[Step 18080] Loss: 2.3405\n",
            "[Step 18090] Loss: 2.1063\n",
            "[Step 18100] Loss: 2.5905\n",
            "[Step 18110] Loss: 2.2661\n",
            "[Step 18120] Loss: 2.6752\n",
            "[Step 18130] Loss: 2.7748\n",
            "[Step 18140] Loss: 2.5870\n",
            "[Step 18150] Loss: 2.3561\n",
            "[Step 18160] Loss: 2.5010\n",
            "[Step 18170] Loss: 2.4156\n",
            "[Step 18180] Loss: 2.9749\n",
            "[Step 18190] Loss: 2.4254\n",
            "[Step 18200] Loss: 2.6119\n",
            "[Step 18210] Loss: 2.4044\n",
            "[Step 18220] Loss: 2.1405\n",
            "[Step 18230] Loss: 2.4292\n",
            "[Step 18240] Loss: 2.9985\n",
            "[Step 18250] Loss: 2.8857\n",
            "[Step 18260] Loss: 2.2035\n",
            "[Step 18270] Loss: 2.5916\n",
            "[Step 18280] Loss: 2.5973\n",
            "[Step 18290] Loss: 2.4604\n",
            "[Step 18300] Loss: 3.0904\n",
            "[Step 18310] Loss: 2.4920\n",
            "[Step 18320] Loss: 2.8499\n",
            "[Step 18330] Loss: 2.8832\n",
            "[Step 18340] Loss: 2.3657\n",
            "[Step 18350] Loss: 2.2756\n",
            "[Step 18360] Loss: 2.5478\n",
            "[Step 18370] Loss: 2.4882\n",
            "[Step 18380] Loss: 2.5362\n",
            "[Step 18390] Loss: 2.5695\n",
            "[Step 18400] Loss: 2.3881\n",
            "[Step 18410] Loss: 1.9667\n",
            "[Step 18420] Loss: 2.5721\n",
            "[Step 18430] Loss: 3.1608\n",
            "[Step 18440] Loss: 2.4409\n",
            "[Step 18450] Loss: 2.7130\n",
            "[Step 18460] Loss: 2.6947\n",
            "[Step 18470] Loss: 1.9244\n",
            "[Step 18480] Loss: 2.6757\n",
            "[Step 18490] Loss: 2.0832\n",
            "[Step 18500] Loss: 2.5802\n",
            "[Step 18510] Loss: 2.3359\n",
            "[Step 18520] Loss: 2.6429\n",
            "[Step 18530] Loss: 2.3573\n",
            "[Step 18540] Loss: 2.6679\n",
            "[Step 18550] Loss: 2.2491\n",
            "[Step 18560] Loss: 2.5664\n",
            "[Step 18570] Loss: 2.6025\n",
            "[Step 18580] Loss: 2.7223\n",
            "[Step 18590] Loss: 2.4366\n",
            "[Step 18600] Loss: 3.1746\n",
            "[Step 18610] Loss: 1.9498\n",
            "[Step 18620] Loss: 2.7267\n",
            "[Step 18630] Loss: 2.5184\n",
            "[Step 18640] Loss: 3.0534\n",
            "[Step 18650] Loss: 2.8018\n",
            "[Step 18660] Loss: 2.2322\n",
            "[Step 18670] Loss: 2.1379\n",
            "[Step 18680] Loss: 2.4909\n",
            "[Step 18690] Loss: 2.6870\n",
            "[Step 18700] Loss: 2.8834\n",
            "[Step 18710] Loss: 3.0847\n",
            "[Step 18720] Loss: 2.6226\n",
            "[Step 18730] Loss: 2.8781\n",
            "[Step 18740] Loss: 2.8023\n",
            "[Step 18750] Loss: 2.7745\n",
            "[Step 18760] Loss: 2.1169\n",
            "[Step 18770] Loss: 2.4541\n",
            "[Step 18780] Loss: 2.4329\n",
            "[Step 18790] Loss: 2.9114\n",
            "[Step 18800] Loss: 2.7064\n",
            "[Step 18810] Loss: 2.3838\n",
            "[Step 18820] Loss: 2.3363\n",
            "[Step 18830] Loss: 2.4282\n",
            "[Step 18840] Loss: 2.5030\n",
            "[Step 18850] Loss: 2.3069\n",
            "[Step 18860] Loss: 2.4801\n",
            "[Step 18870] Loss: 2.8039\n",
            "[Step 18880] Loss: 1.9719\n",
            "[Step 18890] Loss: 2.7766\n",
            "[Step 18900] Loss: 2.5324\n",
            "[Step 18910] Loss: 3.3749\n",
            "[Step 18920] Loss: 2.2737\n",
            "[Step 18930] Loss: 2.0648\n",
            "[Step 18940] Loss: 2.9249\n",
            "ðŸ“˜ Epoch 35 - Avg Training Loss: 2.5618\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6280 | Accuracy: 0.3447 | Precision: 0.3212\n",
            "[Step 18950] Loss: 2.5146\n",
            "[Step 18960] Loss: 2.2222\n",
            "[Step 18970] Loss: 2.4671\n",
            "[Step 18980] Loss: 2.3542\n",
            "[Step 18990] Loss: 2.1232\n",
            "[Step 19000] Loss: 3.0526\n",
            "[Step 19010] Loss: 2.6459\n",
            "[Step 19020] Loss: 2.2379\n",
            "[Step 19030] Loss: 2.7311\n",
            "[Step 19040] Loss: 2.7389\n",
            "[Step 19050] Loss: 2.4234\n",
            "[Step 19060] Loss: 2.5566\n",
            "[Step 19070] Loss: 3.0498\n",
            "[Step 19080] Loss: 2.8574\n",
            "[Step 19090] Loss: 2.6941\n",
            "[Step 19100] Loss: 2.5387\n",
            "[Step 19110] Loss: 2.4975\n",
            "[Step 19120] Loss: 3.1725\n",
            "[Step 19130] Loss: 2.6680\n",
            "[Step 19140] Loss: 2.2304\n",
            "[Step 19150] Loss: 2.8472\n",
            "[Step 19160] Loss: 2.6422\n",
            "[Step 19170] Loss: 2.5449\n",
            "[Step 19180] Loss: 2.4980\n",
            "[Step 19190] Loss: 2.9533\n",
            "[Step 19200] Loss: 2.7775\n",
            "[Step 19210] Loss: 2.3675\n",
            "[Step 19220] Loss: 2.9362\n",
            "[Step 19230] Loss: 2.4762\n",
            "[Step 19240] Loss: 2.0927\n",
            "[Step 19250] Loss: 2.4103\n",
            "[Step 19260] Loss: 2.6190\n",
            "[Step 19270] Loss: 2.7881\n",
            "[Step 19280] Loss: 2.3173\n",
            "[Step 19290] Loss: 2.4345\n",
            "[Step 19300] Loss: 2.4768\n",
            "[Step 19310] Loss: 2.5929\n",
            "[Step 19320] Loss: 2.4178\n",
            "[Step 19330] Loss: 2.2461\n",
            "[Step 19340] Loss: 2.7246\n",
            "[Step 19350] Loss: 2.1951\n",
            "[Step 19360] Loss: 2.8309\n",
            "[Step 19370] Loss: 2.2228\n",
            "[Step 19380] Loss: 2.9088\n",
            "[Step 19390] Loss: 3.1259\n",
            "[Step 19400] Loss: 2.5985\n",
            "[Step 19410] Loss: 2.4805\n",
            "[Step 19420] Loss: 2.7998\n",
            "[Step 19430] Loss: 2.8255\n",
            "[Step 19440] Loss: 2.4430\n",
            "[Step 19450] Loss: 2.4257\n",
            "[Step 19460] Loss: 2.3134\n",
            "[Step 19470] Loss: 2.2306\n",
            "[Step 19480] Loss: 2.6928\n",
            "[Step 19490] Loss: 2.5307\n",
            "[Step 19500] Loss: 2.7146\n",
            "[Step 19510] Loss: 2.8834\n",
            "[Step 19520] Loss: 2.8143\n",
            "[Step 19530] Loss: 2.0575\n",
            "[Step 19540] Loss: 2.6894\n",
            "[Step 19550] Loss: 2.6947\n",
            "[Step 19560] Loss: 2.9475\n",
            "[Step 19570] Loss: 2.0630\n",
            "[Step 19580] Loss: 2.1200\n",
            "[Step 19590] Loss: 2.1522\n",
            "[Step 19600] Loss: 2.3680\n",
            "[Step 19610] Loss: 3.0189\n",
            "[Step 19620] Loss: 2.3029\n",
            "[Step 19630] Loss: 2.6026\n",
            "[Step 19640] Loss: 2.6533\n",
            "[Step 19650] Loss: 2.6053\n",
            "[Step 19660] Loss: 2.5828\n",
            "[Step 19670] Loss: 2.8150\n",
            "[Step 19680] Loss: 2.3865\n",
            "[Step 19690] Loss: 2.5802\n",
            "[Step 19700] Loss: 2.0994\n",
            "[Step 19710] Loss: 2.3963\n",
            "[Step 19720] Loss: 2.8545\n",
            "[Step 19730] Loss: 2.0187\n",
            "[Step 19740] Loss: 2.9682\n",
            "[Step 19750] Loss: 2.5035\n",
            "[Step 19760] Loss: 2.4998\n",
            "[Step 19770] Loss: 2.7122\n",
            "[Step 19780] Loss: 2.2218\n",
            "[Step 19790] Loss: 2.0710\n",
            "[Step 19800] Loss: 2.5138\n",
            "[Step 19810] Loss: 2.3764\n",
            "[Step 19820] Loss: 2.4017\n",
            "[Step 19830] Loss: 2.2150\n",
            "[Step 19840] Loss: 2.2537\n",
            "[Step 19850] Loss: 2.3838\n",
            "[Step 19860] Loss: 2.4169\n",
            "[Step 19870] Loss: 2.6175\n",
            "[Step 19880] Loss: 2.6007\n",
            "[Step 19890] Loss: 2.1542\n",
            "[Step 19900] Loss: 2.3229\n",
            "[Step 19910] Loss: 2.7802\n",
            "[Step 19920] Loss: 2.3435\n",
            "[Step 19930] Loss: 2.1218\n",
            "[Step 19940] Loss: 2.2017\n",
            "[Step 19950] Loss: 3.1766\n",
            "[Step 19960] Loss: 2.2311\n",
            "[Step 19970] Loss: 2.4233\n",
            "[Step 19980] Loss: 2.2607\n",
            "[Step 19990] Loss: 2.6007\n",
            "[Step 20000] Loss: 2.7203\n",
            "[Step 20010] Loss: 2.9292\n",
            "[Step 20020] Loss: 2.4668\n",
            "[Step 20030] Loss: 2.5631\n",
            "[Step 20040] Loss: 2.4395\n",
            "[Step 20050] Loss: 2.6326\n",
            "[Step 20060] Loss: 2.2773\n",
            "[Step 20070] Loss: 2.5424\n",
            "[Step 20080] Loss: 2.5163\n",
            "[Step 20090] Loss: 2.5659\n",
            "[Step 20100] Loss: 1.9228\n",
            "[Step 20110] Loss: 2.6554\n",
            "[Step 20120] Loss: 2.8689\n",
            "[Step 20130] Loss: 2.3902\n",
            "[Step 20140] Loss: 2.0074\n",
            "[Step 20150] Loss: 1.9030\n",
            "[Step 20160] Loss: 2.7986\n",
            "[Step 20170] Loss: 2.3157\n",
            "[Step 20180] Loss: 2.8207\n",
            "[Step 20190] Loss: 2.3751\n",
            "[Step 20200] Loss: 2.4701\n",
            "[Step 20210] Loss: 2.3102\n",
            "[Step 20220] Loss: 2.7828\n",
            "[Step 20230] Loss: 2.5721\n",
            "[Step 20240] Loss: 2.1981\n",
            "[Step 20250] Loss: 2.6674\n",
            "[Step 20260] Loss: 2.2101\n",
            "[Step 20270] Loss: 2.3709\n",
            "[Step 20280] Loss: 2.1451\n",
            "[Step 20290] Loss: 2.8252\n",
            "[Step 20300] Loss: 2.5326\n",
            "[Step 20310] Loss: 3.0040\n",
            "[Step 20320] Loss: 2.2754\n",
            "[Step 20330] Loss: 2.4525\n",
            "[Step 20340] Loss: 2.2867\n",
            "[Step 20350] Loss: 2.8293\n",
            "[Step 20360] Loss: 2.3421\n",
            "[Step 20370] Loss: 1.9655\n",
            "[Step 20380] Loss: 3.0774\n",
            "[Step 20390] Loss: 3.0661\n",
            "[Step 20400] Loss: 2.6859\n",
            "[Step 20410] Loss: 2.5156\n",
            "[Step 20420] Loss: 2.7504\n",
            "[Step 20430] Loss: 2.5275\n",
            "[Step 20440] Loss: 2.4194\n",
            "[Step 20450] Loss: 1.7466\n",
            "[Step 20460] Loss: 2.7887\n",
            "[Step 20470] Loss: 2.8197\n",
            "[Step 20480] Loss: 2.1713\n",
            "[Step 20490] Loss: 2.4965\n",
            "[Step 20500] Loss: 2.9260\n",
            "[Step 20510] Loss: 2.3792\n",
            "[Step 20520] Loss: 2.4030\n",
            "[Step 20530] Loss: 2.3472\n",
            "[Step 20540] Loss: 3.1601\n",
            "[Step 20550] Loss: 2.7919\n",
            "[Step 20560] Loss: 2.5487\n",
            "[Step 20570] Loss: 2.5328\n",
            "[Step 20580] Loss: 2.8909\n",
            "[Step 20590] Loss: 2.1333\n",
            "[Step 20600] Loss: 2.3580\n",
            "[Step 20610] Loss: 2.5519\n",
            "[Step 20620] Loss: 1.9786\n",
            "[Step 20630] Loss: 2.2367\n",
            "[Step 20640] Loss: 2.1995\n",
            "[Step 20650] Loss: 2.6222\n",
            "[Step 20660] Loss: 2.6972\n",
            "[Step 20670] Loss: 2.7364\n",
            "[Step 20680] Loss: 2.2488\n",
            "[Step 20690] Loss: 2.3967\n",
            "[Step 20700] Loss: 2.3239\n",
            "[Step 20710] Loss: 2.0846\n",
            "[Step 20720] Loss: 2.8083\n",
            "[Step 20730] Loss: 2.5213\n",
            "[Step 20740] Loss: 2.1401\n",
            "[Step 20750] Loss: 2.1581\n",
            "[Step 20760] Loss: 2.2547\n",
            "[Step 20770] Loss: 2.6963\n",
            "[Step 20780] Loss: 2.3846\n",
            "[Step 20790] Loss: 2.3278\n",
            "[Step 20800] Loss: 3.0011\n",
            "[Step 20810] Loss: 2.5229\n",
            "[Step 20820] Loss: 2.0969\n",
            "[Step 20830] Loss: 2.7341\n",
            "ðŸ“˜ Epoch 36 - Avg Training Loss: 2.5508\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6293 | Accuracy: 0.3459 | Precision: 0.3236\n",
            "[Step 20840] Loss: 2.6956\n",
            "[Step 20850] Loss: 2.6537\n",
            "[Step 20860] Loss: 2.6813\n",
            "[Step 20870] Loss: 2.7831\n",
            "[Step 20880] Loss: 2.2101\n",
            "[Step 20890] Loss: 2.9161\n",
            "[Step 20900] Loss: 2.7584\n",
            "[Step 20910] Loss: 3.0647\n",
            "[Step 20920] Loss: 3.1117\n",
            "[Step 20930] Loss: 2.5365\n",
            "[Step 20940] Loss: 2.6775\n",
            "[Step 20950] Loss: 2.9321\n",
            "[Step 20960] Loss: 2.5713\n",
            "[Step 20970] Loss: 2.5042\n",
            "[Step 20980] Loss: 2.7562\n",
            "[Step 20990] Loss: 2.3161\n",
            "[Step 21000] Loss: 2.8482\n",
            "[Step 21010] Loss: 2.5712\n",
            "[Step 21020] Loss: 2.2488\n",
            "[Step 21030] Loss: 2.6802\n",
            "[Step 21040] Loss: 2.3318\n",
            "[Step 21050] Loss: 2.0905\n",
            "[Step 21060] Loss: 2.4806\n",
            "[Step 21070] Loss: 2.1752\n",
            "[Step 21080] Loss: 1.9943\n",
            "[Step 21090] Loss: 2.4269\n",
            "[Step 21100] Loss: 2.3659\n",
            "[Step 21110] Loss: 2.1572\n",
            "[Step 21120] Loss: 2.8336\n",
            "[Step 21130] Loss: 2.5578\n",
            "[Step 21140] Loss: 2.1332\n",
            "[Step 21150] Loss: 2.8898\n",
            "[Step 21160] Loss: 2.7581\n",
            "[Step 21170] Loss: 2.9260\n",
            "[Step 21180] Loss: 2.3714\n",
            "[Step 21190] Loss: 2.5046\n",
            "[Step 21200] Loss: 2.1519\n",
            "[Step 21210] Loss: 2.4238\n",
            "[Step 21220] Loss: 2.3801\n",
            "[Step 21230] Loss: 2.2705\n",
            "[Step 21240] Loss: 2.8619\n",
            "[Step 21250] Loss: 2.6311\n",
            "[Step 21260] Loss: 2.8500\n",
            "[Step 21270] Loss: 2.4856\n",
            "[Step 21280] Loss: 2.2727\n",
            "[Step 21290] Loss: 2.4017\n",
            "[Step 21300] Loss: 2.7332\n",
            "[Step 21310] Loss: 2.4622\n",
            "[Step 21320] Loss: 2.4609\n",
            "[Step 21330] Loss: 3.1242\n",
            "[Step 21340] Loss: 2.9510\n",
            "[Step 21350] Loss: 2.7675\n",
            "[Step 21360] Loss: 2.8405\n",
            "[Step 21370] Loss: 2.7769\n",
            "[Step 21380] Loss: 2.7867\n",
            "[Step 21390] Loss: 2.5753\n",
            "[Step 21400] Loss: 2.4193\n",
            "[Step 21410] Loss: 2.0798\n",
            "[Step 21420] Loss: 2.7205\n",
            "[Step 21430] Loss: 2.0998\n",
            "[Step 21440] Loss: 2.2835\n",
            "[Step 21450] Loss: 2.8842\n",
            "[Step 21460] Loss: 2.2522\n",
            "[Step 21470] Loss: 2.4574\n",
            "[Step 21480] Loss: 2.2924\n",
            "[Step 21490] Loss: 2.4897\n",
            "[Step 21500] Loss: 2.6222\n",
            "[Step 21510] Loss: 2.3988\n",
            "[Step 21520] Loss: 2.5402\n",
            "[Step 21530] Loss: 2.5225\n",
            "[Step 21540] Loss: 2.2744\n",
            "[Step 21550] Loss: 2.5067\n",
            "[Step 21560] Loss: 2.7447\n",
            "[Step 21570] Loss: 2.3921\n",
            "[Step 21580] Loss: 2.6554\n",
            "[Step 21590] Loss: 2.4486\n",
            "[Step 21600] Loss: 2.6281\n",
            "[Step 21610] Loss: 2.6343\n",
            "[Step 21620] Loss: 2.3948\n",
            "[Step 21630] Loss: 2.6493\n",
            "[Step 21640] Loss: 2.2709\n",
            "[Step 21650] Loss: 2.7029\n",
            "[Step 21660] Loss: 2.5599\n",
            "[Step 21670] Loss: 2.2787\n",
            "[Step 21680] Loss: 2.5322\n",
            "[Step 21690] Loss: 2.6482\n",
            "[Step 21700] Loss: 2.6949\n",
            "[Step 21710] Loss: 3.0023\n",
            "[Step 21720] Loss: 2.1489\n",
            "[Step 21730] Loss: 2.9992\n",
            "[Step 21740] Loss: 2.9267\n",
            "[Step 21750] Loss: 2.6322\n",
            "[Step 21760] Loss: 2.3877\n",
            "[Step 21770] Loss: 2.5443\n",
            "[Step 21780] Loss: 2.4051\n",
            "[Step 21790] Loss: 2.3178\n",
            "[Step 21800] Loss: 2.7441\n",
            "[Step 21810] Loss: 2.6973\n",
            "[Step 21820] Loss: 2.4966\n",
            "[Step 21830] Loss: 2.4717\n",
            "[Step 21840] Loss: 2.2761\n",
            "[Step 21850] Loss: 2.7549\n",
            "[Step 21860] Loss: 2.2283\n",
            "[Step 21870] Loss: 2.2367\n",
            "[Step 21880] Loss: 2.5219\n",
            "[Step 21890] Loss: 2.2194\n",
            "[Step 21900] Loss: 2.0535\n",
            "[Step 21910] Loss: 2.3761\n",
            "[Step 21920] Loss: 2.2219\n",
            "[Step 21930] Loss: 2.9390\n",
            "[Step 21940] Loss: 2.4536\n",
            "[Step 21950] Loss: 2.6505\n",
            "[Step 21960] Loss: 2.7344\n",
            "[Step 21970] Loss: 2.2976\n",
            "[Step 21980] Loss: 2.0956\n",
            "[Step 21990] Loss: 2.7758\n",
            "[Step 22000] Loss: 2.3960\n",
            "[Step 22010] Loss: 2.3838\n",
            "[Step 22020] Loss: 2.5308\n",
            "[Step 22030] Loss: 2.4900\n",
            "[Step 22040] Loss: 2.5652\n",
            "[Step 22050] Loss: 2.3115\n",
            "[Step 22060] Loss: 1.9156\n",
            "[Step 22070] Loss: 2.7676\n",
            "[Step 22080] Loss: 2.3200\n",
            "[Step 22090] Loss: 2.5754\n",
            "[Step 22100] Loss: 2.7146\n",
            "[Step 22110] Loss: 2.9155\n",
            "[Step 22120] Loss: 2.5433\n",
            "[Step 22130] Loss: 3.3214\n",
            "[Step 22140] Loss: 2.9585\n",
            "[Step 22150] Loss: 2.2447\n",
            "[Step 22160] Loss: 2.6666\n",
            "[Step 22170] Loss: 2.8107\n",
            "[Step 22180] Loss: 2.5673\n",
            "[Step 22190] Loss: 1.9868\n",
            "[Step 22200] Loss: 2.4914\n",
            "[Step 22210] Loss: 2.4694\n",
            "[Step 22220] Loss: 2.3739\n",
            "[Step 22230] Loss: 2.8382\n",
            "[Step 22240] Loss: 1.8837\n",
            "[Step 22250] Loss: 2.1080\n",
            "[Step 22260] Loss: 2.6621\n",
            "[Step 22270] Loss: 2.3053\n",
            "[Step 22280] Loss: 2.8796\n",
            "[Step 22290] Loss: 2.8178\n",
            "[Step 22300] Loss: 2.8280\n",
            "[Step 22310] Loss: 2.2920\n",
            "[Step 22320] Loss: 2.3728\n",
            "[Step 22330] Loss: 2.6376\n",
            "[Step 22340] Loss: 2.4183\n",
            "[Step 22350] Loss: 2.6223\n",
            "[Step 22360] Loss: 3.3564\n",
            "[Step 22370] Loss: 2.4401\n",
            "[Step 22380] Loss: 2.8833\n",
            "[Step 22390] Loss: 2.8091\n",
            "[Step 22400] Loss: 2.7854\n",
            "[Step 22410] Loss: 1.9934\n",
            "[Step 22420] Loss: 2.7139\n",
            "[Step 22430] Loss: 2.5845\n",
            "[Step 22440] Loss: 2.5052\n",
            "[Step 22450] Loss: 2.2200\n",
            "[Step 22460] Loss: 2.6263\n",
            "[Step 22470] Loss: 2.3305\n",
            "[Step 22480] Loss: 2.8032\n",
            "[Step 22490] Loss: 2.6850\n",
            "[Step 22500] Loss: 2.7554\n",
            "[Step 22510] Loss: 2.7107\n",
            "[Step 22520] Loss: 2.7133\n",
            "[Step 22530] Loss: 2.5354\n",
            "[Step 22540] Loss: 2.2320\n",
            "[Step 22550] Loss: 2.6699\n",
            "[Step 22560] Loss: 3.2738\n",
            "[Step 22570] Loss: 2.4066\n",
            "[Step 22580] Loss: 2.1487\n",
            "[Step 22590] Loss: 2.5565\n",
            "[Step 22600] Loss: 2.7108\n",
            "[Step 22610] Loss: 2.2981\n",
            "[Step 22620] Loss: 2.5959\n",
            "[Step 22630] Loss: 2.9699\n",
            "[Step 22640] Loss: 2.4084\n",
            "[Step 22650] Loss: 3.0256\n",
            "[Step 22660] Loss: 2.7678\n",
            "[Step 22670] Loss: 2.5262\n",
            "[Step 22680] Loss: 2.7511\n",
            "[Step 22690] Loss: 2.9147\n",
            "[Step 22700] Loss: 3.0838\n",
            "[Step 22710] Loss: 2.7296\n",
            "[Step 22720] Loss: 2.6933\n",
            "ðŸ“˜ Epoch 37 - Avg Training Loss: 2.5563\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6294 | Accuracy: 0.3470 | Precision: 0.3276\n",
            "[Step 22730] Loss: 2.1571\n",
            "[Step 22740] Loss: 2.1919\n",
            "[Step 22750] Loss: 3.0970\n",
            "[Step 22760] Loss: 2.2432\n",
            "[Step 22770] Loss: 2.3466\n",
            "[Step 22780] Loss: 2.4566\n",
            "[Step 22790] Loss: 2.6505\n",
            "[Step 22800] Loss: 2.6022\n",
            "[Step 22810] Loss: 2.6326\n",
            "[Step 22820] Loss: 2.5330\n",
            "[Step 22830] Loss: 2.3968\n",
            "[Step 22840] Loss: 2.5199\n",
            "[Step 22850] Loss: 2.3975\n",
            "[Step 22860] Loss: 2.9231\n",
            "[Step 22870] Loss: 2.6796\n",
            "[Step 22880] Loss: 2.3791\n",
            "[Step 22890] Loss: 3.2123\n",
            "[Step 22900] Loss: 2.6999\n",
            "[Step 22910] Loss: 2.3176\n",
            "[Step 22920] Loss: 2.3818\n",
            "[Step 22930] Loss: 2.9501\n",
            "[Step 22940] Loss: 2.6815\n",
            "[Step 22950] Loss: 2.4046\n",
            "[Step 22960] Loss: 2.6207\n",
            "[Step 22970] Loss: 3.1257\n",
            "[Step 22980] Loss: 2.8722\n",
            "[Step 22990] Loss: 2.4577\n",
            "[Step 23000] Loss: 2.6497\n",
            "[Step 23010] Loss: 2.8580\n",
            "[Step 23020] Loss: 2.5179\n",
            "[Step 23030] Loss: 2.2278\n",
            "[Step 23040] Loss: 2.6746\n",
            "[Step 23050] Loss: 1.9838\n",
            "[Step 23060] Loss: 2.3734\n",
            "[Step 23070] Loss: 2.2703\n",
            "[Step 23080] Loss: 2.3357\n",
            "[Step 23090] Loss: 2.1958\n",
            "[Step 23100] Loss: 2.3409\n",
            "[Step 23110] Loss: 2.3880\n",
            "[Step 23120] Loss: 2.2362\n",
            "[Step 23130] Loss: 2.4963\n",
            "[Step 23140] Loss: 2.8890\n",
            "[Step 23150] Loss: 2.8537\n",
            "[Step 23160] Loss: 2.8616\n",
            "[Step 23170] Loss: 2.0700\n",
            "[Step 23180] Loss: 2.2432\n",
            "[Step 23190] Loss: 2.2582\n",
            "[Step 23200] Loss: 2.5968\n",
            "[Step 23210] Loss: 2.3233\n",
            "[Step 23220] Loss: 2.4692\n",
            "[Step 23230] Loss: 2.7443\n",
            "[Step 23240] Loss: 2.8095\n",
            "[Step 23250] Loss: 2.7544\n",
            "[Step 23260] Loss: 2.2521\n",
            "[Step 23270] Loss: 2.3550\n",
            "[Step 23280] Loss: 3.1939\n",
            "[Step 23290] Loss: 2.4132\n",
            "[Step 23300] Loss: 2.8662\n",
            "[Step 23310] Loss: 2.5365\n",
            "[Step 23320] Loss: 2.3633\n",
            "[Step 23330] Loss: 3.3014\n",
            "[Step 23340] Loss: 3.2107\n",
            "[Step 23350] Loss: 2.8073\n",
            "[Step 23360] Loss: 2.9343\n",
            "[Step 23370] Loss: 2.6775\n",
            "[Step 23380] Loss: 3.2242\n",
            "[Step 23390] Loss: 2.2555\n",
            "[Step 23400] Loss: 2.6622\n",
            "[Step 23410] Loss: 3.1056\n",
            "[Step 23420] Loss: 2.5705\n",
            "[Step 23430] Loss: 2.3667\n",
            "[Step 23440] Loss: 2.5951\n",
            "[Step 23450] Loss: 2.7579\n",
            "[Step 23460] Loss: 2.4265\n",
            "[Step 23470] Loss: 2.4198\n",
            "[Step 23480] Loss: 2.3720\n",
            "[Step 23490] Loss: 2.7459\n",
            "[Step 23500] Loss: 2.1804\n",
            "[Step 23510] Loss: 2.3538\n",
            "[Step 23520] Loss: 2.8714\n",
            "[Step 23530] Loss: 2.4338\n",
            "[Step 23540] Loss: 3.2723\n",
            "[Step 23550] Loss: 2.6191\n",
            "[Step 23560] Loss: 3.0240\n",
            "[Step 23570] Loss: 2.1899\n",
            "[Step 23580] Loss: 2.4652\n",
            "[Step 23590] Loss: 2.1435\n",
            "[Step 23600] Loss: 2.3523\n",
            "[Step 23610] Loss: 2.6986\n",
            "[Step 23620] Loss: 2.2878\n",
            "[Step 23630] Loss: 3.2903\n",
            "[Step 23640] Loss: 2.0746\n",
            "[Step 23650] Loss: 2.5274\n",
            "[Step 23660] Loss: 1.8686\n",
            "[Step 23670] Loss: 2.6568\n",
            "[Step 23680] Loss: 2.7310\n",
            "[Step 23690] Loss: 2.6902\n",
            "[Step 23700] Loss: 2.8159\n",
            "[Step 23710] Loss: 3.0342\n",
            "[Step 23720] Loss: 2.2258\n",
            "[Step 23730] Loss: 2.3239\n",
            "[Step 23740] Loss: 2.1684\n",
            "[Step 23750] Loss: 2.8629\n",
            "[Step 23760] Loss: 3.1210\n",
            "[Step 23770] Loss: 2.2279\n",
            "[Step 23780] Loss: 2.3215\n",
            "[Step 23790] Loss: 2.2105\n",
            "[Step 23800] Loss: 2.3251\n",
            "[Step 23810] Loss: 2.4234\n",
            "[Step 23820] Loss: 2.7062\n",
            "[Step 23830] Loss: 1.7163\n",
            "[Step 23840] Loss: 2.6299\n",
            "[Step 23850] Loss: 2.6300\n",
            "[Step 23860] Loss: 3.2373\n",
            "[Step 23870] Loss: 2.8025\n",
            "[Step 23880] Loss: 2.4718\n",
            "[Step 23890] Loss: 3.1500\n",
            "[Step 23900] Loss: 2.3287\n",
            "[Step 23910] Loss: 2.0790\n",
            "[Step 23920] Loss: 2.5375\n",
            "[Step 23930] Loss: 2.5811\n",
            "[Step 23940] Loss: 2.0009\n",
            "[Step 23950] Loss: 2.3163\n",
            "[Step 23960] Loss: 2.8840\n",
            "[Step 23970] Loss: 2.8208\n",
            "[Step 23980] Loss: 2.4890\n",
            "[Step 23990] Loss: 2.3312\n",
            "[Step 24000] Loss: 2.5588\n",
            "[Step 24010] Loss: 2.2256\n",
            "[Step 24020] Loss: 2.5943\n",
            "[Step 24030] Loss: 2.7937\n",
            "[Step 24040] Loss: 2.3969\n",
            "[Step 24050] Loss: 2.4021\n",
            "[Step 24060] Loss: 2.6264\n",
            "[Step 24070] Loss: 2.3255\n",
            "[Step 24080] Loss: 2.7080\n",
            "[Step 24090] Loss: 2.4394\n",
            "[Step 24100] Loss: 2.5286\n",
            "[Step 24110] Loss: 2.4929\n",
            "[Step 24120] Loss: 2.7963\n",
            "[Step 24130] Loss: 2.7622\n",
            "[Step 24140] Loss: 2.6521\n",
            "[Step 24150] Loss: 2.5017\n",
            "[Step 24160] Loss: 2.6922\n",
            "[Step 24170] Loss: 2.5554\n",
            "[Step 24180] Loss: 2.7359\n",
            "[Step 24190] Loss: 2.4388\n",
            "[Step 24200] Loss: 2.9622\n",
            "[Step 24210] Loss: 2.5881\n",
            "[Step 24220] Loss: 2.7190\n",
            "[Step 24230] Loss: 2.7647\n",
            "[Step 24240] Loss: 2.0260\n",
            "[Step 24250] Loss: 2.6110\n",
            "[Step 24260] Loss: 2.4508\n",
            "[Step 24270] Loss: 2.8274\n",
            "[Step 24280] Loss: 2.3453\n",
            "[Step 24290] Loss: 3.2811\n",
            "[Step 24300] Loss: 2.1524\n",
            "[Step 24310] Loss: 2.5883\n",
            "[Step 24320] Loss: 2.4409\n",
            "[Step 24330] Loss: 2.3905\n",
            "[Step 24340] Loss: 2.2900\n",
            "[Step 24350] Loss: 2.5789\n",
            "[Step 24360] Loss: 2.6747\n",
            "[Step 24370] Loss: 2.4008\n",
            "[Step 24380] Loss: 2.5151\n",
            "[Step 24390] Loss: 2.5760\n",
            "[Step 24400] Loss: 2.0934\n",
            "[Step 24410] Loss: 2.7170\n",
            "[Step 24420] Loss: 2.3568\n",
            "[Step 24430] Loss: 2.2676\n",
            "[Step 24440] Loss: 2.3719\n",
            "[Step 24450] Loss: 2.8671\n",
            "[Step 24460] Loss: 2.4547\n",
            "[Step 24470] Loss: 2.7989\n",
            "[Step 24480] Loss: 2.7894\n",
            "[Step 24490] Loss: 2.9901\n",
            "[Step 24500] Loss: 2.7106\n",
            "[Step 24510] Loss: 2.6972\n",
            "[Step 24520] Loss: 2.3199\n",
            "[Step 24530] Loss: 2.1968\n",
            "[Step 24540] Loss: 3.0133\n",
            "[Step 24550] Loss: 3.0259\n",
            "[Step 24560] Loss: 2.5921\n",
            "[Step 24570] Loss: 2.3230\n",
            "[Step 24580] Loss: 2.5427\n",
            "[Step 24590] Loss: 2.7288\n",
            "[Step 24600] Loss: 2.8431\n",
            "[Step 24610] Loss: 2.7045\n",
            "[Step 24620] Loss: 2.9793\n",
            "ðŸ“˜ Epoch 38 - Avg Training Loss: 2.5780\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6542 | Accuracy: 0.3385 | Precision: 0.3211\n",
            "[Step 24630] Loss: 2.5362\n",
            "[Step 24640] Loss: 2.1200\n",
            "[Step 24650] Loss: 2.8447\n",
            "[Step 24660] Loss: 2.0509\n",
            "[Step 24670] Loss: 2.6092\n",
            "[Step 24680] Loss: 2.3423\n",
            "[Step 24690] Loss: 2.2230\n",
            "[Step 24700] Loss: 2.0006\n",
            "[Step 24710] Loss: 2.6888\n",
            "[Step 24720] Loss: 2.8180\n",
            "[Step 24730] Loss: 1.9734\n",
            "[Step 24740] Loss: 2.3137\n",
            "[Step 24750] Loss: 2.5702\n",
            "[Step 24760] Loss: 2.9889\n",
            "[Step 24770] Loss: 2.2426\n",
            "[Step 24780] Loss: 2.5256\n",
            "[Step 24790] Loss: 2.7679\n",
            "[Step 24800] Loss: 2.7774\n",
            "[Step 24810] Loss: 2.1698\n",
            "[Step 24820] Loss: 2.8613\n",
            "[Step 24830] Loss: 2.6601\n",
            "[Step 24840] Loss: 2.9008\n",
            "[Step 24850] Loss: 2.2869\n",
            "[Step 24860] Loss: 2.9223\n",
            "[Step 24870] Loss: 2.6028\n",
            "[Step 24880] Loss: 2.2631\n",
            "[Step 24890] Loss: 2.6668\n",
            "[Step 24900] Loss: 2.8750\n",
            "[Step 24910] Loss: 2.1039\n",
            "[Step 24920] Loss: 2.4652\n",
            "[Step 24930] Loss: 2.4066\n",
            "[Step 24940] Loss: 2.5634\n",
            "[Step 24950] Loss: 2.6434\n",
            "[Step 24960] Loss: 2.2711\n",
            "[Step 24970] Loss: 2.4908\n",
            "[Step 24980] Loss: 3.0218\n",
            "[Step 24990] Loss: 2.6217\n",
            "[Step 25000] Loss: 3.0553\n",
            "[Step 25010] Loss: 2.6557\n",
            "[Step 25020] Loss: 2.4436\n",
            "[Step 25030] Loss: 2.9699\n",
            "[Step 25040] Loss: 2.8177\n",
            "[Step 25050] Loss: 2.5366\n",
            "[Step 25060] Loss: 3.0244\n",
            "[Step 25070] Loss: 2.1304\n",
            "[Step 25080] Loss: 2.1235\n",
            "[Step 25090] Loss: 2.2621\n",
            "[Step 25100] Loss: 2.6231\n",
            "[Step 25110] Loss: 2.4595\n",
            "[Step 25120] Loss: 3.7264\n",
            "[Step 25130] Loss: 2.7478\n",
            "[Step 25140] Loss: 2.9061\n",
            "[Step 25150] Loss: 2.9744\n",
            "[Step 25160] Loss: 2.9142\n",
            "[Step 25170] Loss: 2.5823\n",
            "[Step 25180] Loss: 2.4443\n",
            "[Step 25190] Loss: 2.0839\n",
            "[Step 25200] Loss: 2.5537\n",
            "[Step 25210] Loss: 2.6078\n",
            "[Step 25220] Loss: 2.5003\n",
            "[Step 25230] Loss: 2.9350\n",
            "[Step 25240] Loss: 2.7581\n",
            "[Step 25250] Loss: 2.8533\n",
            "[Step 25260] Loss: 2.8951\n",
            "[Step 25270] Loss: 2.7235\n",
            "[Step 25280] Loss: 2.5864\n",
            "[Step 25290] Loss: 2.1249\n",
            "[Step 25300] Loss: 2.5875\n",
            "[Step 25310] Loss: 2.6408\n",
            "[Step 25320] Loss: 2.4232\n",
            "[Step 25330] Loss: 2.7009\n",
            "[Step 25340] Loss: 2.8174\n",
            "[Step 25350] Loss: 3.0962\n",
            "[Step 25360] Loss: 2.5226\n",
            "[Step 25370] Loss: 2.0962\n",
            "[Step 25380] Loss: 2.7161\n",
            "[Step 25390] Loss: 2.6974\n",
            "[Step 25400] Loss: 2.6356\n",
            "[Step 25410] Loss: 2.6005\n",
            "[Step 25420] Loss: 3.0856\n",
            "[Step 25430] Loss: 2.7903\n",
            "[Step 25440] Loss: 2.4727\n",
            "[Step 25450] Loss: 2.6850\n",
            "[Step 25460] Loss: 2.2166\n",
            "[Step 25470] Loss: 2.3510\n",
            "[Step 25480] Loss: 2.7395\n",
            "[Step 25490] Loss: 2.7234\n",
            "[Step 25500] Loss: 2.7751\n",
            "[Step 25510] Loss: 3.6353\n",
            "[Step 25520] Loss: 2.8602\n",
            "[Step 25530] Loss: 2.4854\n",
            "[Step 25540] Loss: 2.3465\n",
            "[Step 25550] Loss: 2.8104\n",
            "[Step 25560] Loss: 2.4440\n",
            "[Step 25570] Loss: 2.5815\n",
            "[Step 25580] Loss: 2.3081\n",
            "[Step 25590] Loss: 2.8954\n",
            "[Step 25600] Loss: 3.0376\n",
            "[Step 25610] Loss: 2.1170\n",
            "[Step 25620] Loss: 2.6055\n",
            "[Step 25630] Loss: 2.4612\n",
            "[Step 25640] Loss: 2.5587\n",
            "[Step 25650] Loss: 2.6147\n",
            "[Step 25660] Loss: 2.3055\n",
            "[Step 25670] Loss: 2.2660\n",
            "[Step 25680] Loss: 2.3542\n",
            "[Step 25690] Loss: 2.5149\n",
            "[Step 25700] Loss: 2.2754\n",
            "[Step 25710] Loss: 2.3891\n",
            "[Step 25720] Loss: 2.6481\n",
            "[Step 25730] Loss: 2.5824\n",
            "[Step 25740] Loss: 2.3931\n",
            "[Step 25750] Loss: 2.5206\n",
            "[Step 25760] Loss: 2.6429\n",
            "[Step 25770] Loss: 2.3626\n",
            "[Step 25780] Loss: 2.7941\n",
            "[Step 25790] Loss: 3.0511\n",
            "[Step 25800] Loss: 2.5198\n",
            "[Step 25810] Loss: 2.8513\n",
            "[Step 25820] Loss: 2.6350\n",
            "[Step 25830] Loss: 2.7126\n",
            "[Step 25840] Loss: 2.3654\n",
            "[Step 25850] Loss: 2.2575\n",
            "[Step 25860] Loss: 2.6739\n",
            "[Step 25870] Loss: 2.4528\n",
            "[Step 25880] Loss: 2.5716\n",
            "[Step 25890] Loss: 2.7194\n",
            "[Step 25900] Loss: 2.7052\n",
            "[Step 25910] Loss: 2.5388\n",
            "[Step 25920] Loss: 2.7258\n",
            "[Step 25930] Loss: 2.3760\n",
            "[Step 25940] Loss: 2.8583\n",
            "[Step 25950] Loss: 3.1471\n",
            "[Step 25960] Loss: 2.5545\n",
            "[Step 25970] Loss: 3.1153\n",
            "[Step 25980] Loss: 2.8989\n",
            "[Step 25990] Loss: 2.5827\n",
            "[Step 26000] Loss: 2.7955\n",
            "[Step 26010] Loss: 2.7906\n",
            "[Step 26020] Loss: 2.3820\n",
            "[Step 26030] Loss: 2.2373\n",
            "[Step 26040] Loss: 2.4919\n",
            "[Step 26050] Loss: 2.7481\n",
            "[Step 26060] Loss: 2.6499\n",
            "[Step 26070] Loss: 2.4725\n",
            "[Step 26080] Loss: 1.7905\n",
            "[Step 26090] Loss: 2.6957\n",
            "[Step 26100] Loss: 2.5138\n",
            "[Step 26110] Loss: 3.0007\n",
            "[Step 26120] Loss: 2.1079\n",
            "[Step 26130] Loss: 2.1147\n",
            "[Step 26140] Loss: 2.7985\n",
            "[Step 26150] Loss: 2.5015\n",
            "[Step 26160] Loss: 2.7660\n",
            "[Step 26170] Loss: 1.9390\n",
            "[Step 26180] Loss: 2.7336\n",
            "[Step 26190] Loss: 2.4855\n",
            "[Step 26200] Loss: 2.5652\n",
            "[Step 26210] Loss: 2.4091\n",
            "[Step 26220] Loss: 2.4746\n",
            "[Step 26230] Loss: 2.6451\n",
            "[Step 26240] Loss: 2.3150\n",
            "[Step 26250] Loss: 2.5796\n",
            "[Step 26260] Loss: 2.7214\n",
            "[Step 26270] Loss: 2.4029\n",
            "[Step 26280] Loss: 2.8100\n",
            "[Step 26290] Loss: 2.7326\n",
            "[Step 26300] Loss: 2.5799\n",
            "[Step 26310] Loss: 2.4585\n",
            "[Step 26320] Loss: 2.6304\n",
            "[Step 26330] Loss: 2.5341\n",
            "[Step 26340] Loss: 2.8440\n",
            "[Step 26350] Loss: 3.2070\n",
            "[Step 26360] Loss: 2.1115\n",
            "[Step 26370] Loss: 2.6755\n",
            "[Step 26380] Loss: 2.5437\n",
            "[Step 26390] Loss: 2.3165\n",
            "[Step 26400] Loss: 2.7803\n",
            "[Step 26410] Loss: 2.2830\n",
            "[Step 26420] Loss: 2.7008\n",
            "[Step 26430] Loss: 2.6460\n",
            "[Step 26440] Loss: 2.5448\n",
            "[Step 26450] Loss: 2.6129\n",
            "[Step 26460] Loss: 2.6935\n",
            "[Step 26470] Loss: 2.8711\n",
            "[Step 26480] Loss: 2.2380\n",
            "[Step 26490] Loss: 2.6889\n",
            "[Step 26500] Loss: 2.5110\n",
            "[Step 26510] Loss: 2.9034\n",
            "ðŸ“˜ Epoch 39 - Avg Training Loss: 2.6122\n",
            "ðŸ“Š Final Validation â€” Loss: 2.7060 | Accuracy: 0.3281 | Precision: 0.3187\n",
            "[Step 26520] Loss: 3.1342\n",
            "[Step 26530] Loss: 2.7766\n",
            "[Step 26540] Loss: 2.4109\n",
            "[Step 26550] Loss: 2.9083\n",
            "[Step 26560] Loss: 3.2488\n",
            "[Step 26570] Loss: 2.7864\n",
            "[Step 26580] Loss: 2.6733\n",
            "[Step 26590] Loss: 2.8786\n",
            "[Step 26600] Loss: 2.2635\n",
            "[Step 26610] Loss: 2.2048\n",
            "[Step 26620] Loss: 2.1653\n",
            "[Step 26630] Loss: 2.5665\n",
            "[Step 26640] Loss: 2.5403\n",
            "[Step 26650] Loss: 2.2880\n",
            "[Step 26660] Loss: 2.5391\n",
            "[Step 26670] Loss: 2.3033\n",
            "[Step 26680] Loss: 2.3223\n",
            "[Step 26690] Loss: 2.4214\n",
            "[Step 26700] Loss: 2.7686\n",
            "[Step 26710] Loss: 2.1470\n",
            "[Step 26720] Loss: 2.8715\n",
            "[Step 26730] Loss: 2.5054\n",
            "[Step 26740] Loss: 3.2518\n",
            "[Step 26750] Loss: 2.5796\n",
            "[Step 26760] Loss: 2.4159\n",
            "[Step 26770] Loss: 2.8068\n",
            "[Step 26780] Loss: 2.9693\n",
            "[Step 26790] Loss: 2.5625\n",
            "[Step 26800] Loss: 2.4445\n",
            "[Step 26810] Loss: 2.9040\n",
            "[Step 26820] Loss: 2.7355\n",
            "[Step 26830] Loss: 2.6613\n",
            "[Step 26840] Loss: 2.4796\n",
            "[Step 26850] Loss: 2.5351\n",
            "[Step 26860] Loss: 2.2278\n",
            "[Step 26870] Loss: 2.7581\n",
            "[Step 26880] Loss: 2.4519\n",
            "[Step 26890] Loss: 2.6131\n",
            "[Step 26900] Loss: 3.0450\n",
            "[Step 26910] Loss: 2.5888\n",
            "[Step 26920] Loss: 2.5666\n",
            "[Step 26930] Loss: 2.6689\n",
            "[Step 26940] Loss: 2.7634\n",
            "[Step 26950] Loss: 2.4496\n",
            "[Step 26960] Loss: 2.7870\n",
            "[Step 26970] Loss: 2.5439\n",
            "[Step 26980] Loss: 2.6348\n",
            "[Step 26990] Loss: 3.0363\n",
            "[Step 27000] Loss: 2.1887\n",
            "[Step 27010] Loss: 2.6729\n",
            "[Step 27020] Loss: 2.9491\n",
            "[Step 27030] Loss: 2.4563\n",
            "[Step 27040] Loss: 2.8626\n",
            "[Step 27050] Loss: 2.8427\n",
            "[Step 27060] Loss: 2.4770\n",
            "[Step 27070] Loss: 2.6216\n",
            "[Step 27080] Loss: 2.5168\n",
            "[Step 27090] Loss: 2.5901\n",
            "[Step 27100] Loss: 2.7166\n",
            "[Step 27110] Loss: 2.3129\n",
            "[Step 27120] Loss: 3.0700\n",
            "[Step 27130] Loss: 2.6948\n",
            "[Step 27140] Loss: 2.6566\n",
            "[Step 27150] Loss: 2.7844\n",
            "[Step 27160] Loss: 2.5289\n",
            "[Step 27170] Loss: 2.3075\n",
            "[Step 27180] Loss: 2.4906\n",
            "[Step 27190] Loss: 2.9829\n",
            "[Step 27200] Loss: 2.0189\n",
            "[Step 27210] Loss: 3.1311\n",
            "[Step 27220] Loss: 2.5553\n",
            "[Step 27230] Loss: 2.9286\n",
            "[Step 27240] Loss: 3.0668\n",
            "[Step 27250] Loss: 2.7532\n",
            "[Step 27260] Loss: 2.7396\n",
            "[Step 27270] Loss: 2.8261\n",
            "[Step 27280] Loss: 2.9662\n",
            "[Step 27290] Loss: 2.3553\n",
            "[Step 27300] Loss: 2.6264\n",
            "[Step 27310] Loss: 2.5583\n",
            "[Step 27320] Loss: 2.7792\n",
            "[Step 27330] Loss: 3.0246\n",
            "[Step 27340] Loss: 2.5018\n",
            "[Step 27350] Loss: 2.8226\n",
            "[Step 27360] Loss: 2.0847\n",
            "[Step 27370] Loss: 2.1090\n",
            "[Step 27380] Loss: 3.1235\n",
            "[Step 27390] Loss: 2.6494\n",
            "[Step 27400] Loss: 2.6337\n",
            "[Step 27410] Loss: 2.6349\n",
            "[Step 27420] Loss: 2.4789\n",
            "[Step 27430] Loss: 3.3471\n",
            "[Step 27440] Loss: 2.3974\n",
            "[Step 27450] Loss: 2.7271\n",
            "[Step 27460] Loss: 2.6221\n",
            "[Step 27470] Loss: 2.6322\n",
            "[Step 27480] Loss: 2.8950\n",
            "[Step 27490] Loss: 2.9324\n",
            "[Step 27500] Loss: 3.3036\n",
            "[Step 27510] Loss: 2.4942\n",
            "[Step 27520] Loss: 2.5320\n",
            "[Step 27530] Loss: 2.5724\n",
            "[Step 27540] Loss: 2.8028\n",
            "[Step 27550] Loss: 2.9006\n",
            "[Step 27560] Loss: 2.8523\n",
            "[Step 27570] Loss: 2.7531\n",
            "[Step 27580] Loss: 2.5671\n",
            "[Step 27590] Loss: 2.9775\n",
            "[Step 27600] Loss: 2.8441\n",
            "[Step 27610] Loss: 2.5621\n",
            "[Step 27620] Loss: 2.7503\n",
            "[Step 27630] Loss: 2.5985\n",
            "[Step 27640] Loss: 2.7302\n",
            "[Step 27650] Loss: 2.8078\n",
            "[Step 27660] Loss: 2.8922\n",
            "[Step 27670] Loss: 3.0298\n",
            "[Step 27680] Loss: 2.5000\n",
            "[Step 27690] Loss: 2.3180\n",
            "[Step 27700] Loss: 2.7282\n",
            "[Step 27710] Loss: 3.0982\n",
            "[Step 27720] Loss: 2.4483\n",
            "[Step 27730] Loss: 2.6837\n",
            "[Step 27740] Loss: 2.7021\n",
            "[Step 27750] Loss: 2.7002\n",
            "[Step 27760] Loss: 2.9836\n",
            "[Step 27770] Loss: 2.5366\n",
            "[Step 27780] Loss: 2.3040\n",
            "[Step 27790] Loss: 2.5647\n",
            "[Step 27800] Loss: 2.5742\n",
            "[Step 27810] Loss: 2.4699\n",
            "[Step 27820] Loss: 2.6716\n",
            "[Step 27830] Loss: 2.6575\n",
            "[Step 27840] Loss: 2.5577\n",
            "[Step 27850] Loss: 2.9031\n",
            "[Step 27860] Loss: 2.7807\n",
            "[Step 27870] Loss: 2.4570\n",
            "[Step 27880] Loss: 2.9367\n",
            "[Step 27890] Loss: 2.3834\n",
            "[Step 27900] Loss: 2.3377\n",
            "[Step 27910] Loss: 2.4639\n",
            "[Step 27920] Loss: 3.2537\n",
            "[Step 27930] Loss: 2.6178\n",
            "[Step 27940] Loss: 2.5563\n",
            "[Step 27950] Loss: 2.3277\n",
            "[Step 27960] Loss: 2.6393\n",
            "[Step 27970] Loss: 2.8203\n",
            "[Step 27980] Loss: 2.6114\n",
            "[Step 27990] Loss: 2.7215\n",
            "[Step 28000] Loss: 2.7055\n",
            "[Step 28010] Loss: 2.5805\n",
            "[Step 28020] Loss: 2.5450\n",
            "[Step 28030] Loss: 2.9285\n",
            "[Step 28040] Loss: 3.3851\n",
            "[Step 28050] Loss: 2.3744\n",
            "[Step 28060] Loss: 3.0459\n",
            "[Step 28070] Loss: 2.3789\n",
            "[Step 28080] Loss: 2.9518\n",
            "[Step 28090] Loss: 2.7344\n",
            "[Step 28100] Loss: 2.7569\n",
            "[Step 28110] Loss: 2.6464\n",
            "[Step 28120] Loss: 2.6467\n",
            "[Step 28130] Loss: 2.9487\n",
            "[Step 28140] Loss: 2.6048\n",
            "[Step 28150] Loss: 2.3071\n",
            "[Step 28160] Loss: 2.4505\n",
            "[Step 28170] Loss: 2.5009\n",
            "[Step 28180] Loss: 3.3108\n",
            "[Step 28190] Loss: 2.8626\n",
            "[Step 28200] Loss: 2.3161\n",
            "[Step 28210] Loss: 2.6770\n",
            "[Step 28220] Loss: 2.6356\n",
            "[Step 28230] Loss: 2.9697\n",
            "[Step 28240] Loss: 2.6185\n",
            "[Step 28250] Loss: 2.7004\n",
            "[Step 28260] Loss: 3.1805\n",
            "[Step 28270] Loss: 3.1789\n",
            "[Step 28280] Loss: 2.6222\n",
            "[Step 28290] Loss: 3.1007\n",
            "[Step 28300] Loss: 2.7741\n",
            "[Step 28310] Loss: 2.1914\n",
            "[Step 28320] Loss: 2.6888\n",
            "[Step 28330] Loss: 2.4051\n",
            "[Step 28340] Loss: 2.6900\n",
            "[Step 28350] Loss: 2.5297\n",
            "[Step 28360] Loss: 2.3669\n",
            "[Step 28370] Loss: 2.6809\n",
            "[Step 28380] Loss: 3.4489\n",
            "[Step 28390] Loss: 2.5723\n",
            "[Step 28400] Loss: 2.7276\n",
            "[Step 28410] Loss: 2.3799\n",
            "ðŸ“˜ Epoch 40 - Avg Training Loss: 2.6571\n",
            "ðŸ“Š Final Validation â€” Loss: 2.7657 | Accuracy: 0.3128 | Precision: 0.3084\n",
            "âœ… Continued training complete\n"
          ]
        }
      ],
      "source": [
        "new_training_args = {\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"num_additional_epochs\": 15,\n",
        "    \"logging_steps\": 10,\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=new_training_args[\"learning_rate\"],\n",
        "    weight_decay=new_training_args[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "starting_epoch = 25\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(starting_epoch, starting_epoch + new_training_args[\"num_additional_epochs\"]):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % new_training_args[\"logging_steps\"] == 0:\n",
        "            print(f\"[Step {global_step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"ðŸ“˜ Epoch {epoch+1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    evaluate_val(model, val_loader, criterion, device)\n",
        "\n",
        "print(\"âœ… Continued training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qx-NGfPamn2P",
        "outputId": "7a423f61-30a0-4b9e-b9d2-f55c1e193184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 10] Loss: 2.8996\n",
            "[Step 20] Loss: 2.9825\n",
            "[Step 30] Loss: 2.9736\n",
            "[Step 40] Loss: 2.2423\n",
            "[Step 50] Loss: 2.5002\n",
            "[Step 60] Loss: 2.3907\n",
            "[Step 70] Loss: 2.5786\n",
            "[Step 80] Loss: 2.5043\n",
            "[Step 90] Loss: 2.5447\n",
            "[Step 100] Loss: 2.4959\n",
            "[Step 110] Loss: 2.5949\n",
            "[Step 120] Loss: 2.5815\n",
            "[Step 130] Loss: 3.0214\n",
            "[Step 140] Loss: 2.8642\n",
            "[Step 150] Loss: 2.0740\n",
            "[Step 160] Loss: 2.7558\n",
            "[Step 170] Loss: 2.5127\n",
            "[Step 180] Loss: 2.4783\n",
            "[Step 190] Loss: 2.6986\n",
            "[Step 200] Loss: 2.7745\n",
            "[Step 210] Loss: 2.2503\n",
            "[Step 220] Loss: 2.6669\n",
            "[Step 230] Loss: 2.9887\n",
            "[Step 240] Loss: 2.9217\n",
            "[Step 250] Loss: 2.4554\n",
            "[Step 260] Loss: 2.6888\n",
            "[Step 270] Loss: 2.2445\n",
            "[Step 280] Loss: 2.3707\n",
            "[Step 290] Loss: 2.2342\n",
            "[Step 300] Loss: 2.5975\n",
            "[Step 310] Loss: 2.7981\n",
            "[Step 320] Loss: 2.9354\n",
            "[Step 330] Loss: 2.1851\n",
            "[Step 340] Loss: 2.6840\n",
            "[Step 350] Loss: 2.6771\n",
            "[Step 360] Loss: 2.7968\n",
            "[Step 370] Loss: 2.6644\n",
            "[Step 380] Loss: 2.9910\n",
            "[Step 390] Loss: 2.6233\n",
            "[Step 400] Loss: 2.2438\n",
            "[Step 410] Loss: 3.0317\n",
            "[Step 420] Loss: 2.6126\n",
            "[Step 430] Loss: 2.5228\n",
            "[Step 440] Loss: 2.5245\n",
            "[Step 450] Loss: 2.3662\n",
            "[Step 460] Loss: 2.3613\n",
            "[Step 470] Loss: 3.0711\n",
            "[Step 480] Loss: 2.6185\n",
            "[Step 490] Loss: 2.8208\n",
            "[Step 500] Loss: 3.3736\n",
            "[Step 510] Loss: 2.4715\n",
            "[Step 520] Loss: 2.7694\n",
            "[Step 530] Loss: 2.1033\n",
            "[Step 540] Loss: 2.6381\n",
            "[Step 550] Loss: 2.4464\n",
            "[Step 560] Loss: 2.5309\n",
            "[Step 570] Loss: 3.1272\n",
            "[Step 580] Loss: 2.9328\n",
            "[Step 590] Loss: 2.7350\n",
            "[Step 600] Loss: 3.0750\n",
            "[Step 610] Loss: 2.2625\n",
            "[Step 620] Loss: 2.2764\n",
            "[Step 630] Loss: 2.7958\n",
            "[Step 640] Loss: 2.1676\n",
            "[Step 650] Loss: 2.5761\n",
            "[Step 660] Loss: 2.2918\n",
            "[Step 670] Loss: 2.3815\n",
            "[Step 680] Loss: 2.1486\n",
            "[Step 690] Loss: 2.6397\n",
            "[Step 700] Loss: 2.3553\n",
            "[Step 710] Loss: 2.6639\n",
            "[Step 720] Loss: 2.5663\n",
            "[Step 730] Loss: 2.4068\n",
            "[Step 740] Loss: 2.4533\n",
            "[Step 750] Loss: 2.4780\n",
            "[Step 760] Loss: 2.2759\n",
            "[Step 770] Loss: 2.7106\n",
            "[Step 780] Loss: 2.6728\n",
            "[Step 790] Loss: 2.7549\n",
            "[Step 800] Loss: 2.2200\n",
            "[Step 810] Loss: 2.3593\n",
            "[Step 820] Loss: 2.6855\n",
            "[Step 830] Loss: 1.9342\n",
            "[Step 840] Loss: 2.7409\n",
            "[Step 850] Loss: 2.6261\n",
            "[Step 860] Loss: 2.2649\n",
            "[Step 870] Loss: 2.8046\n",
            "[Step 880] Loss: 3.1718\n",
            "[Step 890] Loss: 2.2272\n",
            "[Step 900] Loss: 2.8735\n",
            "[Step 910] Loss: 2.4922\n",
            "[Step 920] Loss: 2.8005\n",
            "[Step 930] Loss: 2.8821\n",
            "[Step 940] Loss: 1.9716\n",
            "[Step 950] Loss: 1.6009\n",
            "[Step 960] Loss: 2.8437\n",
            "[Step 970] Loss: 2.3775\n",
            "[Step 980] Loss: 2.4028\n",
            "[Step 990] Loss: 2.3586\n",
            "[Step 1000] Loss: 3.0963\n",
            "[Step 1010] Loss: 2.3032\n",
            "[Step 1020] Loss: 2.9403\n",
            "[Step 1030] Loss: 2.6054\n",
            "[Step 1040] Loss: 2.6086\n",
            "[Step 1050] Loss: 2.7460\n",
            "[Step 1060] Loss: 2.5256\n",
            "[Step 1070] Loss: 2.5818\n",
            "[Step 1080] Loss: 2.5388\n",
            "[Step 1090] Loss: 3.0507\n",
            "[Step 1100] Loss: 2.5046\n",
            "[Step 1110] Loss: 2.1340\n",
            "[Step 1120] Loss: 3.0914\n",
            "[Step 1130] Loss: 2.2039\n",
            "[Step 1140] Loss: 3.0605\n",
            "[Step 1150] Loss: 2.8224\n",
            "[Step 1160] Loss: 2.4661\n",
            "[Step 1170] Loss: 2.7802\n",
            "[Step 1180] Loss: 3.4720\n",
            "[Step 1190] Loss: 2.5127\n",
            "[Step 1200] Loss: 2.2706\n",
            "[Step 1210] Loss: 2.3419\n",
            "[Step 1220] Loss: 2.5428\n",
            "[Step 1230] Loss: 2.6400\n",
            "[Step 1240] Loss: 2.2978\n",
            "[Step 1250] Loss: 2.8419\n",
            "[Step 1260] Loss: 2.3940\n",
            "[Step 1270] Loss: 2.4392\n",
            "[Step 1280] Loss: 2.4776\n",
            "[Step 1290] Loss: 2.6365\n",
            "[Step 1300] Loss: 2.3864\n",
            "[Step 1310] Loss: 2.6053\n",
            "[Step 1320] Loss: 1.8966\n",
            "[Step 1330] Loss: 2.9967\n",
            "[Step 1340] Loss: 2.9941\n",
            "[Step 1350] Loss: 2.4201\n",
            "[Step 1360] Loss: 2.7906\n",
            "[Step 1370] Loss: 2.5956\n",
            "[Step 1380] Loss: 2.1984\n",
            "[Step 1390] Loss: 2.3466\n",
            "[Step 1400] Loss: 2.6904\n",
            "[Step 1410] Loss: 2.8670\n",
            "[Step 1420] Loss: 2.6371\n",
            "[Step 1430] Loss: 2.5414\n",
            "[Step 1440] Loss: 2.5125\n",
            "[Step 1450] Loss: 2.8444\n",
            "[Step 1460] Loss: 2.7306\n",
            "[Step 1470] Loss: 2.6576\n",
            "[Step 1480] Loss: 2.7301\n",
            "[Step 1490] Loss: 2.8326\n",
            "[Step 1500] Loss: 2.4571\n",
            "[Step 1510] Loss: 2.4121\n",
            "[Step 1520] Loss: 2.8589\n",
            "[Step 1530] Loss: 2.7941\n",
            "[Step 1540] Loss: 2.8950\n",
            "[Step 1550] Loss: 2.6743\n",
            "[Step 1560] Loss: 2.5832\n",
            "[Step 1570] Loss: 2.3919\n",
            "[Step 1580] Loss: 2.6129\n",
            "[Step 1590] Loss: 2.4499\n",
            "[Step 1600] Loss: 2.3312\n",
            "[Step 1610] Loss: 2.8732\n",
            "[Step 1620] Loss: 2.9940\n",
            "[Step 1630] Loss: 2.3440\n",
            "[Step 1640] Loss: 2.7610\n",
            "[Step 1650] Loss: 2.3771\n",
            "[Step 1660] Loss: 2.1358\n",
            "[Step 1670] Loss: 3.1647\n",
            "[Step 1680] Loss: 2.4553\n",
            "[Step 1690] Loss: 2.5736\n",
            "[Step 1700] Loss: 2.3700\n",
            "[Step 1710] Loss: 2.7405\n",
            "[Step 1720] Loss: 2.2048\n",
            "[Step 1730] Loss: 3.3275\n",
            "[Step 1740] Loss: 2.6196\n",
            "[Step 1750] Loss: 2.2328\n",
            "[Step 1760] Loss: 2.6277\n",
            "[Step 1770] Loss: 2.7248\n",
            "[Step 1780] Loss: 2.5346\n",
            "[Step 1790] Loss: 2.6743\n",
            "[Step 1800] Loss: 2.5281\n",
            "[Step 1810] Loss: 2.2339\n",
            "[Step 1820] Loss: 2.5586\n",
            "[Step 1830] Loss: 2.2228\n",
            "[Step 1840] Loss: 2.6804\n",
            "[Step 1850] Loss: 2.4321\n",
            "[Step 1860] Loss: 2.3881\n",
            "[Step 1870] Loss: 2.4898\n",
            "[Step 1880] Loss: 2.8894\n",
            "[Step 1890] Loss: 2.6485\n",
            "ðŸ“˜ Epoch 41 - Avg Training Loss: 2.5672\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6264 | Accuracy: 0.3448 | Precision: 0.3288\n",
            "[Step 1900] Loss: 2.1061\n",
            "[Step 1910] Loss: 2.1826\n",
            "[Step 1920] Loss: 2.6105\n",
            "[Step 1930] Loss: 2.8598\n",
            "[Step 1940] Loss: 3.6275\n",
            "[Step 1950] Loss: 2.6762\n",
            "[Step 1960] Loss: 1.9926\n",
            "[Step 1970] Loss: 2.1934\n",
            "[Step 1980] Loss: 2.6460\n",
            "[Step 1990] Loss: 2.0277\n",
            "[Step 2000] Loss: 2.9290\n",
            "[Step 2010] Loss: 2.4564\n",
            "[Step 2020] Loss: 2.5741\n",
            "[Step 2030] Loss: 2.3348\n",
            "[Step 2040] Loss: 2.2204\n",
            "[Step 2050] Loss: 2.3483\n",
            "[Step 2060] Loss: 2.2961\n",
            "[Step 2070] Loss: 2.3203\n",
            "[Step 2080] Loss: 2.7367\n",
            "[Step 2090] Loss: 2.7318\n",
            "[Step 2100] Loss: 2.3729\n",
            "[Step 2110] Loss: 3.1605\n",
            "[Step 2120] Loss: 2.4557\n",
            "[Step 2130] Loss: 2.4595\n",
            "[Step 2140] Loss: 2.7478\n",
            "[Step 2150] Loss: 2.8263\n",
            "[Step 2160] Loss: 2.7779\n",
            "[Step 2170] Loss: 2.4188\n",
            "[Step 2180] Loss: 2.6689\n",
            "[Step 2190] Loss: 2.6836\n",
            "[Step 2200] Loss: 2.2754\n",
            "[Step 2210] Loss: 2.5641\n",
            "[Step 2220] Loss: 2.7127\n",
            "[Step 2230] Loss: 2.5233\n",
            "[Step 2240] Loss: 2.4386\n",
            "[Step 2250] Loss: 2.7595\n",
            "[Step 2260] Loss: 3.1137\n",
            "[Step 2270] Loss: 2.2376\n",
            "[Step 2280] Loss: 2.6119\n",
            "[Step 2290] Loss: 2.3848\n",
            "[Step 2300] Loss: 2.8125\n",
            "[Step 2310] Loss: 2.8619\n",
            "[Step 2320] Loss: 2.2148\n",
            "[Step 2330] Loss: 2.3378\n",
            "[Step 2340] Loss: 2.5451\n",
            "[Step 2350] Loss: 2.4699\n",
            "[Step 2360] Loss: 2.2545\n",
            "[Step 2370] Loss: 2.5086\n",
            "[Step 2380] Loss: 2.8388\n",
            "[Step 2390] Loss: 2.3780\n",
            "[Step 2400] Loss: 2.7381\n",
            "[Step 2410] Loss: 2.6200\n",
            "[Step 2420] Loss: 2.5762\n",
            "[Step 2430] Loss: 2.5036\n",
            "[Step 2440] Loss: 2.7975\n",
            "[Step 2450] Loss: 2.5363\n",
            "[Step 2460] Loss: 2.7330\n",
            "[Step 2470] Loss: 2.4540\n",
            "[Step 2480] Loss: 2.8890\n",
            "[Step 2490] Loss: 2.3150\n",
            "[Step 2500] Loss: 2.3122\n",
            "[Step 2510] Loss: 2.6960\n",
            "[Step 2520] Loss: 2.3778\n",
            "[Step 2530] Loss: 2.8111\n",
            "[Step 2540] Loss: 2.9249\n",
            "[Step 2550] Loss: 2.6362\n",
            "[Step 2560] Loss: 2.5122\n",
            "[Step 2570] Loss: 2.3409\n",
            "[Step 2580] Loss: 2.4259\n",
            "[Step 2590] Loss: 2.3500\n",
            "[Step 2600] Loss: 2.2793\n",
            "[Step 2610] Loss: 2.5505\n",
            "[Step 2620] Loss: 2.3024\n",
            "[Step 2630] Loss: 2.7663\n",
            "[Step 2640] Loss: 2.6228\n",
            "[Step 2650] Loss: 2.6311\n",
            "[Step 2660] Loss: 2.5365\n",
            "[Step 2670] Loss: 2.3752\n",
            "[Step 2680] Loss: 3.1906\n",
            "[Step 2690] Loss: 2.5082\n",
            "[Step 2700] Loss: 2.0552\n",
            "[Step 2710] Loss: 2.2045\n",
            "[Step 2720] Loss: 2.6352\n",
            "[Step 2730] Loss: 2.3623\n",
            "[Step 2740] Loss: 2.6256\n",
            "[Step 2750] Loss: 2.5107\n",
            "[Step 2760] Loss: 2.3049\n",
            "[Step 2770] Loss: 2.8269\n",
            "[Step 2780] Loss: 2.5701\n",
            "[Step 2790] Loss: 2.3739\n",
            "[Step 2800] Loss: 2.6680\n",
            "[Step 2810] Loss: 2.6881\n",
            "[Step 2820] Loss: 2.1758\n",
            "[Step 2830] Loss: 2.5730\n",
            "[Step 2840] Loss: 1.8226\n",
            "[Step 2850] Loss: 2.8722\n",
            "[Step 2860] Loss: 2.3947\n",
            "[Step 2870] Loss: 2.1471\n",
            "[Step 2880] Loss: 2.7849\n",
            "[Step 2890] Loss: 2.5100\n",
            "[Step 2900] Loss: 3.0447\n",
            "[Step 2910] Loss: 2.8668\n",
            "[Step 2920] Loss: 2.5876\n",
            "[Step 2930] Loss: 2.9426\n",
            "[Step 2940] Loss: 2.8242\n",
            "[Step 2950] Loss: 2.1805\n",
            "[Step 2960] Loss: 2.2473\n",
            "[Step 2970] Loss: 2.4024\n",
            "[Step 2980] Loss: 2.1698\n",
            "[Step 2990] Loss: 2.6057\n",
            "[Step 3000] Loss: 2.5203\n",
            "[Step 3010] Loss: 2.5179\n",
            "[Step 3020] Loss: 2.5780\n",
            "[Step 3030] Loss: 2.6214\n",
            "[Step 3040] Loss: 2.2920\n",
            "[Step 3050] Loss: 2.4935\n",
            "[Step 3060] Loss: 2.2736\n",
            "[Step 3070] Loss: 2.6734\n",
            "[Step 3080] Loss: 2.9843\n",
            "[Step 3090] Loss: 2.3077\n",
            "[Step 3100] Loss: 2.4093\n",
            "[Step 3110] Loss: 2.7245\n",
            "[Step 3120] Loss: 2.8656\n",
            "[Step 3130] Loss: 2.8751\n",
            "[Step 3140] Loss: 2.7719\n",
            "[Step 3150] Loss: 2.9427\n",
            "[Step 3160] Loss: 2.7836\n",
            "[Step 3170] Loss: 2.5984\n",
            "[Step 3180] Loss: 2.8709\n",
            "[Step 3190] Loss: 2.4231\n",
            "[Step 3200] Loss: 2.7077\n",
            "[Step 3210] Loss: 2.3930\n",
            "[Step 3220] Loss: 2.9694\n",
            "[Step 3230] Loss: 2.6577\n",
            "[Step 3240] Loss: 2.3026\n",
            "[Step 3250] Loss: 2.3417\n",
            "[Step 3260] Loss: 2.9500\n",
            "[Step 3270] Loss: 2.3682\n",
            "[Step 3280] Loss: 2.6122\n",
            "[Step 3290] Loss: 2.6830\n",
            "[Step 3300] Loss: 2.3488\n",
            "[Step 3310] Loss: 2.8848\n",
            "[Step 3320] Loss: 2.6667\n",
            "[Step 3330] Loss: 2.9696\n",
            "[Step 3340] Loss: 2.4252\n",
            "[Step 3350] Loss: 2.6846\n",
            "[Step 3360] Loss: 2.4970\n",
            "[Step 3370] Loss: 2.5785\n",
            "[Step 3380] Loss: 2.6201\n",
            "[Step 3390] Loss: 2.6743\n",
            "[Step 3400] Loss: 2.4194\n",
            "[Step 3410] Loss: 2.4721\n",
            "[Step 3420] Loss: 2.4215\n",
            "[Step 3430] Loss: 2.8063\n",
            "[Step 3440] Loss: 2.0235\n",
            "[Step 3450] Loss: 2.7930\n",
            "[Step 3460] Loss: 2.5589\n",
            "[Step 3470] Loss: 2.3960\n",
            "[Step 3480] Loss: 2.3360\n",
            "[Step 3490] Loss: 2.3921\n",
            "[Step 3500] Loss: 2.3059\n",
            "[Step 3510] Loss: 2.8976\n",
            "[Step 3520] Loss: 2.5723\n",
            "[Step 3530] Loss: 2.3770\n",
            "[Step 3540] Loss: 2.0481\n",
            "[Step 3550] Loss: 2.3755\n",
            "[Step 3560] Loss: 3.0164\n",
            "[Step 3570] Loss: 2.2582\n",
            "[Step 3580] Loss: 2.3418\n",
            "[Step 3590] Loss: 2.4186\n",
            "[Step 3600] Loss: 2.5040\n",
            "[Step 3610] Loss: 2.9166\n",
            "[Step 3620] Loss: 2.4467\n",
            "[Step 3630] Loss: 2.7823\n",
            "[Step 3640] Loss: 2.9471\n",
            "[Step 3650] Loss: 2.8195\n",
            "[Step 3660] Loss: 2.0093\n",
            "[Step 3670] Loss: 2.1843\n",
            "[Step 3680] Loss: 2.5523\n",
            "[Step 3690] Loss: 2.6014\n",
            "[Step 3700] Loss: 2.2548\n",
            "[Step 3710] Loss: 2.6065\n",
            "[Step 3720] Loss: 2.9114\n",
            "[Step 3730] Loss: 2.7299\n",
            "[Step 3740] Loss: 2.6407\n",
            "[Step 3750] Loss: 2.7673\n",
            "[Step 3760] Loss: 2.4774\n",
            "[Step 3770] Loss: 2.5580\n",
            "[Step 3780] Loss: 2.3971\n",
            "ðŸ“˜ Epoch 42 - Avg Training Loss: 2.5669\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6376 | Accuracy: 0.3434 | Precision: 0.3243\n",
            "[Step 3790] Loss: 2.8560\n",
            "[Step 3800] Loss: 2.4692\n",
            "[Step 3810] Loss: 2.3746\n",
            "[Step 3820] Loss: 2.2236\n",
            "[Step 3830] Loss: 2.5156\n",
            "[Step 3840] Loss: 2.7975\n",
            "[Step 3850] Loss: 2.4028\n",
            "[Step 3860] Loss: 2.2696\n",
            "[Step 3870] Loss: 2.6392\n",
            "[Step 3880] Loss: 2.7310\n",
            "[Step 3890] Loss: 2.7014\n",
            "[Step 3900] Loss: 2.2156\n",
            "[Step 3910] Loss: 2.4922\n",
            "[Step 3920] Loss: 2.4018\n",
            "[Step 3930] Loss: 2.6336\n",
            "[Step 3940] Loss: 2.4967\n",
            "[Step 3950] Loss: 2.3539\n",
            "[Step 3960] Loss: 2.4636\n",
            "[Step 3970] Loss: 2.4364\n",
            "[Step 3980] Loss: 2.5119\n",
            "[Step 3990] Loss: 2.9227\n",
            "[Step 4000] Loss: 3.1242\n",
            "[Step 4010] Loss: 2.3170\n",
            "[Step 4020] Loss: 2.3777\n",
            "[Step 4030] Loss: 2.6411\n",
            "[Step 4040] Loss: 2.5572\n",
            "[Step 4050] Loss: 2.1211\n",
            "[Step 4060] Loss: 2.4082\n",
            "[Step 4070] Loss: 2.4214\n",
            "[Step 4080] Loss: 2.9700\n",
            "[Step 4090] Loss: 2.4775\n",
            "[Step 4100] Loss: 2.5329\n",
            "[Step 4110] Loss: 2.7552\n",
            "[Step 4120] Loss: 2.4282\n",
            "[Step 4130] Loss: 2.9309\n",
            "[Step 4140] Loss: 2.5567\n",
            "[Step 4150] Loss: 3.0616\n",
            "[Step 4160] Loss: 2.8071\n",
            "[Step 4170] Loss: 2.8907\n",
            "[Step 4180] Loss: 2.2814\n",
            "[Step 4190] Loss: 2.3991\n",
            "[Step 4200] Loss: 2.0900\n",
            "[Step 4210] Loss: 2.7270\n",
            "[Step 4220] Loss: 3.3174\n",
            "[Step 4230] Loss: 2.3072\n",
            "[Step 4240] Loss: 2.5506\n",
            "[Step 4250] Loss: 2.9947\n",
            "[Step 4260] Loss: 3.0194\n",
            "[Step 4270] Loss: 2.6744\n",
            "[Step 4280] Loss: 2.6704\n",
            "[Step 4290] Loss: 2.8645\n",
            "[Step 4300] Loss: 2.3708\n",
            "[Step 4310] Loss: 2.7082\n",
            "[Step 4320] Loss: 2.5757\n",
            "[Step 4330] Loss: 2.5376\n",
            "[Step 4340] Loss: 2.2302\n",
            "[Step 4350] Loss: 2.3958\n",
            "[Step 4360] Loss: 2.1222\n",
            "[Step 4370] Loss: 2.5484\n",
            "[Step 4380] Loss: 2.5891\n",
            "[Step 4390] Loss: 2.5528\n",
            "[Step 4400] Loss: 2.6744\n",
            "[Step 4410] Loss: 2.1475\n",
            "[Step 4420] Loss: 2.6909\n",
            "[Step 4430] Loss: 2.7667\n",
            "[Step 4440] Loss: 2.4154\n",
            "[Step 4450] Loss: 2.4756\n",
            "[Step 4460] Loss: 2.5699\n",
            "[Step 4470] Loss: 2.5886\n",
            "[Step 4480] Loss: 2.5805\n",
            "[Step 4490] Loss: 2.8866\n",
            "[Step 4500] Loss: 2.9562\n",
            "[Step 4510] Loss: 2.5436\n",
            "[Step 4520] Loss: 2.9706\n",
            "[Step 4530] Loss: 2.9141\n",
            "[Step 4540] Loss: 2.4103\n",
            "[Step 4550] Loss: 3.0314\n",
            "[Step 4560] Loss: 2.1529\n",
            "[Step 4570] Loss: 2.2887\n",
            "[Step 4580] Loss: 2.4152\n",
            "[Step 4590] Loss: 2.3874\n",
            "[Step 4600] Loss: 2.6680\n",
            "[Step 4610] Loss: 2.5358\n",
            "[Step 4620] Loss: 2.7809\n",
            "[Step 4630] Loss: 2.3347\n",
            "[Step 4640] Loss: 1.9890\n",
            "[Step 4650] Loss: 2.4949\n",
            "[Step 4660] Loss: 2.9632\n",
            "[Step 4670] Loss: 2.4366\n",
            "[Step 4680] Loss: 2.5794\n",
            "[Step 4690] Loss: 2.7067\n",
            "[Step 4700] Loss: 2.2563\n",
            "[Step 4710] Loss: 2.7101\n",
            "[Step 4720] Loss: 2.8778\n",
            "[Step 4730] Loss: 2.5480\n",
            "[Step 4740] Loss: 2.4290\n",
            "[Step 4750] Loss: 2.1325\n",
            "[Step 4760] Loss: 2.6314\n",
            "[Step 4770] Loss: 2.6268\n",
            "[Step 4780] Loss: 2.5596\n",
            "[Step 4790] Loss: 2.6661\n",
            "[Step 4800] Loss: 2.8105\n",
            "[Step 4810] Loss: 2.1662\n",
            "[Step 4820] Loss: 2.9939\n",
            "[Step 4830] Loss: 2.1427\n",
            "[Step 4840] Loss: 2.5765\n",
            "[Step 4850] Loss: 2.3201\n",
            "[Step 4860] Loss: 2.3701\n",
            "[Step 4870] Loss: 2.5793\n",
            "[Step 4880] Loss: 2.2279\n",
            "[Step 4890] Loss: 2.4351\n",
            "[Step 4900] Loss: 2.4472\n",
            "[Step 4910] Loss: 2.4698\n",
            "[Step 4920] Loss: 2.5358\n",
            "[Step 4930] Loss: 3.0474\n",
            "[Step 4940] Loss: 3.1191\n",
            "[Step 4950] Loss: 2.1991\n",
            "[Step 4960] Loss: 2.4239\n",
            "[Step 4970] Loss: 2.5565\n",
            "[Step 4980] Loss: 2.4705\n",
            "[Step 4990] Loss: 2.7551\n",
            "[Step 5000] Loss: 2.6583\n",
            "[Step 5010] Loss: 2.5511\n",
            "[Step 5020] Loss: 2.2756\n",
            "[Step 5030] Loss: 2.4373\n",
            "[Step 5040] Loss: 3.0643\n",
            "[Step 5050] Loss: 3.1883\n",
            "[Step 5060] Loss: 2.6636\n",
            "[Step 5070] Loss: 2.8450\n",
            "[Step 5080] Loss: 2.7286\n",
            "[Step 5090] Loss: 2.9883\n",
            "[Step 5100] Loss: 2.6163\n",
            "[Step 5110] Loss: 2.6832\n",
            "[Step 5120] Loss: 2.7209\n",
            "[Step 5130] Loss: 2.5556\n",
            "[Step 5140] Loss: 2.5998\n",
            "[Step 5150] Loss: 2.5370\n",
            "[Step 5160] Loss: 2.3247\n",
            "[Step 5170] Loss: 2.6163\n",
            "[Step 5180] Loss: 2.5471\n",
            "[Step 5190] Loss: 2.5142\n",
            "[Step 5200] Loss: 2.9845\n",
            "[Step 5210] Loss: 2.5768\n",
            "[Step 5220] Loss: 2.7724\n",
            "[Step 5230] Loss: 2.1945\n",
            "[Step 5240] Loss: 2.4199\n",
            "[Step 5250] Loss: 3.1586\n",
            "[Step 5260] Loss: 2.5491\n",
            "[Step 5270] Loss: 3.2965\n",
            "[Step 5280] Loss: 2.3583\n",
            "[Step 5290] Loss: 2.7109\n",
            "[Step 5300] Loss: 2.7606\n",
            "[Step 5310] Loss: 2.5918\n",
            "[Step 5320] Loss: 2.0864\n",
            "[Step 5330] Loss: 2.6696\n",
            "[Step 5340] Loss: 2.1037\n",
            "[Step 5350] Loss: 2.4186\n",
            "[Step 5360] Loss: 2.1522\n",
            "[Step 5370] Loss: 2.6173\n",
            "[Step 5380] Loss: 2.5516\n",
            "[Step 5390] Loss: 2.5426\n",
            "[Step 5400] Loss: 2.4913\n",
            "[Step 5410] Loss: 2.9503\n",
            "[Step 5420] Loss: 2.8203\n",
            "[Step 5430] Loss: 2.9689\n",
            "[Step 5440] Loss: 2.4749\n",
            "[Step 5450] Loss: 2.3833\n",
            "[Step 5460] Loss: 2.3588\n",
            "[Step 5470] Loss: 2.3925\n",
            "[Step 5480] Loss: 2.7314\n",
            "[Step 5490] Loss: 2.0019\n",
            "[Step 5500] Loss: 3.0648\n",
            "[Step 5510] Loss: 2.4993\n",
            "[Step 5520] Loss: 2.6789\n",
            "[Step 5530] Loss: 2.5322\n",
            "[Step 5540] Loss: 2.3471\n",
            "[Step 5550] Loss: 2.6000\n",
            "[Step 5560] Loss: 2.3807\n",
            "[Step 5570] Loss: 2.8042\n",
            "[Step 5580] Loss: 2.2961\n",
            "[Step 5590] Loss: 2.4175\n",
            "[Step 5600] Loss: 2.4360\n",
            "[Step 5610] Loss: 2.0732\n",
            "[Step 5620] Loss: 2.9046\n",
            "[Step 5630] Loss: 2.5573\n",
            "[Step 5640] Loss: 2.7222\n",
            "[Step 5650] Loss: 2.7022\n",
            "[Step 5660] Loss: 2.7957\n",
            "[Step 5670] Loss: 2.4252\n",
            "[Step 5680] Loss: 2.5391\n",
            "ðŸ“˜ Epoch 43 - Avg Training Loss: 2.5717\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6369 | Accuracy: 0.3469 | Precision: 0.3262\n",
            "[Step 5690] Loss: 2.9724\n",
            "[Step 5700] Loss: 2.8280\n",
            "[Step 5710] Loss: 2.2455\n",
            "[Step 5720] Loss: 2.4913\n",
            "[Step 5730] Loss: 2.5683\n",
            "[Step 5740] Loss: 2.3310\n",
            "[Step 5750] Loss: 2.3310\n",
            "[Step 5760] Loss: 2.3808\n",
            "[Step 5770] Loss: 2.4666\n",
            "[Step 5780] Loss: 2.5863\n",
            "[Step 5790] Loss: 2.5357\n",
            "[Step 5800] Loss: 2.3774\n",
            "[Step 5810] Loss: 2.9546\n",
            "[Step 5820] Loss: 2.8855\n",
            "[Step 5830] Loss: 2.3673\n",
            "[Step 5840] Loss: 2.1883\n",
            "[Step 5850] Loss: 2.5468\n",
            "[Step 5860] Loss: 2.8023\n",
            "[Step 5870] Loss: 2.6469\n",
            "[Step 5880] Loss: 2.5203\n",
            "[Step 5890] Loss: 2.8033\n",
            "[Step 5900] Loss: 2.6808\n",
            "[Step 5910] Loss: 2.2902\n",
            "[Step 5920] Loss: 2.2671\n",
            "[Step 5930] Loss: 2.9073\n",
            "[Step 5940] Loss: 2.9918\n",
            "[Step 5950] Loss: 2.4641\n",
            "[Step 5960] Loss: 2.0485\n",
            "[Step 5970] Loss: 2.2224\n",
            "[Step 5980] Loss: 2.8514\n",
            "[Step 5990] Loss: 2.7360\n",
            "[Step 6000] Loss: 2.7450\n",
            "[Step 6010] Loss: 2.3656\n",
            "[Step 6020] Loss: 2.5663\n",
            "[Step 6030] Loss: 2.5386\n",
            "[Step 6040] Loss: 2.3837\n",
            "[Step 6050] Loss: 2.6233\n",
            "[Step 6060] Loss: 2.4350\n",
            "[Step 6070] Loss: 2.4682\n",
            "[Step 6080] Loss: 2.2974\n",
            "[Step 6090] Loss: 2.5401\n",
            "[Step 6100] Loss: 2.5990\n",
            "[Step 6110] Loss: 2.1909\n",
            "[Step 6120] Loss: 2.4327\n",
            "[Step 6130] Loss: 2.2885\n",
            "[Step 6140] Loss: 2.9747\n",
            "[Step 6150] Loss: 2.7109\n",
            "[Step 6160] Loss: 2.3995\n",
            "[Step 6170] Loss: 2.4776\n",
            "[Step 6180] Loss: 2.5219\n",
            "[Step 6190] Loss: 2.7320\n",
            "[Step 6200] Loss: 2.5734\n",
            "[Step 6210] Loss: 2.7884\n",
            "[Step 6220] Loss: 2.5612\n",
            "[Step 6230] Loss: 2.6695\n",
            "[Step 6240] Loss: 2.5060\n",
            "[Step 6250] Loss: 2.2555\n",
            "[Step 6260] Loss: 2.7409\n",
            "[Step 6270] Loss: 2.5953\n",
            "[Step 6280] Loss: 2.6537\n",
            "[Step 6290] Loss: 2.8517\n",
            "[Step 6300] Loss: 2.4240\n",
            "[Step 6310] Loss: 2.8670\n",
            "[Step 6320] Loss: 2.4325\n",
            "[Step 6330] Loss: 2.3312\n",
            "[Step 6340] Loss: 3.2633\n",
            "[Step 6350] Loss: 2.6341\n",
            "[Step 6360] Loss: 2.2093\n",
            "[Step 6370] Loss: 2.9036\n",
            "[Step 6380] Loss: 2.4685\n",
            "[Step 6390] Loss: 2.5759\n",
            "[Step 6400] Loss: 2.5811\n",
            "[Step 6410] Loss: 2.6815\n",
            "[Step 6420] Loss: 2.0891\n",
            "[Step 6430] Loss: 2.3509\n",
            "[Step 6440] Loss: 2.3130\n",
            "[Step 6450] Loss: 2.5438\n",
            "[Step 6460] Loss: 2.7709\n",
            "[Step 6470] Loss: 2.4131\n",
            "[Step 6480] Loss: 3.1257\n",
            "[Step 6490] Loss: 2.9812\n",
            "[Step 6500] Loss: 2.5358\n",
            "[Step 6510] Loss: 2.9013\n",
            "[Step 6520] Loss: 2.7230\n",
            "[Step 6530] Loss: 2.4073\n",
            "[Step 6540] Loss: 2.2024\n",
            "[Step 6550] Loss: 2.7604\n",
            "[Step 6560] Loss: 2.6299\n",
            "[Step 6570] Loss: 2.3495\n",
            "[Step 6580] Loss: 2.5062\n",
            "[Step 6590] Loss: 2.6066\n",
            "[Step 6600] Loss: 2.2219\n",
            "[Step 6610] Loss: 2.5848\n",
            "[Step 6620] Loss: 2.3318\n",
            "[Step 6630] Loss: 2.1794\n",
            "[Step 6640] Loss: 2.2796\n",
            "[Step 6650] Loss: 3.0664\n",
            "[Step 6660] Loss: 2.8685\n",
            "[Step 6670] Loss: 2.3252\n",
            "[Step 6680] Loss: 2.5974\n",
            "[Step 6690] Loss: 2.2743\n",
            "[Step 6700] Loss: 2.8767\n",
            "[Step 6710] Loss: 2.9569\n",
            "[Step 6720] Loss: 2.7163\n",
            "[Step 6730] Loss: 2.9973\n",
            "[Step 6740] Loss: 2.6480\n",
            "[Step 6750] Loss: 2.8231\n",
            "[Step 6760] Loss: 2.5036\n",
            "[Step 6770] Loss: 2.7975\n",
            "[Step 6780] Loss: 2.0870\n",
            "[Step 6790] Loss: 2.7247\n",
            "[Step 6800] Loss: 2.9135\n",
            "[Step 6810] Loss: 2.4054\n",
            "[Step 6820] Loss: 2.2639\n",
            "[Step 6830] Loss: 2.8647\n",
            "[Step 6840] Loss: 2.0372\n",
            "[Step 6850] Loss: 2.5295\n",
            "[Step 6860] Loss: 2.2864\n",
            "[Step 6870] Loss: 2.3152\n",
            "[Step 6880] Loss: 2.4844\n",
            "[Step 6890] Loss: 2.2917\n",
            "[Step 6900] Loss: 3.6010\n",
            "[Step 6910] Loss: 2.5537\n",
            "[Step 6920] Loss: 2.4795\n",
            "[Step 6930] Loss: 2.6503\n",
            "[Step 6940] Loss: 2.3095\n",
            "[Step 6950] Loss: 2.6518\n",
            "[Step 6960] Loss: 2.7191\n",
            "[Step 6970] Loss: 3.0977\n",
            "[Step 6980] Loss: 2.1502\n",
            "[Step 6990] Loss: 2.8346\n",
            "[Step 7000] Loss: 2.2235\n",
            "[Step 7010] Loss: 2.9698\n",
            "[Step 7020] Loss: 2.4210\n",
            "[Step 7030] Loss: 2.3127\n",
            "[Step 7040] Loss: 2.6770\n",
            "[Step 7050] Loss: 2.3295\n",
            "[Step 7060] Loss: 2.6232\n",
            "[Step 7070] Loss: 2.6180\n",
            "[Step 7080] Loss: 2.3210\n",
            "[Step 7090] Loss: 2.5629\n",
            "[Step 7100] Loss: 2.5790\n",
            "[Step 7110] Loss: 2.5401\n",
            "[Step 7120] Loss: 2.3903\n",
            "[Step 7130] Loss: 2.8736\n",
            "[Step 7140] Loss: 2.9895\n",
            "[Step 7150] Loss: 2.7049\n",
            "[Step 7160] Loss: 2.1542\n",
            "[Step 7170] Loss: 2.9248\n",
            "[Step 7180] Loss: 3.2489\n",
            "[Step 7190] Loss: 2.8471\n",
            "[Step 7200] Loss: 2.1873\n",
            "[Step 7210] Loss: 2.7801\n",
            "[Step 7220] Loss: 2.7524\n",
            "[Step 7230] Loss: 2.9139\n",
            "[Step 7240] Loss: 2.9064\n",
            "[Step 7250] Loss: 2.5892\n",
            "[Step 7260] Loss: 3.1820\n",
            "[Step 7270] Loss: 2.6687\n",
            "[Step 7280] Loss: 2.5180\n",
            "[Step 7290] Loss: 2.9588\n",
            "[Step 7300] Loss: 2.0471\n",
            "[Step 7310] Loss: 2.6783\n",
            "[Step 7320] Loss: 2.0594\n",
            "[Step 7330] Loss: 2.2957\n",
            "[Step 7340] Loss: 2.4004\n",
            "[Step 7350] Loss: 2.7343\n",
            "[Step 7360] Loss: 2.5415\n",
            "[Step 7370] Loss: 2.2426\n",
            "[Step 7380] Loss: 2.4221\n",
            "[Step 7390] Loss: 2.5646\n",
            "[Step 7400] Loss: 2.8344\n",
            "[Step 7410] Loss: 2.2700\n",
            "[Step 7420] Loss: 2.9166\n",
            "[Step 7430] Loss: 2.6284\n",
            "[Step 7440] Loss: 2.4325\n",
            "[Step 7450] Loss: 2.4855\n",
            "[Step 7460] Loss: 2.9480\n",
            "[Step 7470] Loss: 3.1107\n",
            "[Step 7480] Loss: 2.5094\n",
            "[Step 7490] Loss: 2.6098\n",
            "[Step 7500] Loss: 2.5209\n",
            "[Step 7510] Loss: 2.7997\n",
            "[Step 7520] Loss: 2.2075\n",
            "[Step 7530] Loss: 2.5267\n",
            "[Step 7540] Loss: 2.4684\n",
            "[Step 7550] Loss: 2.6094\n",
            "[Step 7560] Loss: 2.8403\n",
            "[Step 7570] Loss: 2.5702\n",
            "ðŸ“˜ Epoch 44 - Avg Training Loss: 2.5738\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6410 | Accuracy: 0.3459 | Precision: 0.3292\n",
            "[Step 7580] Loss: 2.1960\n",
            "[Step 7590] Loss: 2.6498\n",
            "[Step 7600] Loss: 2.7840\n",
            "[Step 7610] Loss: 2.4656\n",
            "[Step 7620] Loss: 2.8296\n",
            "[Step 7630] Loss: 2.4533\n",
            "[Step 7640] Loss: 2.8255\n",
            "[Step 7650] Loss: 2.0526\n",
            "[Step 7660] Loss: 2.8565\n",
            "[Step 7670] Loss: 2.7797\n",
            "[Step 7680] Loss: 2.0218\n",
            "[Step 7690] Loss: 2.4558\n",
            "[Step 7700] Loss: 2.5505\n",
            "[Step 7710] Loss: 2.3559\n",
            "[Step 7720] Loss: 2.5740\n",
            "[Step 7730] Loss: 2.8893\n",
            "[Step 7740] Loss: 2.5526\n",
            "[Step 7750] Loss: 2.0224\n",
            "[Step 7760] Loss: 2.6421\n",
            "[Step 7770] Loss: 2.6649\n",
            "[Step 7780] Loss: 2.3173\n",
            "[Step 7790] Loss: 2.4756\n",
            "[Step 7800] Loss: 2.7528\n",
            "[Step 7810] Loss: 2.7193\n",
            "[Step 7820] Loss: 2.7798\n",
            "[Step 7830] Loss: 2.4577\n",
            "[Step 7840] Loss: 2.4889\n",
            "[Step 7850] Loss: 2.8947\n",
            "[Step 7860] Loss: 2.7366\n",
            "[Step 7870] Loss: 2.5787\n",
            "[Step 7880] Loss: 2.8272\n",
            "[Step 7890] Loss: 2.6451\n",
            "[Step 7900] Loss: 2.6901\n",
            "[Step 7910] Loss: 2.3250\n",
            "[Step 7920] Loss: 2.5206\n",
            "[Step 7930] Loss: 2.0098\n",
            "[Step 7940] Loss: 2.5831\n",
            "[Step 7950] Loss: 2.2643\n",
            "[Step 7960] Loss: 2.4688\n",
            "[Step 7970] Loss: 2.9386\n",
            "[Step 7980] Loss: 3.0008\n",
            "[Step 7990] Loss: 2.4140\n",
            "[Step 8000] Loss: 2.6233\n",
            "[Step 8010] Loss: 2.7694\n",
            "[Step 8020] Loss: 2.9524\n",
            "[Step 8030] Loss: 2.1068\n",
            "[Step 8040] Loss: 2.3873\n",
            "[Step 8050] Loss: 2.1087\n",
            "[Step 8060] Loss: 2.4994\n",
            "[Step 8070] Loss: 2.4204\n",
            "[Step 8080] Loss: 2.5382\n",
            "[Step 8090] Loss: 2.4063\n",
            "[Step 8100] Loss: 2.3488\n",
            "[Step 8110] Loss: 2.5665\n",
            "[Step 8120] Loss: 2.7250\n",
            "[Step 8130] Loss: 2.8380\n",
            "[Step 8140] Loss: 2.3859\n",
            "[Step 8150] Loss: 2.7562\n",
            "[Step 8160] Loss: 2.8660\n",
            "[Step 8170] Loss: 2.4266\n",
            "[Step 8180] Loss: 2.7148\n",
            "[Step 8190] Loss: 2.4151\n",
            "[Step 8200] Loss: 3.0620\n",
            "[Step 8210] Loss: 2.4220\n",
            "[Step 8220] Loss: 2.2478\n",
            "[Step 8230] Loss: 2.9794\n",
            "[Step 8240] Loss: 2.7199\n",
            "[Step 8250] Loss: 2.9967\n",
            "[Step 8260] Loss: 2.4786\n",
            "[Step 8270] Loss: 2.4002\n",
            "[Step 8280] Loss: 2.7529\n",
            "[Step 8290] Loss: 2.7563\n",
            "[Step 8300] Loss: 2.4940\n",
            "[Step 8310] Loss: 2.7676\n",
            "[Step 8320] Loss: 2.3052\n",
            "[Step 8330] Loss: 2.4120\n",
            "[Step 8340] Loss: 2.2210\n",
            "[Step 8350] Loss: 2.6565\n",
            "[Step 8360] Loss: 2.5177\n",
            "[Step 8370] Loss: 2.0576\n",
            "[Step 8380] Loss: 2.7300\n",
            "[Step 8390] Loss: 2.8888\n",
            "[Step 8400] Loss: 2.1958\n",
            "[Step 8410] Loss: 2.6313\n",
            "[Step 8420] Loss: 2.6206\n",
            "[Step 8430] Loss: 2.4344\n",
            "[Step 8440] Loss: 2.8208\n",
            "[Step 8450] Loss: 2.5721\n",
            "[Step 8460] Loss: 2.6525\n",
            "[Step 8470] Loss: 2.8913\n",
            "[Step 8480] Loss: 2.5305\n",
            "[Step 8490] Loss: 2.3397\n",
            "[Step 8500] Loss: 2.8338\n",
            "[Step 8510] Loss: 2.6361\n",
            "[Step 8520] Loss: 2.2854\n",
            "[Step 8530] Loss: 3.0703\n",
            "[Step 8540] Loss: 2.4716\n",
            "[Step 8550] Loss: 2.6935\n",
            "[Step 8560] Loss: 2.2133\n",
            "[Step 8570] Loss: 2.0639\n",
            "[Step 8580] Loss: 2.6208\n",
            "[Step 8590] Loss: 2.7802\n",
            "[Step 8600] Loss: 2.3762\n",
            "[Step 8610] Loss: 2.8840\n",
            "[Step 8620] Loss: 3.0280\n",
            "[Step 8630] Loss: 2.7805\n",
            "[Step 8640] Loss: 2.5482\n",
            "[Step 8650] Loss: 3.3101\n",
            "[Step 8660] Loss: 2.2950\n",
            "[Step 8670] Loss: 2.8902\n",
            "[Step 8680] Loss: 2.4926\n",
            "[Step 8690] Loss: 2.3107\n",
            "[Step 8700] Loss: 2.5923\n",
            "[Step 8710] Loss: 2.5231\n",
            "[Step 8720] Loss: 3.1055\n",
            "[Step 8730] Loss: 2.1522\n",
            "[Step 8740] Loss: 2.5558\n",
            "[Step 8750] Loss: 2.4796\n",
            "[Step 8760] Loss: 2.3760\n",
            "[Step 8770] Loss: 2.3838\n",
            "[Step 8780] Loss: 2.7115\n",
            "[Step 8790] Loss: 2.4729\n",
            "[Step 8800] Loss: 2.2356\n",
            "[Step 8810] Loss: 2.5481\n",
            "[Step 8820] Loss: 2.5825\n",
            "[Step 8830] Loss: 2.7318\n",
            "[Step 8840] Loss: 2.3925\n",
            "[Step 8850] Loss: 2.3176\n",
            "[Step 8860] Loss: 2.3955\n",
            "[Step 8870] Loss: 2.4695\n",
            "[Step 8880] Loss: 2.3813\n",
            "[Step 8890] Loss: 2.3769\n",
            "[Step 8900] Loss: 2.7209\n",
            "[Step 8910] Loss: 2.5035\n",
            "[Step 8920] Loss: 2.4367\n",
            "[Step 8930] Loss: 2.3083\n",
            "[Step 8940] Loss: 2.8718\n",
            "[Step 8950] Loss: 2.4011\n",
            "[Step 8960] Loss: 2.6727\n",
            "[Step 8970] Loss: 2.3710\n",
            "[Step 8980] Loss: 2.8559\n",
            "[Step 8990] Loss: 2.4879\n",
            "[Step 9000] Loss: 2.8062\n",
            "[Step 9010] Loss: 2.8462\n",
            "[Step 9020] Loss: 3.0543\n",
            "[Step 9030] Loss: 2.2923\n",
            "[Step 9040] Loss: 2.3264\n",
            "[Step 9050] Loss: 2.8710\n",
            "[Step 9060] Loss: 2.3154\n",
            "[Step 9070] Loss: 2.4381\n",
            "[Step 9080] Loss: 2.5966\n",
            "[Step 9090] Loss: 2.7198\n",
            "[Step 9100] Loss: 2.5262\n",
            "[Step 9110] Loss: 2.5069\n",
            "[Step 9120] Loss: 2.8968\n",
            "[Step 9130] Loss: 2.5071\n",
            "[Step 9140] Loss: 2.5818\n",
            "[Step 9150] Loss: 2.2855\n",
            "[Step 9160] Loss: 2.3874\n",
            "[Step 9170] Loss: 2.5874\n",
            "[Step 9180] Loss: 2.9671\n",
            "[Step 9190] Loss: 2.8540\n",
            "[Step 9200] Loss: 2.7538\n",
            "[Step 9210] Loss: 2.6935\n",
            "[Step 9220] Loss: 2.2201\n",
            "[Step 9230] Loss: 2.3533\n",
            "[Step 9240] Loss: 2.4584\n",
            "[Step 9250] Loss: 2.4975\n",
            "[Step 9260] Loss: 2.5306\n",
            "[Step 9270] Loss: 2.3904\n",
            "[Step 9280] Loss: 2.2247\n",
            "[Step 9290] Loss: 2.7512\n",
            "[Step 9300] Loss: 2.0418\n",
            "[Step 9310] Loss: 2.3852\n",
            "[Step 9320] Loss: 2.7791\n",
            "[Step 9330] Loss: 2.7515\n",
            "[Step 9340] Loss: 2.2696\n",
            "[Step 9350] Loss: 2.4418\n",
            "[Step 9360] Loss: 2.6417\n",
            "[Step 9370] Loss: 3.0959\n",
            "[Step 9380] Loss: 2.3868\n",
            "[Step 9390] Loss: 2.5901\n",
            "[Step 9400] Loss: 2.6384\n",
            "[Step 9410] Loss: 2.2727\n",
            "[Step 9420] Loss: 2.4400\n",
            "[Step 9430] Loss: 2.5689\n",
            "[Step 9440] Loss: 2.6532\n",
            "[Step 9450] Loss: 2.8232\n",
            "[Step 9460] Loss: 2.7582\n",
            "[Step 9470] Loss: 2.8472\n",
            "ðŸ“˜ Epoch 45 - Avg Training Loss: 2.5658\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6334 | Accuracy: 0.3496 | Precision: 0.3287\n",
            "[Step 9480] Loss: 1.8452\n",
            "[Step 9490] Loss: 2.5120\n",
            "[Step 9500] Loss: 2.6316\n",
            "[Step 9510] Loss: 2.2724\n",
            "[Step 9520] Loss: 2.8584\n",
            "[Step 9530] Loss: 2.1683\n",
            "[Step 9540] Loss: 2.8875\n",
            "[Step 9550] Loss: 2.6731\n",
            "[Step 9560] Loss: 2.4342\n",
            "[Step 9570] Loss: 2.9936\n",
            "[Step 9580] Loss: 2.2127\n",
            "[Step 9590] Loss: 2.1242\n",
            "[Step 9600] Loss: 2.3477\n",
            "[Step 9610] Loss: 2.6379\n",
            "[Step 9620] Loss: 2.8429\n",
            "[Step 9630] Loss: 1.9793\n",
            "[Step 9640] Loss: 2.9162\n",
            "[Step 9650] Loss: 2.4900\n",
            "[Step 9660] Loss: 2.6727\n",
            "[Step 9670] Loss: 2.9855\n",
            "[Step 9680] Loss: 2.5818\n",
            "[Step 9690] Loss: 2.4529\n",
            "[Step 9700] Loss: 2.5775\n",
            "[Step 9710] Loss: 2.5251\n",
            "[Step 9720] Loss: 2.4242\n",
            "[Step 9730] Loss: 2.0457\n",
            "[Step 9740] Loss: 2.1632\n",
            "[Step 9750] Loss: 2.5709\n",
            "[Step 9760] Loss: 2.4866\n",
            "[Step 9770] Loss: 3.1389\n",
            "[Step 9780] Loss: 2.5410\n",
            "[Step 9790] Loss: 2.6547\n",
            "[Step 9800] Loss: 2.3555\n",
            "[Step 9810] Loss: 2.5865\n",
            "[Step 9820] Loss: 2.5921\n",
            "[Step 9830] Loss: 2.2342\n",
            "[Step 9840] Loss: 2.4139\n",
            "[Step 9850] Loss: 2.1978\n",
            "[Step 9860] Loss: 3.0140\n",
            "[Step 9870] Loss: 2.4971\n",
            "[Step 9880] Loss: 2.5895\n",
            "[Step 9890] Loss: 2.7343\n",
            "[Step 9900] Loss: 2.5110\n",
            "[Step 9910] Loss: 2.3244\n",
            "[Step 9920] Loss: 2.8749\n",
            "[Step 9930] Loss: 2.5413\n",
            "[Step 9940] Loss: 3.1147\n",
            "[Step 9950] Loss: 2.8017\n",
            "[Step 9960] Loss: 3.1189\n",
            "[Step 9970] Loss: 2.8084\n",
            "[Step 9980] Loss: 2.3109\n",
            "[Step 9990] Loss: 3.0744\n",
            "[Step 10000] Loss: 2.6613\n",
            "[Step 10010] Loss: 2.5509\n",
            "[Step 10020] Loss: 2.8048\n",
            "[Step 10030] Loss: 2.5508\n",
            "[Step 10040] Loss: 2.5019\n",
            "[Step 10050] Loss: 2.5553\n",
            "[Step 10060] Loss: 2.7947\n",
            "[Step 10070] Loss: 2.8021\n",
            "[Step 10080] Loss: 2.7158\n",
            "[Step 10090] Loss: 2.4683\n",
            "[Step 10100] Loss: 2.6801\n",
            "[Step 10110] Loss: 2.6574\n",
            "[Step 10120] Loss: 2.5823\n",
            "[Step 10130] Loss: 2.7099\n",
            "[Step 10140] Loss: 2.4020\n",
            "[Step 10150] Loss: 3.1546\n",
            "[Step 10160] Loss: 3.0250\n",
            "[Step 10170] Loss: 2.7595\n",
            "[Step 10180] Loss: 2.4518\n",
            "[Step 10190] Loss: 2.9171\n",
            "[Step 10200] Loss: 2.6919\n",
            "[Step 10210] Loss: 2.7600\n",
            "[Step 10220] Loss: 2.9104\n",
            "[Step 10230] Loss: 2.4654\n",
            "[Step 10240] Loss: 2.4597\n",
            "[Step 10250] Loss: 2.3201\n",
            "[Step 10260] Loss: 2.7602\n",
            "[Step 10270] Loss: 2.4260\n",
            "[Step 10280] Loss: 2.4746\n",
            "[Step 10290] Loss: 2.7352\n",
            "[Step 10300] Loss: 2.1224\n",
            "[Step 10310] Loss: 2.6488\n",
            "[Step 10320] Loss: 2.6138\n",
            "[Step 10330] Loss: 2.1908\n",
            "[Step 10340] Loss: 2.5287\n",
            "[Step 10350] Loss: 2.5237\n",
            "[Step 10360] Loss: 2.5562\n",
            "[Step 10370] Loss: 2.7630\n",
            "[Step 10380] Loss: 2.6081\n",
            "[Step 10390] Loss: 2.3566\n",
            "[Step 10400] Loss: 2.3364\n",
            "[Step 10410] Loss: 2.2601\n",
            "[Step 10420] Loss: 2.9579\n",
            "[Step 10430] Loss: 2.4973\n",
            "[Step 10440] Loss: 2.7589\n",
            "[Step 10450] Loss: 2.6036\n",
            "[Step 10460] Loss: 2.6710\n",
            "[Step 10470] Loss: 2.3763\n",
            "[Step 10480] Loss: 2.5512\n",
            "[Step 10490] Loss: 2.2543\n",
            "[Step 10500] Loss: 2.5295\n",
            "[Step 10510] Loss: 2.5539\n",
            "[Step 10520] Loss: 3.2508\n",
            "[Step 10530] Loss: 2.1196\n",
            "[Step 10540] Loss: 2.4131\n",
            "[Step 10550] Loss: 2.2520\n",
            "[Step 10560] Loss: 2.8501\n",
            "[Step 10570] Loss: 2.5194\n",
            "[Step 10580] Loss: 2.2774\n",
            "[Step 10590] Loss: 2.1869\n",
            "[Step 10600] Loss: 2.4390\n",
            "[Step 10610] Loss: 2.4217\n",
            "[Step 10620] Loss: 2.2837\n",
            "[Step 10630] Loss: 2.5314\n",
            "[Step 10640] Loss: 2.7785\n",
            "[Step 10650] Loss: 2.5561\n",
            "[Step 10660] Loss: 3.0679\n",
            "[Step 10670] Loss: 2.4079\n",
            "[Step 10680] Loss: 2.8371\n",
            "[Step 10690] Loss: 2.6164\n",
            "[Step 10700] Loss: 2.5685\n",
            "[Step 10710] Loss: 2.7738\n",
            "[Step 10720] Loss: 2.4868\n",
            "[Step 10730] Loss: 2.5501\n",
            "[Step 10740] Loss: 2.9511\n",
            "[Step 10750] Loss: 2.7256\n",
            "[Step 10760] Loss: 2.6975\n",
            "[Step 10770] Loss: 3.0874\n",
            "[Step 10780] Loss: 2.2321\n",
            "[Step 10790] Loss: 2.4689\n",
            "[Step 10800] Loss: 2.0898\n",
            "[Step 10810] Loss: 3.2325\n",
            "[Step 10820] Loss: 2.6853\n",
            "[Step 10830] Loss: 2.5692\n",
            "[Step 10840] Loss: 2.4569\n",
            "[Step 10850] Loss: 2.9714\n",
            "[Step 10860] Loss: 2.2563\n",
            "[Step 10870] Loss: 2.7550\n",
            "[Step 10880] Loss: 3.1259\n",
            "[Step 10890] Loss: 2.5220\n",
            "[Step 10900] Loss: 2.6334\n",
            "[Step 10910] Loss: 2.3930\n",
            "[Step 10920] Loss: 2.5445\n",
            "[Step 10930] Loss: 2.0913\n",
            "[Step 10940] Loss: 2.4169\n",
            "[Step 10950] Loss: 2.7627\n",
            "[Step 10960] Loss: 2.6834\n",
            "[Step 10970] Loss: 2.9967\n",
            "[Step 10980] Loss: 2.3671\n",
            "[Step 10990] Loss: 2.9514\n",
            "[Step 11000] Loss: 2.5871\n",
            "[Step 11010] Loss: 2.2514\n",
            "[Step 11020] Loss: 2.8938\n",
            "[Step 11030] Loss: 2.3042\n",
            "[Step 11040] Loss: 2.3187\n",
            "[Step 11050] Loss: 2.7163\n",
            "[Step 11060] Loss: 2.7471\n",
            "[Step 11070] Loss: 2.7614\n",
            "[Step 11080] Loss: 2.5521\n",
            "[Step 11090] Loss: 2.5705\n",
            "[Step 11100] Loss: 2.6671\n",
            "[Step 11110] Loss: 3.1594\n",
            "[Step 11120] Loss: 2.7277\n",
            "[Step 11130] Loss: 2.4027\n",
            "[Step 11140] Loss: 3.0207\n",
            "[Step 11150] Loss: 2.1635\n",
            "[Step 11160] Loss: 2.1276\n",
            "[Step 11170] Loss: 2.4909\n",
            "[Step 11180] Loss: 2.6153\n",
            "[Step 11190] Loss: 2.6056\n",
            "[Step 11200] Loss: 2.8973\n",
            "[Step 11210] Loss: 2.7652\n",
            "[Step 11220] Loss: 2.9189\n",
            "[Step 11230] Loss: 2.7754\n",
            "[Step 11240] Loss: 3.1938\n",
            "[Step 11250] Loss: 2.4779\n",
            "[Step 11260] Loss: 2.5337\n",
            "[Step 11270] Loss: 2.8024\n",
            "[Step 11280] Loss: 2.2815\n",
            "[Step 11290] Loss: 2.9106\n",
            "[Step 11300] Loss: 2.6542\n",
            "[Step 11310] Loss: 2.6075\n",
            "[Step 11320] Loss: 2.8077\n",
            "[Step 11330] Loss: 2.7437\n",
            "[Step 11340] Loss: 2.7846\n",
            "[Step 11350] Loss: 2.0469\n",
            "[Step 11360] Loss: 2.6252\n",
            "ðŸ“˜ Epoch 46 - Avg Training Loss: 2.5543\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6209 | Accuracy: 0.3509 | Precision: 0.3289\n",
            "[Step 11370] Loss: 2.1341\n",
            "[Step 11380] Loss: 2.2691\n",
            "[Step 11390] Loss: 2.5923\n",
            "[Step 11400] Loss: 2.3944\n",
            "[Step 11410] Loss: 2.5489\n",
            "[Step 11420] Loss: 2.2174\n",
            "[Step 11430] Loss: 2.2488\n",
            "[Step 11440] Loss: 2.4149\n",
            "[Step 11450] Loss: 2.5464\n",
            "[Step 11460] Loss: 2.6563\n",
            "[Step 11470] Loss: 2.6967\n",
            "[Step 11480] Loss: 2.3337\n",
            "[Step 11490] Loss: 2.6263\n",
            "[Step 11500] Loss: 2.4506\n",
            "[Step 11510] Loss: 2.6199\n",
            "[Step 11520] Loss: 2.6244\n",
            "[Step 11530] Loss: 2.6081\n",
            "[Step 11540] Loss: 2.5555\n",
            "[Step 11550] Loss: 2.7348\n",
            "[Step 11560] Loss: 2.2395\n",
            "[Step 11570] Loss: 2.8073\n",
            "[Step 11580] Loss: 2.4426\n",
            "[Step 11590] Loss: 2.6232\n",
            "[Step 11600] Loss: 2.1564\n",
            "[Step 11610] Loss: 3.0010\n",
            "[Step 11620] Loss: 2.6301\n",
            "[Step 11630] Loss: 2.3994\n",
            "[Step 11640] Loss: 2.8866\n",
            "[Step 11650] Loss: 2.4726\n",
            "[Step 11660] Loss: 2.3529\n",
            "[Step 11670] Loss: 2.4141\n",
            "[Step 11680] Loss: 2.8186\n",
            "[Step 11690] Loss: 2.6736\n",
            "[Step 11700] Loss: 2.1849\n",
            "[Step 11710] Loss: 2.5228\n",
            "[Step 11720] Loss: 2.4457\n",
            "[Step 11730] Loss: 2.6324\n",
            "[Step 11740] Loss: 2.8924\n",
            "[Step 11750] Loss: 2.7122\n",
            "[Step 11760] Loss: 2.7467\n",
            "[Step 11770] Loss: 3.0281\n",
            "[Step 11780] Loss: 2.6429\n",
            "[Step 11790] Loss: 2.3235\n",
            "[Step 11800] Loss: 2.7850\n",
            "[Step 11810] Loss: 2.2441\n",
            "[Step 11820] Loss: 2.5836\n",
            "[Step 11830] Loss: 2.2129\n",
            "[Step 11840] Loss: 2.1649\n",
            "[Step 11850] Loss: 2.5711\n",
            "[Step 11860] Loss: 2.3210\n",
            "[Step 11870] Loss: 2.5764\n",
            "[Step 11880] Loss: 2.1607\n",
            "[Step 11890] Loss: 2.6262\n",
            "[Step 11900] Loss: 2.4028\n",
            "[Step 11910] Loss: 2.9340\n",
            "[Step 11920] Loss: 2.9292\n",
            "[Step 11930] Loss: 2.3969\n",
            "[Step 11940] Loss: 2.6138\n",
            "[Step 11950] Loss: 2.7211\n",
            "[Step 11960] Loss: 2.4866\n",
            "[Step 11970] Loss: 2.5260\n",
            "[Step 11980] Loss: 2.8691\n",
            "[Step 11990] Loss: 2.4174\n",
            "[Step 12000] Loss: 2.5578\n",
            "[Step 12010] Loss: 2.1000\n",
            "[Step 12020] Loss: 2.1868\n",
            "[Step 12030] Loss: 2.3542\n",
            "[Step 12040] Loss: 2.5424\n",
            "[Step 12050] Loss: 2.4879\n",
            "[Step 12060] Loss: 2.8098\n",
            "[Step 12070] Loss: 2.5798\n",
            "[Step 12080] Loss: 2.5394\n",
            "[Step 12090] Loss: 2.2748\n",
            "[Step 12100] Loss: 2.5744\n",
            "[Step 12110] Loss: 2.4296\n",
            "[Step 12120] Loss: 2.3977\n",
            "[Step 12130] Loss: 2.4782\n",
            "[Step 12140] Loss: 2.6033\n",
            "[Step 12150] Loss: 2.4107\n",
            "[Step 12160] Loss: 2.4398\n",
            "[Step 12170] Loss: 2.6279\n",
            "[Step 12180] Loss: 2.9549\n",
            "[Step 12190] Loss: 2.3190\n",
            "[Step 12200] Loss: 2.3320\n",
            "[Step 12210] Loss: 2.4534\n",
            "[Step 12220] Loss: 2.6352\n",
            "[Step 12230] Loss: 2.2498\n",
            "[Step 12240] Loss: 2.1424\n",
            "[Step 12250] Loss: 2.6338\n",
            "[Step 12260] Loss: 2.8366\n",
            "[Step 12270] Loss: 2.8441\n",
            "[Step 12280] Loss: 2.2953\n",
            "[Step 12290] Loss: 2.4925\n",
            "[Step 12300] Loss: 2.2754\n",
            "[Step 12310] Loss: 2.8307\n",
            "[Step 12320] Loss: 2.3631\n",
            "[Step 12330] Loss: 2.7833\n",
            "[Step 12340] Loss: 2.2391\n",
            "[Step 12350] Loss: 2.5346\n",
            "[Step 12360] Loss: 2.4109\n",
            "[Step 12370] Loss: 2.4382\n",
            "[Step 12380] Loss: 2.0191\n",
            "[Step 12390] Loss: 2.5344\n",
            "[Step 12400] Loss: 2.4658\n",
            "[Step 12410] Loss: 2.7129\n",
            "[Step 12420] Loss: 2.6183\n",
            "[Step 12430] Loss: 2.6332\n",
            "[Step 12440] Loss: 2.2206\n",
            "[Step 12450] Loss: 2.4118\n",
            "[Step 12460] Loss: 2.3319\n",
            "[Step 12470] Loss: 2.3416\n",
            "[Step 12480] Loss: 2.2993\n",
            "[Step 12490] Loss: 2.0317\n",
            "[Step 12500] Loss: 2.6994\n",
            "[Step 12510] Loss: 2.4889\n",
            "[Step 12520] Loss: 2.3136\n",
            "[Step 12530] Loss: 2.5653\n",
            "[Step 12540] Loss: 2.9393\n",
            "[Step 12550] Loss: 2.4754\n",
            "[Step 12560] Loss: 2.5398\n",
            "[Step 12570] Loss: 2.5838\n",
            "[Step 12580] Loss: 2.3301\n",
            "[Step 12590] Loss: 2.5565\n",
            "[Step 12600] Loss: 2.8435\n",
            "[Step 12610] Loss: 2.3968\n",
            "[Step 12620] Loss: 2.6703\n",
            "[Step 12630] Loss: 2.4554\n",
            "[Step 12640] Loss: 2.2358\n",
            "[Step 12650] Loss: 2.6655\n",
            "[Step 12660] Loss: 3.0493\n",
            "[Step 12670] Loss: 2.1526\n",
            "[Step 12680] Loss: 2.4612\n",
            "[Step 12690] Loss: 2.2293\n",
            "[Step 12700] Loss: 2.6067\n",
            "[Step 12710] Loss: 2.4143\n",
            "[Step 12720] Loss: 2.3543\n",
            "[Step 12730] Loss: 2.2448\n",
            "[Step 12740] Loss: 2.2193\n",
            "[Step 12750] Loss: 2.6490\n",
            "[Step 12760] Loss: 2.7840\n",
            "[Step 12770] Loss: 2.5613\n",
            "[Step 12780] Loss: 2.8695\n",
            "[Step 12790] Loss: 1.8632\n",
            "[Step 12800] Loss: 2.5980\n",
            "[Step 12810] Loss: 2.8753\n",
            "[Step 12820] Loss: 2.6771\n",
            "[Step 12830] Loss: 2.5633\n",
            "[Step 12840] Loss: 2.2766\n",
            "[Step 12850] Loss: 2.9606\n",
            "[Step 12860] Loss: 2.9257\n",
            "[Step 12870] Loss: 2.4366\n",
            "[Step 12880] Loss: 3.0063\n",
            "[Step 12890] Loss: 2.6783\n",
            "[Step 12900] Loss: 2.4973\n",
            "[Step 12910] Loss: 2.3942\n",
            "[Step 12920] Loss: 2.9481\n",
            "[Step 12930] Loss: 2.5884\n",
            "[Step 12940] Loss: 2.4226\n",
            "[Step 12950] Loss: 2.2814\n",
            "[Step 12960] Loss: 2.8922\n",
            "[Step 12970] Loss: 2.6005\n",
            "[Step 12980] Loss: 2.5105\n",
            "[Step 12990] Loss: 2.4797\n",
            "[Step 13000] Loss: 2.8359\n",
            "[Step 13010] Loss: 3.0749\n",
            "[Step 13020] Loss: 2.2019\n",
            "[Step 13030] Loss: 2.0504\n",
            "[Step 13040] Loss: 2.5855\n",
            "[Step 13050] Loss: 2.2073\n",
            "[Step 13060] Loss: 2.2852\n",
            "[Step 13070] Loss: 2.4176\n",
            "[Step 13080] Loss: 2.8967\n",
            "[Step 13090] Loss: 3.1193\n",
            "[Step 13100] Loss: 1.9800\n",
            "[Step 13110] Loss: 2.2446\n",
            "[Step 13120] Loss: 2.2646\n",
            "[Step 13130] Loss: 2.4031\n",
            "[Step 13140] Loss: 2.3681\n",
            "[Step 13150] Loss: 2.8406\n",
            "[Step 13160] Loss: 2.6088\n",
            "[Step 13170] Loss: 2.8779\n",
            "[Step 13180] Loss: 2.4543\n",
            "[Step 13190] Loss: 2.6467\n",
            "[Step 13200] Loss: 2.4376\n",
            "[Step 13210] Loss: 2.3679\n",
            "[Step 13220] Loss: 2.4516\n",
            "[Step 13230] Loss: 2.7093\n",
            "[Step 13240] Loss: 3.0166\n",
            "[Step 13250] Loss: 2.5592\n",
            "ðŸ“˜ Epoch 47 - Avg Training Loss: 2.5453\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6184 | Accuracy: 0.3508 | Precision: 0.3292\n",
            "[Step 13260] Loss: 2.6103\n",
            "[Step 13270] Loss: 2.7571\n",
            "[Step 13280] Loss: 2.5150\n",
            "[Step 13290] Loss: 2.4789\n",
            "[Step 13300] Loss: 2.4879\n",
            "[Step 13310] Loss: 2.5817\n",
            "[Step 13320] Loss: 2.1887\n",
            "[Step 13330] Loss: 2.6849\n",
            "[Step 13340] Loss: 2.7763\n",
            "[Step 13350] Loss: 2.3609\n",
            "[Step 13360] Loss: 2.3903\n",
            "[Step 13370] Loss: 2.3383\n",
            "[Step 13380] Loss: 2.3526\n",
            "[Step 13390] Loss: 2.6960\n",
            "[Step 13400] Loss: 2.4403\n",
            "[Step 13410] Loss: 1.6923\n",
            "[Step 13420] Loss: 2.6917\n",
            "[Step 13430] Loss: 3.1907\n",
            "[Step 13440] Loss: 2.3227\n",
            "[Step 13450] Loss: 2.8417\n",
            "[Step 13460] Loss: 2.3448\n",
            "[Step 13470] Loss: 2.6842\n",
            "[Step 13480] Loss: 2.9628\n",
            "[Step 13490] Loss: 2.7115\n",
            "[Step 13500] Loss: 2.1767\n",
            "[Step 13510] Loss: 2.4688\n",
            "[Step 13520] Loss: 3.1890\n",
            "[Step 13530] Loss: 2.5032\n",
            "[Step 13540] Loss: 2.3183\n",
            "[Step 13550] Loss: 2.1522\n",
            "[Step 13560] Loss: 2.6481\n",
            "[Step 13570] Loss: 2.6373\n",
            "[Step 13580] Loss: 2.5075\n",
            "[Step 13590] Loss: 2.8504\n",
            "[Step 13600] Loss: 2.8610\n",
            "[Step 13610] Loss: 2.6696\n",
            "[Step 13620] Loss: 2.4285\n",
            "[Step 13630] Loss: 2.8064\n",
            "[Step 13640] Loss: 2.5228\n",
            "[Step 13650] Loss: 3.0999\n",
            "[Step 13660] Loss: 2.6933\n",
            "[Step 13670] Loss: 2.7539\n",
            "[Step 13680] Loss: 2.4687\n",
            "[Step 13690] Loss: 2.2724\n",
            "[Step 13700] Loss: 2.2063\n",
            "[Step 13710] Loss: 3.0923\n",
            "[Step 13720] Loss: 2.4776\n",
            "[Step 13730] Loss: 2.5268\n",
            "[Step 13740] Loss: 2.4534\n",
            "[Step 13750] Loss: 2.7888\n",
            "[Step 13760] Loss: 2.3493\n",
            "[Step 13770] Loss: 2.4163\n",
            "[Step 13780] Loss: 2.5684\n",
            "[Step 13790] Loss: 2.9052\n",
            "[Step 13800] Loss: 2.3949\n",
            "[Step 13810] Loss: 2.7706\n",
            "[Step 13820] Loss: 2.1256\n",
            "[Step 13830] Loss: 2.3439\n",
            "[Step 13840] Loss: 2.3889\n",
            "[Step 13850] Loss: 2.6888\n",
            "[Step 13860] Loss: 2.8446\n",
            "[Step 13870] Loss: 2.2496\n",
            "[Step 13880] Loss: 2.9771\n",
            "[Step 13890] Loss: 2.6635\n",
            "[Step 13900] Loss: 2.6960\n",
            "[Step 13910] Loss: 2.2780\n",
            "[Step 13920] Loss: 2.6260\n",
            "[Step 13930] Loss: 2.8529\n",
            "[Step 13940] Loss: 2.3817\n",
            "[Step 13950] Loss: 2.5894\n",
            "[Step 13960] Loss: 2.5418\n",
            "[Step 13970] Loss: 2.5964\n",
            "[Step 13980] Loss: 2.9841\n",
            "[Step 13990] Loss: 2.4765\n",
            "[Step 14000] Loss: 2.7938\n",
            "[Step 14010] Loss: 2.5900\n",
            "[Step 14020] Loss: 2.6249\n",
            "[Step 14030] Loss: 2.2750\n",
            "[Step 14040] Loss: 2.8411\n",
            "[Step 14050] Loss: 2.1368\n",
            "[Step 14060] Loss: 2.4500\n",
            "[Step 14070] Loss: 2.3311\n",
            "[Step 14080] Loss: 2.7460\n",
            "[Step 14090] Loss: 2.5910\n",
            "[Step 14100] Loss: 2.7452\n",
            "[Step 14110] Loss: 2.6042\n",
            "[Step 14120] Loss: 2.6411\n",
            "[Step 14130] Loss: 2.1167\n",
            "[Step 14140] Loss: 2.5169\n",
            "[Step 14150] Loss: 2.2955\n",
            "[Step 14160] Loss: 2.5592\n",
            "[Step 14170] Loss: 2.5794\n",
            "[Step 14180] Loss: 1.9489\n",
            "[Step 14190] Loss: 2.1073\n",
            "[Step 14200] Loss: 2.7897\n",
            "[Step 14210] Loss: 1.9925\n",
            "[Step 14220] Loss: 2.4454\n",
            "[Step 14230] Loss: 2.3492\n",
            "[Step 14240] Loss: 2.4871\n",
            "[Step 14250] Loss: 2.7922\n",
            "[Step 14260] Loss: 2.6676\n",
            "[Step 14270] Loss: 2.2468\n",
            "[Step 14280] Loss: 2.9166\n",
            "[Step 14290] Loss: 2.9962\n",
            "[Step 14300] Loss: 2.7873\n",
            "[Step 14310] Loss: 2.4132\n",
            "[Step 14320] Loss: 2.7596\n",
            "[Step 14330] Loss: 2.3810\n",
            "[Step 14340] Loss: 2.5977\n",
            "[Step 14350] Loss: 2.0801\n",
            "[Step 14360] Loss: 2.1203\n",
            "[Step 14370] Loss: 2.2896\n",
            "[Step 14380] Loss: 2.5464\n",
            "[Step 14390] Loss: 2.6421\n",
            "[Step 14400] Loss: 2.8427\n",
            "[Step 14410] Loss: 2.5881\n",
            "[Step 14420] Loss: 2.5887\n",
            "[Step 14430] Loss: 2.1368\n",
            "[Step 14440] Loss: 2.7026\n",
            "[Step 14450] Loss: 2.4719\n",
            "[Step 14460] Loss: 2.5844\n",
            "[Step 14470] Loss: 2.5684\n",
            "[Step 14480] Loss: 2.3687\n",
            "[Step 14490] Loss: 2.2278\n",
            "[Step 14500] Loss: 2.3252\n",
            "[Step 14510] Loss: 2.5801\n",
            "[Step 14520] Loss: 3.0305\n",
            "[Step 14530] Loss: 2.3196\n",
            "[Step 14540] Loss: 2.6725\n",
            "[Step 14550] Loss: 2.3765\n",
            "[Step 14560] Loss: 2.3524\n",
            "[Step 14570] Loss: 2.3325\n",
            "[Step 14580] Loss: 2.3152\n",
            "[Step 14590] Loss: 3.1042\n",
            "[Step 14600] Loss: 1.8693\n",
            "[Step 14610] Loss: 2.3078\n",
            "[Step 14620] Loss: 2.7050\n",
            "[Step 14630] Loss: 2.6970\n",
            "[Step 14640] Loss: 2.5259\n",
            "[Step 14650] Loss: 2.3206\n",
            "[Step 14660] Loss: 2.5968\n",
            "[Step 14670] Loss: 2.3284\n",
            "[Step 14680] Loss: 2.3353\n",
            "[Step 14690] Loss: 2.7790\n",
            "[Step 14700] Loss: 2.3202\n",
            "[Step 14710] Loss: 2.2866\n",
            "[Step 14720] Loss: 2.4442\n",
            "[Step 14730] Loss: 2.5862\n",
            "[Step 14740] Loss: 2.4599\n",
            "[Step 14750] Loss: 2.3352\n",
            "[Step 14760] Loss: 2.8210\n",
            "[Step 14770] Loss: 2.6779\n",
            "[Step 14780] Loss: 2.5721\n",
            "[Step 14790] Loss: 2.4462\n",
            "[Step 14800] Loss: 2.6066\n",
            "[Step 14810] Loss: 2.1042\n",
            "[Step 14820] Loss: 2.7128\n",
            "[Step 14830] Loss: 3.1514\n",
            "[Step 14840] Loss: 2.2064\n",
            "[Step 14850] Loss: 2.7189\n",
            "[Step 14860] Loss: 2.7597\n",
            "[Step 14870] Loss: 3.0845\n",
            "[Step 14880] Loss: 2.4906\n",
            "[Step 14890] Loss: 2.5076\n",
            "[Step 14900] Loss: 2.8641\n",
            "[Step 14910] Loss: 2.5141\n",
            "[Step 14920] Loss: 2.1694\n",
            "[Step 14930] Loss: 2.4398\n",
            "[Step 14940] Loss: 2.7167\n",
            "[Step 14950] Loss: 2.3858\n",
            "[Step 14960] Loss: 2.4121\n",
            "[Step 14970] Loss: 2.4271\n",
            "[Step 14980] Loss: 2.7019\n",
            "[Step 14990] Loss: 2.5732\n",
            "[Step 15000] Loss: 2.8894\n",
            "[Step 15010] Loss: 2.6304\n",
            "[Step 15020] Loss: 2.4698\n",
            "[Step 15030] Loss: 2.2043\n",
            "[Step 15040] Loss: 2.1698\n",
            "[Step 15050] Loss: 2.7154\n",
            "[Step 15060] Loss: 2.8282\n",
            "[Step 15070] Loss: 2.6576\n",
            "[Step 15080] Loss: 2.4458\n",
            "[Step 15090] Loss: 2.1542\n",
            "[Step 15100] Loss: 2.8222\n",
            "[Step 15110] Loss: 2.6875\n",
            "[Step 15120] Loss: 2.7244\n",
            "[Step 15130] Loss: 1.9956\n",
            "[Step 15140] Loss: 2.7162\n",
            "[Step 15150] Loss: 2.4479\n",
            "ðŸ“˜ Epoch 48 - Avg Training Loss: 2.5367\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6086 | Accuracy: 0.3560 | Precision: 0.3311\n",
            "[Step 15160] Loss: 2.5683\n",
            "[Step 15170] Loss: 2.4022\n",
            "[Step 15180] Loss: 2.4003\n",
            "[Step 15190] Loss: 2.3986\n",
            "[Step 15200] Loss: 2.2119\n",
            "[Step 15210] Loss: 2.6662\n",
            "[Step 15220] Loss: 2.9851\n",
            "[Step 15230] Loss: 2.5502\n",
            "[Step 15240] Loss: 2.6575\n",
            "[Step 15250] Loss: 2.4665\n",
            "[Step 15260] Loss: 2.8259\n",
            "[Step 15270] Loss: 2.5208\n",
            "[Step 15280] Loss: 3.0783\n",
            "[Step 15290] Loss: 2.2827\n",
            "[Step 15300] Loss: 1.8248\n",
            "[Step 15310] Loss: 2.5384\n",
            "[Step 15320] Loss: 2.4519\n",
            "[Step 15330] Loss: 2.4528\n",
            "[Step 15340] Loss: 2.4013\n",
            "[Step 15350] Loss: 2.6937\n",
            "[Step 15360] Loss: 2.7707\n",
            "[Step 15370] Loss: 2.9697\n",
            "[Step 15380] Loss: 2.2958\n",
            "[Step 15390] Loss: 3.0791\n",
            "[Step 15400] Loss: 2.5690\n",
            "[Step 15410] Loss: 2.8171\n",
            "[Step 15420] Loss: 2.7594\n",
            "[Step 15430] Loss: 2.3975\n",
            "[Step 15440] Loss: 2.5583\n",
            "[Step 15450] Loss: 2.3297\n",
            "[Step 15460] Loss: 2.8054\n",
            "[Step 15470] Loss: 2.6946\n",
            "[Step 15480] Loss: 2.7286\n",
            "[Step 15490] Loss: 2.5946\n",
            "[Step 15500] Loss: 2.4451\n",
            "[Step 15510] Loss: 2.5731\n",
            "[Step 15520] Loss: 2.4378\n",
            "[Step 15530] Loss: 3.0650\n",
            "[Step 15540] Loss: 2.5289\n",
            "[Step 15550] Loss: 3.1866\n",
            "[Step 15560] Loss: 2.4754\n",
            "[Step 15570] Loss: 2.6536\n",
            "[Step 15580] Loss: 2.7909\n",
            "[Step 15590] Loss: 2.5848\n",
            "[Step 15600] Loss: 2.6265\n",
            "[Step 15610] Loss: 2.7388\n",
            "[Step 15620] Loss: 2.4169\n",
            "[Step 15630] Loss: 2.6644\n",
            "[Step 15640] Loss: 2.8068\n",
            "[Step 15650] Loss: 2.1573\n",
            "[Step 15660] Loss: 2.6298\n",
            "[Step 15670] Loss: 2.0681\n",
            "[Step 15680] Loss: 2.5920\n",
            "[Step 15690] Loss: 2.2011\n",
            "[Step 15700] Loss: 2.5300\n",
            "[Step 15710] Loss: 2.5918\n",
            "[Step 15720] Loss: 2.7462\n",
            "[Step 15730] Loss: 2.0596\n",
            "[Step 15740] Loss: 2.6397\n",
            "[Step 15750] Loss: 2.7074\n",
            "[Step 15760] Loss: 2.4226\n",
            "[Step 15770] Loss: 1.9615\n",
            "[Step 15780] Loss: 2.3344\n",
            "[Step 15790] Loss: 2.7617\n",
            "[Step 15800] Loss: 2.2905\n",
            "[Step 15810] Loss: 2.6287\n",
            "[Step 15820] Loss: 2.8995\n",
            "[Step 15830] Loss: 2.5364\n",
            "[Step 15840] Loss: 2.2469\n",
            "[Step 15850] Loss: 2.7957\n",
            "[Step 15860] Loss: 2.6841\n",
            "[Step 15870] Loss: 2.4003\n",
            "[Step 15880] Loss: 2.8920\n",
            "[Step 15890] Loss: 2.3361\n",
            "[Step 15900] Loss: 2.4622\n",
            "[Step 15910] Loss: 2.6403\n",
            "[Step 15920] Loss: 2.4263\n",
            "[Step 15930] Loss: 2.3687\n",
            "[Step 15940] Loss: 2.8111\n",
            "[Step 15950] Loss: 2.4781\n",
            "[Step 15960] Loss: 2.8059\n",
            "[Step 15970] Loss: 2.3468\n",
            "[Step 15980] Loss: 2.1264\n",
            "[Step 15990] Loss: 2.4349\n",
            "[Step 16000] Loss: 2.6366\n",
            "[Step 16010] Loss: 2.5922\n",
            "[Step 16020] Loss: 2.9936\n",
            "[Step 16030] Loss: 2.7949\n",
            "[Step 16040] Loss: 2.1558\n",
            "[Step 16050] Loss: 2.4546\n",
            "[Step 16060] Loss: 1.8242\n",
            "[Step 16070] Loss: 3.0139\n",
            "[Step 16080] Loss: 2.8918\n",
            "[Step 16090] Loss: 2.3086\n",
            "[Step 16100] Loss: 2.6055\n",
            "[Step 16110] Loss: 2.7540\n",
            "[Step 16120] Loss: 2.5332\n",
            "[Step 16130] Loss: 2.1199\n",
            "[Step 16140] Loss: 2.5965\n",
            "[Step 16150] Loss: 2.3630\n",
            "[Step 16160] Loss: 2.2503\n",
            "[Step 16170] Loss: 2.1038\n",
            "[Step 16180] Loss: 2.3384\n",
            "[Step 16190] Loss: 2.6006\n",
            "[Step 16200] Loss: 2.4138\n",
            "[Step 16210] Loss: 2.7751\n",
            "[Step 16220] Loss: 2.2621\n",
            "[Step 16230] Loss: 2.7034\n",
            "[Step 16240] Loss: 2.9442\n",
            "[Step 16250] Loss: 2.2382\n",
            "[Step 16260] Loss: 2.6365\n",
            "[Step 16270] Loss: 2.9496\n",
            "[Step 16280] Loss: 2.5655\n",
            "[Step 16290] Loss: 2.9358\n",
            "[Step 16300] Loss: 2.3775\n",
            "[Step 16310] Loss: 2.1622\n",
            "[Step 16320] Loss: 2.4351\n",
            "[Step 16330] Loss: 2.2543\n",
            "[Step 16340] Loss: 2.2717\n",
            "[Step 16350] Loss: 2.5053\n",
            "[Step 16360] Loss: 2.6022\n",
            "[Step 16370] Loss: 2.6579\n",
            "[Step 16380] Loss: 2.8078\n",
            "[Step 16390] Loss: 2.5003\n",
            "[Step 16400] Loss: 2.2595\n",
            "[Step 16410] Loss: 2.7938\n",
            "[Step 16420] Loss: 2.5758\n",
            "[Step 16430] Loss: 2.0397\n",
            "[Step 16440] Loss: 2.3071\n",
            "[Step 16450] Loss: 2.6283\n",
            "[Step 16460] Loss: 2.3562\n",
            "[Step 16470] Loss: 2.5183\n",
            "[Step 16480] Loss: 3.3100\n",
            "[Step 16490] Loss: 2.6744\n",
            "[Step 16500] Loss: 2.8455\n",
            "[Step 16510] Loss: 2.3488\n",
            "[Step 16520] Loss: 2.7002\n",
            "[Step 16530] Loss: 3.1074\n",
            "[Step 16540] Loss: 2.5551\n",
            "[Step 16550] Loss: 2.4199\n",
            "[Step 16560] Loss: 2.8502\n",
            "[Step 16570] Loss: 2.5476\n",
            "[Step 16580] Loss: 2.6384\n",
            "[Step 16590] Loss: 2.3519\n",
            "[Step 16600] Loss: 2.3580\n",
            "[Step 16610] Loss: 1.9972\n",
            "[Step 16620] Loss: 2.7144\n",
            "[Step 16630] Loss: 2.6692\n",
            "[Step 16640] Loss: 2.8796\n",
            "[Step 16650] Loss: 2.6689\n",
            "[Step 16660] Loss: 2.7330\n",
            "[Step 16670] Loss: 2.5623\n",
            "[Step 16680] Loss: 2.0477\n",
            "[Step 16690] Loss: 2.1898\n",
            "[Step 16700] Loss: 2.2759\n",
            "[Step 16710] Loss: 2.6970\n",
            "[Step 16720] Loss: 2.5956\n",
            "[Step 16730] Loss: 2.5707\n",
            "[Step 16740] Loss: 2.5112\n",
            "[Step 16750] Loss: 2.3359\n",
            "[Step 16760] Loss: 2.2781\n",
            "[Step 16770] Loss: 2.4060\n",
            "[Step 16780] Loss: 2.7761\n",
            "[Step 16790] Loss: 2.7828\n",
            "[Step 16800] Loss: 2.6765\n",
            "[Step 16810] Loss: 2.1466\n",
            "[Step 16820] Loss: 2.9474\n",
            "[Step 16830] Loss: 2.1918\n",
            "[Step 16840] Loss: 2.4178\n",
            "[Step 16850] Loss: 2.6961\n",
            "[Step 16860] Loss: 2.4519\n",
            "[Step 16870] Loss: 2.8718\n",
            "[Step 16880] Loss: 2.3775\n",
            "[Step 16890] Loss: 2.2631\n",
            "[Step 16900] Loss: 2.6982\n",
            "[Step 16910] Loss: 2.6113\n",
            "[Step 16920] Loss: 2.6458\n",
            "[Step 16930] Loss: 2.3808\n",
            "[Step 16940] Loss: 2.5281\n",
            "[Step 16950] Loss: 2.6778\n",
            "[Step 16960] Loss: 2.8832\n",
            "[Step 16970] Loss: 2.4526\n",
            "[Step 16980] Loss: 2.5648\n",
            "[Step 16990] Loss: 2.8222\n",
            "[Step 17000] Loss: 2.2687\n",
            "[Step 17010] Loss: 2.7298\n",
            "[Step 17020] Loss: 2.6893\n",
            "[Step 17030] Loss: 2.0318\n",
            "[Step 17040] Loss: 2.2017\n",
            "ðŸ“˜ Epoch 49 - Avg Training Loss: 2.5321\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6085 | Accuracy: 0.3556 | Precision: 0.3340\n",
            "[Step 17050] Loss: 2.0858\n",
            "[Step 17060] Loss: 2.3296\n",
            "[Step 17070] Loss: 2.5370\n",
            "[Step 17080] Loss: 2.9970\n",
            "[Step 17090] Loss: 2.3320\n",
            "[Step 17100] Loss: 2.3812\n",
            "[Step 17110] Loss: 2.5042\n",
            "[Step 17120] Loss: 2.9651\n",
            "[Step 17130] Loss: 2.1191\n",
            "[Step 17140] Loss: 2.4905\n",
            "[Step 17150] Loss: 2.4872\n",
            "[Step 17160] Loss: 2.8602\n",
            "[Step 17170] Loss: 2.4084\n",
            "[Step 17180] Loss: 2.0037\n",
            "[Step 17190] Loss: 3.0839\n",
            "[Step 17200] Loss: 2.6147\n",
            "[Step 17210] Loss: 3.2599\n",
            "[Step 17220] Loss: 2.4866\n",
            "[Step 17230] Loss: 2.4162\n",
            "[Step 17240] Loss: 2.1964\n",
            "[Step 17250] Loss: 3.1191\n",
            "[Step 17260] Loss: 2.0873\n",
            "[Step 17270] Loss: 2.5822\n",
            "[Step 17280] Loss: 2.7694\n",
            "[Step 17290] Loss: 2.6995\n",
            "[Step 17300] Loss: 2.5732\n",
            "[Step 17310] Loss: 2.7845\n",
            "[Step 17320] Loss: 2.2664\n",
            "[Step 17330] Loss: 2.5205\n",
            "[Step 17340] Loss: 2.4120\n",
            "[Step 17350] Loss: 2.3594\n",
            "[Step 17360] Loss: 2.3617\n",
            "[Step 17370] Loss: 2.4886\n",
            "[Step 17380] Loss: 2.2221\n",
            "[Step 17390] Loss: 2.7738\n",
            "[Step 17400] Loss: 2.5814\n",
            "[Step 17410] Loss: 1.9213\n",
            "[Step 17420] Loss: 2.4774\n",
            "[Step 17430] Loss: 2.4380\n",
            "[Step 17440] Loss: 2.5716\n",
            "[Step 17450] Loss: 2.4161\n",
            "[Step 17460] Loss: 2.7632\n",
            "[Step 17470] Loss: 2.1500\n",
            "[Step 17480] Loss: 2.1059\n",
            "[Step 17490] Loss: 2.6705\n",
            "[Step 17500] Loss: 2.6263\n",
            "[Step 17510] Loss: 2.0341\n",
            "[Step 17520] Loss: 2.0424\n",
            "[Step 17530] Loss: 2.2076\n",
            "[Step 17540] Loss: 2.2859\n",
            "[Step 17550] Loss: 2.3875\n",
            "[Step 17560] Loss: 2.2840\n",
            "[Step 17570] Loss: 2.5639\n",
            "[Step 17580] Loss: 2.5992\n",
            "[Step 17590] Loss: 2.9474\n",
            "[Step 17600] Loss: 2.4208\n",
            "[Step 17610] Loss: 2.1543\n",
            "[Step 17620] Loss: 3.0594\n",
            "[Step 17630] Loss: 2.5770\n",
            "[Step 17640] Loss: 2.3157\n",
            "[Step 17650] Loss: 2.7524\n",
            "[Step 17660] Loss: 2.6857\n",
            "[Step 17670] Loss: 2.6692\n",
            "[Step 17680] Loss: 2.9561\n",
            "[Step 17690] Loss: 2.9521\n",
            "[Step 17700] Loss: 2.7590\n",
            "[Step 17710] Loss: 2.6285\n",
            "[Step 17720] Loss: 2.7090\n",
            "[Step 17730] Loss: 2.4507\n",
            "[Step 17740] Loss: 2.5001\n",
            "[Step 17750] Loss: 2.3124\n",
            "[Step 17760] Loss: 2.5954\n",
            "[Step 17770] Loss: 2.2918\n",
            "[Step 17780] Loss: 2.6975\n",
            "[Step 17790] Loss: 2.2655\n",
            "[Step 17800] Loss: 2.6549\n",
            "[Step 17810] Loss: 2.5394\n",
            "[Step 17820] Loss: 2.2775\n",
            "[Step 17830] Loss: 2.9558\n",
            "[Step 17840] Loss: 2.4810\n",
            "[Step 17850] Loss: 2.5105\n",
            "[Step 17860] Loss: 2.4799\n",
            "[Step 17870] Loss: 2.7271\n",
            "[Step 17880] Loss: 2.6669\n",
            "[Step 17890] Loss: 2.6092\n",
            "[Step 17900] Loss: 2.4432\n",
            "[Step 17910] Loss: 2.0807\n",
            "[Step 17920] Loss: 2.5917\n",
            "[Step 17930] Loss: 2.5992\n",
            "[Step 17940] Loss: 2.3635\n",
            "[Step 17950] Loss: 2.4406\n",
            "[Step 17960] Loss: 2.8461\n",
            "[Step 17970] Loss: 2.2963\n",
            "[Step 17980] Loss: 2.2088\n",
            "[Step 17990] Loss: 2.5683\n",
            "[Step 18000] Loss: 2.3937\n",
            "[Step 18010] Loss: 3.0592\n",
            "[Step 18020] Loss: 2.2518\n",
            "[Step 18030] Loss: 2.3307\n",
            "[Step 18040] Loss: 2.3761\n",
            "[Step 18050] Loss: 2.2744\n",
            "[Step 18060] Loss: 2.7024\n",
            "[Step 18070] Loss: 2.5831\n",
            "[Step 18080] Loss: 2.6041\n",
            "[Step 18090] Loss: 2.0276\n",
            "[Step 18100] Loss: 2.4736\n",
            "[Step 18110] Loss: 2.4444\n",
            "[Step 18120] Loss: 1.9888\n",
            "[Step 18130] Loss: 2.5989\n",
            "[Step 18140] Loss: 2.9743\n",
            "[Step 18150] Loss: 2.4724\n",
            "[Step 18160] Loss: 2.6884\n",
            "[Step 18170] Loss: 2.4032\n",
            "[Step 18180] Loss: 2.5387\n",
            "[Step 18190] Loss: 2.4265\n",
            "[Step 18200] Loss: 2.5826\n",
            "[Step 18210] Loss: 2.3463\n",
            "[Step 18220] Loss: 2.7089\n",
            "[Step 18230] Loss: 2.4022\n",
            "[Step 18240] Loss: 2.5977\n",
            "[Step 18250] Loss: 2.4348\n",
            "[Step 18260] Loss: 2.0945\n",
            "[Step 18270] Loss: 2.7678\n",
            "[Step 18280] Loss: 2.5564\n",
            "[Step 18290] Loss: 2.6877\n",
            "[Step 18300] Loss: 2.4351\n",
            "[Step 18310] Loss: 2.5920\n",
            "[Step 18320] Loss: 2.4935\n",
            "[Step 18330] Loss: 2.6595\n",
            "[Step 18340] Loss: 2.4520\n",
            "[Step 18350] Loss: 2.7502\n",
            "[Step 18360] Loss: 2.1881\n",
            "[Step 18370] Loss: 2.3748\n",
            "[Step 18380] Loss: 2.2348\n",
            "[Step 18390] Loss: 3.0808\n",
            "[Step 18400] Loss: 2.6489\n",
            "[Step 18410] Loss: 2.8203\n",
            "[Step 18420] Loss: 2.3570\n",
            "[Step 18430] Loss: 2.4875\n",
            "[Step 18440] Loss: 2.3788\n",
            "[Step 18450] Loss: 2.4216\n",
            "[Step 18460] Loss: 2.5293\n",
            "[Step 18470] Loss: 2.1377\n",
            "[Step 18480] Loss: 2.8077\n",
            "[Step 18490] Loss: 2.6339\n",
            "[Step 18500] Loss: 2.2645\n",
            "[Step 18510] Loss: 2.2350\n",
            "[Step 18520] Loss: 2.1396\n",
            "[Step 18530] Loss: 2.5532\n",
            "[Step 18540] Loss: 2.4664\n",
            "[Step 18550] Loss: 2.7366\n",
            "[Step 18560] Loss: 2.3399\n",
            "[Step 18570] Loss: 2.4974\n",
            "[Step 18580] Loss: 2.5906\n",
            "[Step 18590] Loss: 2.1864\n",
            "[Step 18600] Loss: 2.4624\n",
            "[Step 18610] Loss: 2.1186\n",
            "[Step 18620] Loss: 3.0829\n",
            "[Step 18630] Loss: 2.4507\n",
            "[Step 18640] Loss: 2.6268\n",
            "[Step 18650] Loss: 2.6780\n",
            "[Step 18660] Loss: 2.1589\n",
            "[Step 18670] Loss: 3.0067\n",
            "[Step 18680] Loss: 2.4194\n",
            "[Step 18690] Loss: 2.9210\n",
            "[Step 18700] Loss: 2.3677\n",
            "[Step 18710] Loss: 3.1298\n",
            "[Step 18720] Loss: 2.4344\n",
            "[Step 18730] Loss: 2.6327\n",
            "[Step 18740] Loss: 2.4428\n",
            "[Step 18750] Loss: 2.5649\n",
            "[Step 18760] Loss: 2.7330\n",
            "[Step 18770] Loss: 2.9977\n",
            "[Step 18780] Loss: 2.4326\n",
            "[Step 18790] Loss: 2.6992\n",
            "[Step 18800] Loss: 2.6057\n",
            "[Step 18810] Loss: 2.8547\n",
            "[Step 18820] Loss: 2.1278\n",
            "[Step 18830] Loss: 2.7232\n",
            "[Step 18840] Loss: 2.4934\n",
            "[Step 18850] Loss: 2.5540\n",
            "[Step 18860] Loss: 2.6336\n",
            "[Step 18870] Loss: 2.7492\n",
            "[Step 18880] Loss: 2.3437\n",
            "[Step 18890] Loss: 2.7053\n",
            "[Step 18900] Loss: 2.2652\n",
            "[Step 18910] Loss: 2.6216\n",
            "[Step 18920] Loss: 2.7146\n",
            "[Step 18930] Loss: 2.4110\n",
            "[Step 18940] Loss: 2.1410\n",
            "ðŸ“˜ Epoch 50 - Avg Training Loss: 2.5262\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6021 | Accuracy: 0.3564 | Precision: 0.3335\n",
            "[Step 18950] Loss: 2.3963\n",
            "[Step 18960] Loss: 2.3943\n",
            "[Step 18970] Loss: 2.2048\n",
            "[Step 18980] Loss: 2.0728\n",
            "[Step 18990] Loss: 1.9690\n",
            "[Step 19000] Loss: 2.1520\n",
            "[Step 19010] Loss: 2.5573\n",
            "[Step 19020] Loss: 2.8851\n",
            "[Step 19030] Loss: 2.8160\n",
            "[Step 19040] Loss: 2.9087\n",
            "[Step 19050] Loss: 2.5397\n",
            "[Step 19060] Loss: 2.7044\n",
            "[Step 19070] Loss: 2.5260\n",
            "[Step 19080] Loss: 2.4908\n",
            "[Step 19090] Loss: 2.6309\n",
            "[Step 19100] Loss: 2.6694\n",
            "[Step 19110] Loss: 2.3507\n",
            "[Step 19120] Loss: 2.5826\n",
            "[Step 19130] Loss: 3.1758\n",
            "[Step 19140] Loss: 2.6133\n",
            "[Step 19150] Loss: 2.2977\n",
            "[Step 19160] Loss: 2.3266\n",
            "[Step 19170] Loss: 2.4714\n",
            "[Step 19180] Loss: 2.3681\n",
            "[Step 19190] Loss: 2.7181\n",
            "[Step 19200] Loss: 2.7890\n",
            "[Step 19210] Loss: 2.7225\n",
            "[Step 19220] Loss: 2.8507\n",
            "[Step 19230] Loss: 2.9839\n",
            "[Step 19240] Loss: 2.7516\n",
            "[Step 19250] Loss: 2.2409\n",
            "[Step 19260] Loss: 2.5433\n",
            "[Step 19270] Loss: 2.2425\n",
            "[Step 19280] Loss: 2.1745\n",
            "[Step 19290] Loss: 2.2132\n",
            "[Step 19300] Loss: 2.3286\n",
            "[Step 19310] Loss: 2.5175\n",
            "[Step 19320] Loss: 2.8265\n",
            "[Step 19330] Loss: 2.4501\n",
            "[Step 19340] Loss: 2.7370\n",
            "[Step 19350] Loss: 2.5280\n",
            "[Step 19360] Loss: 3.2396\n",
            "[Step 19370] Loss: 2.0472\n",
            "[Step 19380] Loss: 2.3918\n",
            "[Step 19390] Loss: 2.2099\n",
            "[Step 19400] Loss: 2.1749\n",
            "[Step 19410] Loss: 2.7228\n",
            "[Step 19420] Loss: 2.3910\n",
            "[Step 19430] Loss: 2.5856\n",
            "[Step 19440] Loss: 2.6455\n",
            "[Step 19450] Loss: 2.3016\n",
            "[Step 19460] Loss: 2.1678\n",
            "[Step 19470] Loss: 2.2710\n",
            "[Step 19480] Loss: 2.7061\n",
            "[Step 19490] Loss: 2.8143\n",
            "[Step 19500] Loss: 2.6115\n",
            "[Step 19510] Loss: 2.2705\n",
            "[Step 19520] Loss: 2.4753\n",
            "[Step 19530] Loss: 2.4424\n",
            "[Step 19540] Loss: 2.7812\n",
            "[Step 19550] Loss: 2.3243\n",
            "[Step 19560] Loss: 2.9090\n",
            "[Step 19570] Loss: 2.3723\n",
            "[Step 19580] Loss: 2.4009\n",
            "[Step 19590] Loss: 2.2517\n",
            "[Step 19600] Loss: 2.8531\n",
            "[Step 19610] Loss: 2.6214\n",
            "[Step 19620] Loss: 2.1578\n",
            "[Step 19630] Loss: 1.9954\n",
            "[Step 19640] Loss: 1.9597\n",
            "[Step 19650] Loss: 2.4951\n",
            "[Step 19660] Loss: 2.5366\n",
            "[Step 19670] Loss: 2.4318\n",
            "[Step 19680] Loss: 2.3633\n",
            "[Step 19690] Loss: 2.9016\n",
            "[Step 19700] Loss: 2.4839\n",
            "[Step 19710] Loss: 2.5344\n",
            "[Step 19720] Loss: 2.1789\n",
            "[Step 19730] Loss: 2.6468\n",
            "[Step 19740] Loss: 2.5247\n",
            "[Step 19750] Loss: 2.3163\n",
            "[Step 19760] Loss: 2.0863\n",
            "[Step 19770] Loss: 2.9974\n",
            "[Step 19780] Loss: 3.0054\n",
            "[Step 19790] Loss: 2.3300\n",
            "[Step 19800] Loss: 2.6155\n",
            "[Step 19810] Loss: 2.5544\n",
            "[Step 19820] Loss: 2.7173\n",
            "[Step 19830] Loss: 2.3328\n",
            "[Step 19840] Loss: 2.3003\n",
            "[Step 19850] Loss: 2.3553\n",
            "[Step 19860] Loss: 2.8493\n",
            "[Step 19870] Loss: 2.1154\n",
            "[Step 19880] Loss: 2.6361\n",
            "[Step 19890] Loss: 2.1673\n",
            "[Step 19900] Loss: 2.2997\n",
            "[Step 19910] Loss: 2.7651\n",
            "[Step 19920] Loss: 2.7483\n",
            "[Step 19930] Loss: 2.4069\n",
            "[Step 19940] Loss: 2.6913\n",
            "[Step 19950] Loss: 2.1611\n",
            "[Step 19960] Loss: 2.5199\n",
            "[Step 19970] Loss: 2.6792\n",
            "[Step 19980] Loss: 2.7293\n",
            "[Step 19990] Loss: 2.2825\n",
            "[Step 20000] Loss: 2.3722\n",
            "[Step 20010] Loss: 2.3790\n",
            "[Step 20020] Loss: 2.4867\n",
            "[Step 20030] Loss: 2.5823\n",
            "[Step 20040] Loss: 2.3167\n",
            "[Step 20050] Loss: 2.5319\n",
            "[Step 20060] Loss: 2.5745\n",
            "[Step 20070] Loss: 2.6783\n",
            "[Step 20080] Loss: 2.4515\n",
            "[Step 20090] Loss: 2.2047\n",
            "[Step 20100] Loss: 2.5324\n",
            "[Step 20110] Loss: 2.3789\n",
            "[Step 20120] Loss: 2.8498\n",
            "[Step 20130] Loss: 2.0984\n",
            "[Step 20140] Loss: 2.8273\n",
            "[Step 20150] Loss: 2.5283\n",
            "[Step 20160] Loss: 2.6704\n",
            "[Step 20170] Loss: 2.0013\n",
            "[Step 20180] Loss: 2.6775\n",
            "[Step 20190] Loss: 2.6542\n",
            "[Step 20200] Loss: 2.3780\n",
            "[Step 20210] Loss: 2.6766\n",
            "[Step 20220] Loss: 2.1962\n",
            "[Step 20230] Loss: 2.4569\n",
            "[Step 20240] Loss: 2.5964\n",
            "[Step 20250] Loss: 2.7005\n",
            "[Step 20260] Loss: 2.4315\n",
            "[Step 20270] Loss: 2.1602\n",
            "[Step 20280] Loss: 2.3713\n",
            "[Step 20290] Loss: 2.8694\n",
            "[Step 20300] Loss: 2.6812\n",
            "[Step 20310] Loss: 2.6627\n",
            "[Step 20320] Loss: 2.2831\n",
            "[Step 20330] Loss: 2.2836\n",
            "[Step 20340] Loss: 2.4442\n",
            "[Step 20350] Loss: 2.7929\n",
            "[Step 20360] Loss: 2.1423\n",
            "[Step 20370] Loss: 2.2975\n",
            "[Step 20380] Loss: 2.7945\n",
            "[Step 20390] Loss: 2.1394\n",
            "[Step 20400] Loss: 2.5555\n",
            "[Step 20410] Loss: 2.5429\n",
            "[Step 20420] Loss: 2.4970\n",
            "[Step 20430] Loss: 2.6290\n",
            "[Step 20440] Loss: 2.3660\n",
            "[Step 20450] Loss: 2.3641\n",
            "[Step 20460] Loss: 2.8598\n",
            "[Step 20470] Loss: 2.5236\n",
            "[Step 20480] Loss: 2.3497\n",
            "[Step 20490] Loss: 2.6620\n",
            "[Step 20500] Loss: 2.4391\n",
            "[Step 20510] Loss: 2.6541\n",
            "[Step 20520] Loss: 2.5900\n",
            "[Step 20530] Loss: 2.7198\n",
            "[Step 20540] Loss: 2.2928\n",
            "[Step 20550] Loss: 2.7756\n",
            "[Step 20560] Loss: 3.0709\n",
            "[Step 20570] Loss: 2.1420\n",
            "[Step 20580] Loss: 2.4921\n",
            "[Step 20590] Loss: 2.3455\n",
            "[Step 20600] Loss: 2.6473\n",
            "[Step 20610] Loss: 2.4018\n",
            "[Step 20620] Loss: 2.2193\n",
            "[Step 20630] Loss: 2.0432\n",
            "[Step 20640] Loss: 2.9167\n",
            "[Step 20650] Loss: 2.4340\n",
            "[Step 20660] Loss: 2.7186\n",
            "[Step 20670] Loss: 2.8052\n",
            "[Step 20680] Loss: 2.0129\n",
            "[Step 20690] Loss: 2.5871\n",
            "[Step 20700] Loss: 2.6563\n",
            "[Step 20710] Loss: 2.4335\n",
            "[Step 20720] Loss: 2.7044\n",
            "[Step 20730] Loss: 2.2937\n",
            "[Step 20740] Loss: 2.4329\n",
            "[Step 20750] Loss: 2.6129\n",
            "[Step 20760] Loss: 2.7685\n",
            "[Step 20770] Loss: 2.2873\n",
            "[Step 20780] Loss: 2.3250\n",
            "[Step 20790] Loss: 2.3665\n",
            "[Step 20800] Loss: 2.8747\n",
            "[Step 20810] Loss: 2.3937\n",
            "[Step 20820] Loss: 2.5754\n",
            "[Step 20830] Loss: 2.3929\n",
            "ðŸ“˜ Epoch 51 - Avg Training Loss: 2.5225\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6076 | Accuracy: 0.3545 | Precision: 0.3305\n",
            "[Step 20840] Loss: 2.4094\n",
            "[Step 20850] Loss: 2.6850\n",
            "[Step 20860] Loss: 2.8332\n",
            "[Step 20870] Loss: 2.4494\n",
            "[Step 20880] Loss: 2.5310\n",
            "[Step 20890] Loss: 2.1725\n",
            "[Step 20900] Loss: 2.1904\n",
            "[Step 20910] Loss: 2.3141\n",
            "[Step 20920] Loss: 2.0378\n",
            "[Step 20930] Loss: 2.1148\n",
            "[Step 20940] Loss: 2.6984\n",
            "[Step 20950] Loss: 2.3117\n",
            "[Step 20960] Loss: 2.6689\n",
            "[Step 20970] Loss: 2.6806\n",
            "[Step 20980] Loss: 2.3657\n",
            "[Step 20990] Loss: 2.2978\n",
            "[Step 21000] Loss: 2.2872\n",
            "[Step 21010] Loss: 2.9220\n",
            "[Step 21020] Loss: 2.5648\n",
            "[Step 21030] Loss: 2.8216\n",
            "[Step 21040] Loss: 2.4677\n",
            "[Step 21050] Loss: 2.2691\n",
            "[Step 21060] Loss: 1.9275\n",
            "[Step 21070] Loss: 2.5369\n",
            "[Step 21080] Loss: 2.2914\n",
            "[Step 21090] Loss: 2.2856\n",
            "[Step 21100] Loss: 2.5575\n",
            "[Step 21110] Loss: 2.5255\n",
            "[Step 21120] Loss: 2.3998\n",
            "[Step 21130] Loss: 2.6233\n",
            "[Step 21140] Loss: 2.4458\n",
            "[Step 21150] Loss: 2.6238\n",
            "[Step 21160] Loss: 2.8237\n",
            "[Step 21170] Loss: 2.4799\n",
            "[Step 21180] Loss: 2.7846\n",
            "[Step 21190] Loss: 2.6178\n",
            "[Step 21200] Loss: 2.6241\n",
            "[Step 21210] Loss: 2.3323\n",
            "[Step 21220] Loss: 2.8857\n",
            "[Step 21230] Loss: 2.6798\n",
            "[Step 21240] Loss: 2.5027\n",
            "[Step 21250] Loss: 3.0166\n",
            "[Step 21260] Loss: 2.5764\n",
            "[Step 21270] Loss: 2.6155\n",
            "[Step 21280] Loss: 2.3172\n",
            "[Step 21290] Loss: 2.4665\n",
            "[Step 21300] Loss: 2.5211\n",
            "[Step 21310] Loss: 2.2867\n",
            "[Step 21320] Loss: 2.2813\n",
            "[Step 21330] Loss: 2.4259\n",
            "[Step 21340] Loss: 2.4653\n",
            "[Step 21350] Loss: 2.0911\n",
            "[Step 21360] Loss: 2.3082\n",
            "[Step 21370] Loss: 2.4707\n",
            "[Step 21380] Loss: 2.1094\n",
            "[Step 21390] Loss: 2.6386\n",
            "[Step 21400] Loss: 2.7056\n",
            "[Step 21410] Loss: 2.8058\n",
            "[Step 21420] Loss: 2.4610\n",
            "[Step 21430] Loss: 2.2276\n",
            "[Step 21440] Loss: 2.5289\n",
            "[Step 21450] Loss: 2.5437\n",
            "[Step 21460] Loss: 2.4664\n",
            "[Step 21470] Loss: 2.2316\n",
            "[Step 21480] Loss: 3.1155\n",
            "[Step 21490] Loss: 2.7696\n",
            "[Step 21500] Loss: 2.4548\n",
            "[Step 21510] Loss: 2.2005\n",
            "[Step 21520] Loss: 3.0122\n",
            "[Step 21530] Loss: 3.0104\n",
            "[Step 21540] Loss: 2.1760\n",
            "[Step 21550] Loss: 2.5863\n",
            "[Step 21560] Loss: 2.4419\n",
            "[Step 21570] Loss: 2.5515\n",
            "[Step 21580] Loss: 2.3194\n",
            "[Step 21590] Loss: 2.0369\n",
            "[Step 21600] Loss: 2.2953\n",
            "[Step 21610] Loss: 2.1073\n",
            "[Step 21620] Loss: 2.3403\n",
            "[Step 21630] Loss: 2.9560\n",
            "[Step 21640] Loss: 2.4521\n",
            "[Step 21650] Loss: 2.5175\n",
            "[Step 21660] Loss: 2.7941\n",
            "[Step 21670] Loss: 2.7388\n",
            "[Step 21680] Loss: 2.3141\n",
            "[Step 21690] Loss: 2.2671\n",
            "[Step 21700] Loss: 2.8970\n",
            "[Step 21710] Loss: 2.4537\n",
            "[Step 21720] Loss: 2.8012\n",
            "[Step 21730] Loss: 2.2346\n",
            "[Step 21740] Loss: 2.6285\n",
            "[Step 21750] Loss: 2.5659\n",
            "[Step 21760] Loss: 2.2177\n",
            "[Step 21770] Loss: 2.7292\n",
            "[Step 21780] Loss: 2.6592\n",
            "[Step 21790] Loss: 2.6157\n",
            "[Step 21800] Loss: 2.1951\n",
            "[Step 21810] Loss: 2.8714\n",
            "[Step 21820] Loss: 2.1498\n",
            "[Step 21830] Loss: 2.6306\n",
            "[Step 21840] Loss: 2.8024\n",
            "[Step 21850] Loss: 2.5925\n",
            "[Step 21860] Loss: 2.2570\n",
            "[Step 21870] Loss: 2.0868\n",
            "[Step 21880] Loss: 2.4830\n",
            "[Step 21890] Loss: 2.9084\n",
            "[Step 21900] Loss: 2.2190\n",
            "[Step 21910] Loss: 2.6844\n",
            "[Step 21920] Loss: 2.8879\n",
            "[Step 21930] Loss: 2.3222\n",
            "[Step 21940] Loss: 2.8780\n",
            "[Step 21950] Loss: 2.4332\n",
            "[Step 21960] Loss: 2.4320\n",
            "[Step 21970] Loss: 2.9096\n",
            "[Step 21980] Loss: 2.3373\n",
            "[Step 21990] Loss: 2.6431\n",
            "[Step 22000] Loss: 2.1551\n",
            "[Step 22010] Loss: 2.2086\n",
            "[Step 22020] Loss: 2.3124\n",
            "[Step 22030] Loss: 2.6808\n",
            "[Step 22040] Loss: 2.1338\n",
            "[Step 22050] Loss: 2.9511\n",
            "[Step 22060] Loss: 2.5451\n",
            "[Step 22070] Loss: 2.2635\n",
            "[Step 22080] Loss: 2.6278\n",
            "[Step 22090] Loss: 2.4747\n",
            "[Step 22100] Loss: 2.5560\n",
            "[Step 22110] Loss: 2.7065\n",
            "[Step 22120] Loss: 2.5537\n",
            "[Step 22130] Loss: 2.2636\n",
            "[Step 22140] Loss: 2.5669\n",
            "[Step 22150] Loss: 2.2067\n",
            "[Step 22160] Loss: 2.4370\n",
            "[Step 22170] Loss: 2.4881\n",
            "[Step 22180] Loss: 2.1519\n",
            "[Step 22190] Loss: 2.3865\n",
            "[Step 22200] Loss: 2.9766\n",
            "[Step 22210] Loss: 2.0096\n",
            "[Step 22220] Loss: 2.5908\n",
            "[Step 22230] Loss: 1.7368\n",
            "[Step 22240] Loss: 2.3184\n",
            "[Step 22250] Loss: 2.3470\n",
            "[Step 22260] Loss: 2.7261\n",
            "[Step 22270] Loss: 2.2916\n",
            "[Step 22280] Loss: 2.7515\n",
            "[Step 22290] Loss: 2.7126\n",
            "[Step 22300] Loss: 2.6274\n",
            "[Step 22310] Loss: 2.5472\n",
            "[Step 22320] Loss: 2.7670\n",
            "[Step 22330] Loss: 2.5023\n",
            "[Step 22340] Loss: 2.3759\n",
            "[Step 22350] Loss: 2.4247\n",
            "[Step 22360] Loss: 2.3776\n",
            "[Step 22370] Loss: 2.2673\n",
            "[Step 22380] Loss: 2.3806\n",
            "[Step 22390] Loss: 2.8198\n",
            "[Step 22400] Loss: 2.1866\n",
            "[Step 22410] Loss: 2.3596\n",
            "[Step 22420] Loss: 2.8574\n",
            "[Step 22430] Loss: 2.9007\n",
            "[Step 22440] Loss: 2.7097\n",
            "[Step 22450] Loss: 2.2487\n",
            "[Step 22460] Loss: 2.6367\n",
            "[Step 22470] Loss: 2.4904\n",
            "[Step 22480] Loss: 2.7611\n",
            "[Step 22490] Loss: 2.4326\n",
            "[Step 22500] Loss: 2.7041\n",
            "[Step 22510] Loss: 2.5742\n",
            "[Step 22520] Loss: 2.3784\n",
            "[Step 22530] Loss: 2.8016\n",
            "[Step 22540] Loss: 2.7004\n",
            "[Step 22550] Loss: 2.5165\n",
            "[Step 22560] Loss: 2.1725\n",
            "[Step 22570] Loss: 2.5830\n",
            "[Step 22580] Loss: 2.3280\n",
            "[Step 22590] Loss: 2.3764\n",
            "[Step 22600] Loss: 2.4410\n",
            "[Step 22610] Loss: 2.4686\n",
            "[Step 22620] Loss: 2.8126\n",
            "[Step 22630] Loss: 2.5353\n",
            "[Step 22640] Loss: 2.3891\n",
            "[Step 22650] Loss: 2.4299\n",
            "[Step 22660] Loss: 2.7409\n",
            "[Step 22670] Loss: 2.6058\n",
            "[Step 22680] Loss: 2.6468\n",
            "[Step 22690] Loss: 2.7451\n",
            "[Step 22700] Loss: 2.4010\n",
            "[Step 22710] Loss: 2.5607\n",
            "[Step 22720] Loss: 2.5524\n",
            "ðŸ“˜ Epoch 52 - Avg Training Loss: 2.5224\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6033 | Accuracy: 0.3552 | Precision: 0.3293\n",
            "[Step 22730] Loss: 2.1925\n",
            "[Step 22740] Loss: 2.3993\n",
            "[Step 22750] Loss: 2.7200\n",
            "[Step 22760] Loss: 2.6453\n",
            "[Step 22770] Loss: 2.6458\n",
            "[Step 22780] Loss: 2.3635\n",
            "[Step 22790] Loss: 2.2907\n",
            "[Step 22800] Loss: 3.0299\n",
            "[Step 22810] Loss: 2.7439\n",
            "[Step 22820] Loss: 2.3887\n",
            "[Step 22830] Loss: 2.3151\n",
            "[Step 22840] Loss: 2.2198\n",
            "[Step 22850] Loss: 2.7610\n",
            "[Step 22860] Loss: 2.6403\n",
            "[Step 22870] Loss: 2.5246\n",
            "[Step 22880] Loss: 2.3669\n",
            "[Step 22890] Loss: 2.8801\n",
            "[Step 22900] Loss: 2.7004\n",
            "[Step 22910] Loss: 3.0262\n",
            "[Step 22920] Loss: 2.4444\n",
            "[Step 22930] Loss: 2.6016\n",
            "[Step 22940] Loss: 2.3382\n",
            "[Step 22950] Loss: 2.6080\n",
            "[Step 22960] Loss: 2.8449\n",
            "[Step 22970] Loss: 2.8010\n",
            "[Step 22980] Loss: 2.2101\n",
            "[Step 22990] Loss: 2.4524\n",
            "[Step 23000] Loss: 2.5550\n",
            "[Step 23010] Loss: 2.2899\n",
            "[Step 23020] Loss: 2.3544\n",
            "[Step 23030] Loss: 2.4323\n",
            "[Step 23040] Loss: 2.2706\n",
            "[Step 23050] Loss: 2.3003\n",
            "[Step 23060] Loss: 2.3680\n",
            "[Step 23070] Loss: 2.2240\n",
            "[Step 23080] Loss: 2.1497\n",
            "[Step 23090] Loss: 2.1467\n",
            "[Step 23100] Loss: 2.4994\n",
            "[Step 23110] Loss: 2.1294\n",
            "[Step 23120] Loss: 2.5038\n",
            "[Step 23130] Loss: 2.7676\n",
            "[Step 23140] Loss: 2.8760\n",
            "[Step 23150] Loss: 2.7452\n",
            "[Step 23160] Loss: 2.1271\n",
            "[Step 23170] Loss: 2.9115\n",
            "[Step 23180] Loss: 2.4541\n",
            "[Step 23190] Loss: 2.5165\n",
            "[Step 23200] Loss: 2.2694\n",
            "[Step 23210] Loss: 2.4085\n",
            "[Step 23220] Loss: 2.8288\n",
            "[Step 23230] Loss: 2.4624\n",
            "[Step 23240] Loss: 2.1791\n",
            "[Step 23250] Loss: 2.9655\n",
            "[Step 23260] Loss: 2.6330\n",
            "[Step 23270] Loss: 2.9731\n",
            "[Step 23280] Loss: 2.8174\n",
            "[Step 23290] Loss: 2.8077\n",
            "[Step 23300] Loss: 2.3833\n",
            "[Step 23310] Loss: 2.3156\n",
            "[Step 23320] Loss: 2.7036\n",
            "[Step 23330] Loss: 2.2841\n",
            "[Step 23340] Loss: 2.7536\n",
            "[Step 23350] Loss: 3.1601\n",
            "[Step 23360] Loss: 2.3983\n",
            "[Step 23370] Loss: 2.3679\n",
            "[Step 23380] Loss: 2.4763\n",
            "[Step 23390] Loss: 2.5813\n",
            "[Step 23400] Loss: 2.5828\n",
            "[Step 23410] Loss: 2.6254\n",
            "[Step 23420] Loss: 2.1594\n",
            "[Step 23430] Loss: 2.1960\n",
            "[Step 23440] Loss: 2.5459\n",
            "[Step 23450] Loss: 2.6160\n",
            "[Step 23460] Loss: 2.7506\n",
            "[Step 23470] Loss: 2.2932\n",
            "[Step 23480] Loss: 2.4929\n",
            "[Step 23490] Loss: 2.2425\n",
            "[Step 23500] Loss: 2.5329\n",
            "[Step 23510] Loss: 2.6154\n",
            "[Step 23520] Loss: 2.9938\n",
            "[Step 23530] Loss: 2.7662\n",
            "[Step 23540] Loss: 2.8604\n",
            "[Step 23550] Loss: 2.4715\n",
            "[Step 23560] Loss: 2.9971\n",
            "[Step 23570] Loss: 2.6339\n",
            "[Step 23580] Loss: 2.5539\n",
            "[Step 23590] Loss: 2.0485\n",
            "[Step 23600] Loss: 2.7730\n",
            "[Step 23610] Loss: 2.7421\n",
            "[Step 23620] Loss: 2.5888\n",
            "[Step 23630] Loss: 2.0056\n",
            "[Step 23640] Loss: 2.8895\n",
            "[Step 23650] Loss: 2.4686\n",
            "[Step 23660] Loss: 2.5107\n",
            "[Step 23670] Loss: 2.4978\n",
            "[Step 23680] Loss: 2.7492\n",
            "[Step 23690] Loss: 2.6338\n",
            "[Step 23700] Loss: 2.2517\n",
            "[Step 23710] Loss: 2.5819\n",
            "[Step 23720] Loss: 3.0271\n",
            "[Step 23730] Loss: 2.5486\n",
            "[Step 23740] Loss: 2.8492\n",
            "[Step 23750] Loss: 2.8854\n",
            "[Step 23760] Loss: 2.5629\n",
            "[Step 23770] Loss: 2.3542\n",
            "[Step 23780] Loss: 2.6027\n",
            "[Step 23790] Loss: 2.6180\n",
            "[Step 23800] Loss: 2.5547\n",
            "[Step 23810] Loss: 2.6758\n",
            "[Step 23820] Loss: 2.4215\n",
            "[Step 23830] Loss: 1.8154\n",
            "[Step 23840] Loss: 2.0577\n",
            "[Step 23850] Loss: 2.5211\n",
            "[Step 23860] Loss: 2.2200\n",
            "[Step 23870] Loss: 2.1514\n",
            "[Step 23880] Loss: 2.4362\n",
            "[Step 23890] Loss: 2.2935\n",
            "[Step 23900] Loss: 2.8134\n",
            "[Step 23910] Loss: 2.7649\n",
            "[Step 23920] Loss: 2.0998\n",
            "[Step 23930] Loss: 2.4815\n",
            "[Step 23940] Loss: 2.8261\n",
            "[Step 23950] Loss: 2.3934\n",
            "[Step 23960] Loss: 2.6573\n",
            "[Step 23970] Loss: 2.5784\n",
            "[Step 23980] Loss: 2.4493\n",
            "[Step 23990] Loss: 3.1502\n",
            "[Step 24000] Loss: 2.3433\n",
            "[Step 24010] Loss: 3.3014\n",
            "[Step 24020] Loss: 2.3637\n",
            "[Step 24030] Loss: 2.0254\n",
            "[Step 24040] Loss: 3.0212\n",
            "[Step 24050] Loss: 2.8248\n",
            "[Step 24060] Loss: 3.0777\n",
            "[Step 24070] Loss: 2.6110\n",
            "[Step 24080] Loss: 2.8446\n",
            "[Step 24090] Loss: 2.5227\n",
            "[Step 24100] Loss: 2.4222\n",
            "[Step 24110] Loss: 2.4723\n",
            "[Step 24120] Loss: 2.3732\n",
            "[Step 24130] Loss: 2.3199\n",
            "[Step 24140] Loss: 2.5994\n",
            "[Step 24150] Loss: 2.5413\n",
            "[Step 24160] Loss: 2.3985\n",
            "[Step 24170] Loss: 2.6121\n",
            "[Step 24180] Loss: 2.3689\n",
            "[Step 24190] Loss: 2.4961\n",
            "[Step 24200] Loss: 1.8849\n",
            "[Step 24210] Loss: 2.7822\n",
            "[Step 24220] Loss: 2.4001\n",
            "[Step 24230] Loss: 2.5183\n",
            "[Step 24240] Loss: 2.1524\n",
            "[Step 24250] Loss: 2.7644\n",
            "[Step 24260] Loss: 2.2091\n",
            "[Step 24270] Loss: 2.5838\n",
            "[Step 24280] Loss: 2.7760\n",
            "[Step 24290] Loss: 2.2308\n",
            "[Step 24300] Loss: 1.9517\n",
            "[Step 24310] Loss: 2.4119\n",
            "[Step 24320] Loss: 2.2634\n",
            "[Step 24330] Loss: 2.2336\n",
            "[Step 24340] Loss: 2.5767\n",
            "[Step 24350] Loss: 2.7135\n",
            "[Step 24360] Loss: 2.9663\n",
            "[Step 24370] Loss: 2.3406\n",
            "[Step 24380] Loss: 2.7105\n",
            "[Step 24390] Loss: 3.2493\n",
            "[Step 24400] Loss: 2.5914\n",
            "[Step 24410] Loss: 2.6118\n",
            "[Step 24420] Loss: 3.0452\n",
            "[Step 24430] Loss: 2.3530\n",
            "[Step 24440] Loss: 2.7787\n",
            "[Step 24450] Loss: 2.7874\n",
            "[Step 24460] Loss: 2.7243\n",
            "[Step 24470] Loss: 2.2155\n",
            "[Step 24480] Loss: 2.5134\n",
            "[Step 24490] Loss: 2.8653\n",
            "[Step 24500] Loss: 2.6524\n",
            "[Step 24510] Loss: 2.8534\n",
            "[Step 24520] Loss: 2.5743\n",
            "[Step 24530] Loss: 2.4260\n",
            "[Step 24540] Loss: 2.5554\n",
            "[Step 24550] Loss: 2.4912\n",
            "[Step 24560] Loss: 2.4862\n",
            "[Step 24570] Loss: 2.5663\n",
            "[Step 24580] Loss: 2.3300\n",
            "[Step 24590] Loss: 2.5444\n",
            "[Step 24600] Loss: 2.0480\n",
            "[Step 24610] Loss: 2.7976\n",
            "[Step 24620] Loss: 2.3695\n",
            "ðŸ“˜ Epoch 53 - Avg Training Loss: 2.5274\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6060 | Accuracy: 0.3564 | Precision: 0.3355\n",
            "[Step 24630] Loss: 2.3593\n",
            "[Step 24640] Loss: 2.4472\n",
            "[Step 24650] Loss: 2.3411\n",
            "[Step 24660] Loss: 2.4090\n",
            "[Step 24670] Loss: 2.3610\n",
            "[Step 24680] Loss: 2.2569\n",
            "[Step 24690] Loss: 2.3500\n",
            "[Step 24700] Loss: 1.8495\n",
            "[Step 24710] Loss: 2.2490\n",
            "[Step 24720] Loss: 2.4126\n",
            "[Step 24730] Loss: 2.4153\n",
            "[Step 24740] Loss: 2.2559\n",
            "[Step 24750] Loss: 2.7131\n",
            "[Step 24760] Loss: 2.5408\n",
            "[Step 24770] Loss: 2.0900\n",
            "[Step 24780] Loss: 2.6496\n",
            "[Step 24790] Loss: 2.0912\n",
            "[Step 24800] Loss: 2.5471\n",
            "[Step 24810] Loss: 3.1722\n",
            "[Step 24820] Loss: 2.8158\n",
            "[Step 24830] Loss: 2.4419\n",
            "[Step 24840] Loss: 2.6691\n",
            "[Step 24850] Loss: 2.5819\n",
            "[Step 24860] Loss: 2.2993\n",
            "[Step 24870] Loss: 2.1307\n",
            "[Step 24880] Loss: 2.9780\n",
            "[Step 24890] Loss: 2.8587\n",
            "[Step 24900] Loss: 2.8258\n",
            "[Step 24910] Loss: 2.4943\n",
            "[Step 24920] Loss: 2.8801\n",
            "[Step 24930] Loss: 2.7462\n",
            "[Step 24940] Loss: 2.5498\n",
            "[Step 24950] Loss: 2.6117\n",
            "[Step 24960] Loss: 2.4924\n",
            "[Step 24970] Loss: 1.9470\n",
            "[Step 24980] Loss: 2.3997\n",
            "[Step 24990] Loss: 2.5483\n",
            "[Step 25000] Loss: 2.2606\n",
            "[Step 25010] Loss: 2.2692\n",
            "[Step 25020] Loss: 3.1628\n",
            "[Step 25030] Loss: 2.5475\n",
            "[Step 25040] Loss: 2.8115\n",
            "[Step 25050] Loss: 2.4260\n",
            "[Step 25060] Loss: 2.8208\n",
            "[Step 25070] Loss: 2.0718\n",
            "[Step 25080] Loss: 3.0672\n",
            "[Step 25090] Loss: 2.4254\n",
            "[Step 25100] Loss: 2.3669\n",
            "[Step 25110] Loss: 2.6323\n",
            "[Step 25120] Loss: 2.6741\n",
            "[Step 25130] Loss: 2.4463\n",
            "[Step 25140] Loss: 2.6555\n",
            "[Step 25150] Loss: 2.4500\n",
            "[Step 25160] Loss: 2.4391\n",
            "[Step 25170] Loss: 2.7845\n",
            "[Step 25180] Loss: 2.4207\n",
            "[Step 25190] Loss: 2.6153\n",
            "[Step 25200] Loss: 2.4775\n",
            "[Step 25210] Loss: 2.7402\n",
            "[Step 25220] Loss: 2.7269\n",
            "[Step 25230] Loss: 1.8864\n",
            "[Step 25240] Loss: 2.5316\n",
            "[Step 25250] Loss: 2.7184\n",
            "[Step 25260] Loss: 2.5242\n",
            "[Step 25270] Loss: 2.4069\n",
            "[Step 25280] Loss: 2.7613\n",
            "[Step 25290] Loss: 2.6997\n",
            "[Step 25300] Loss: 2.4503\n",
            "[Step 25310] Loss: 2.5554\n",
            "[Step 25320] Loss: 1.9532\n",
            "[Step 25330] Loss: 1.8763\n",
            "[Step 25340] Loss: 2.4268\n",
            "[Step 25350] Loss: 2.5845\n",
            "[Step 25360] Loss: 2.6260\n",
            "[Step 25370] Loss: 2.3907\n",
            "[Step 25380] Loss: 1.8879\n",
            "[Step 25390] Loss: 2.4691\n",
            "[Step 25400] Loss: 2.4591\n",
            "[Step 25410] Loss: 2.6286\n",
            "[Step 25420] Loss: 2.0915\n",
            "[Step 25430] Loss: 2.5606\n",
            "[Step 25440] Loss: 2.3134\n",
            "[Step 25450] Loss: 2.4776\n",
            "[Step 25460] Loss: 2.9408\n",
            "[Step 25470] Loss: 2.5491\n",
            "[Step 25480] Loss: 2.4024\n",
            "[Step 25490] Loss: 2.5544\n",
            "[Step 25500] Loss: 2.3920\n",
            "[Step 25510] Loss: 2.5644\n",
            "[Step 25520] Loss: 3.1524\n",
            "[Step 25530] Loss: 2.3596\n",
            "[Step 25540] Loss: 2.4723\n",
            "[Step 25550] Loss: 3.0861\n",
            "[Step 25560] Loss: 2.4566\n",
            "[Step 25570] Loss: 2.2513\n",
            "[Step 25580] Loss: 2.7103\n",
            "[Step 25590] Loss: 2.3032\n",
            "[Step 25600] Loss: 2.2576\n",
            "[Step 25610] Loss: 2.2091\n",
            "[Step 25620] Loss: 2.5556\n",
            "[Step 25630] Loss: 2.4047\n",
            "[Step 25640] Loss: 2.5580\n",
            "[Step 25650] Loss: 2.4407\n",
            "[Step 25660] Loss: 2.3351\n",
            "[Step 25670] Loss: 2.4033\n",
            "[Step 25680] Loss: 2.0923\n",
            "[Step 25690] Loss: 2.4199\n",
            "[Step 25700] Loss: 2.5927\n",
            "[Step 25710] Loss: 2.7557\n",
            "[Step 25720] Loss: 2.7029\n",
            "[Step 25730] Loss: 2.6076\n",
            "[Step 25740] Loss: 2.8467\n",
            "[Step 25750] Loss: 2.4544\n",
            "[Step 25760] Loss: 2.4533\n",
            "[Step 25770] Loss: 2.5158\n",
            "[Step 25780] Loss: 2.6076\n",
            "[Step 25790] Loss: 2.7374\n",
            "[Step 25800] Loss: 2.6323\n",
            "[Step 25810] Loss: 1.8225\n",
            "[Step 25820] Loss: 2.3042\n",
            "[Step 25830] Loss: 2.6364\n",
            "[Step 25840] Loss: 2.4499\n",
            "[Step 25850] Loss: 2.4619\n",
            "[Step 25860] Loss: 2.1292\n",
            "[Step 25870] Loss: 3.0131\n",
            "[Step 25880] Loss: 2.1139\n",
            "[Step 25890] Loss: 2.1514\n",
            "[Step 25900] Loss: 2.3119\n",
            "[Step 25910] Loss: 2.9418\n",
            "[Step 25920] Loss: 2.4587\n",
            "[Step 25930] Loss: 2.7160\n",
            "[Step 25940] Loss: 2.7339\n",
            "[Step 25950] Loss: 2.7411\n",
            "[Step 25960] Loss: 2.7559\n",
            "[Step 25970] Loss: 2.5979\n",
            "[Step 25980] Loss: 2.6303\n",
            "[Step 25990] Loss: 1.9990\n",
            "[Step 26000] Loss: 2.2473\n",
            "[Step 26010] Loss: 2.2346\n",
            "[Step 26020] Loss: 2.3687\n",
            "[Step 26030] Loss: 2.0321\n",
            "[Step 26040] Loss: 2.7606\n",
            "[Step 26050] Loss: 2.0027\n",
            "[Step 26060] Loss: 2.6939\n",
            "[Step 26070] Loss: 2.6996\n",
            "[Step 26080] Loss: 2.6870\n",
            "[Step 26090] Loss: 2.3962\n",
            "[Step 26100] Loss: 2.4320\n",
            "[Step 26110] Loss: 2.4301\n",
            "[Step 26120] Loss: 2.0288\n",
            "[Step 26130] Loss: 2.9949\n",
            "[Step 26140] Loss: 2.5563\n",
            "[Step 26150] Loss: 2.8221\n",
            "[Step 26160] Loss: 2.2201\n",
            "[Step 26170] Loss: 2.7615\n",
            "[Step 26180] Loss: 2.6617\n",
            "[Step 26190] Loss: 2.9575\n",
            "[Step 26200] Loss: 2.1472\n",
            "[Step 26210] Loss: 2.7768\n",
            "[Step 26220] Loss: 2.5645\n",
            "[Step 26230] Loss: 2.7167\n",
            "[Step 26240] Loss: 2.5701\n",
            "[Step 26250] Loss: 2.2436\n",
            "[Step 26260] Loss: 2.8279\n",
            "[Step 26270] Loss: 2.4504\n",
            "[Step 26280] Loss: 2.5645\n",
            "[Step 26290] Loss: 2.2527\n",
            "[Step 26300] Loss: 2.1667\n",
            "[Step 26310] Loss: 2.1958\n",
            "[Step 26320] Loss: 2.4230\n",
            "[Step 26330] Loss: 2.5000\n",
            "[Step 26340] Loss: 2.7833\n",
            "[Step 26350] Loss: 2.5852\n",
            "[Step 26360] Loss: 2.4314\n",
            "[Step 26370] Loss: 2.4269\n",
            "[Step 26380] Loss: 2.7715\n",
            "[Step 26390] Loss: 2.5751\n",
            "[Step 26400] Loss: 2.4835\n",
            "[Step 26410] Loss: 2.7905\n",
            "[Step 26420] Loss: 3.0607\n",
            "[Step 26430] Loss: 2.3986\n",
            "[Step 26440] Loss: 2.4377\n",
            "[Step 26450] Loss: 2.6879\n",
            "[Step 26460] Loss: 2.6788\n",
            "[Step 26470] Loss: 2.2844\n",
            "[Step 26480] Loss: 2.7262\n",
            "[Step 26490] Loss: 2.6989\n",
            "[Step 26500] Loss: 2.7475\n",
            "[Step 26510] Loss: 3.0717\n",
            "ðŸ“˜ Epoch 54 - Avg Training Loss: 2.5339\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6104 | Accuracy: 0.3572 | Precision: 0.3353\n",
            "[Step 26520] Loss: 2.3846\n",
            "[Step 26530] Loss: 2.4177\n",
            "[Step 26540] Loss: 2.7249\n",
            "[Step 26550] Loss: 2.0371\n",
            "[Step 26560] Loss: 2.4361\n",
            "[Step 26570] Loss: 3.1177\n",
            "[Step 26580] Loss: 2.8741\n",
            "[Step 26590] Loss: 2.3601\n",
            "[Step 26600] Loss: 2.5079\n",
            "[Step 26610] Loss: 2.5930\n",
            "[Step 26620] Loss: 2.3330\n",
            "[Step 26630] Loss: 2.3498\n",
            "[Step 26640] Loss: 2.6908\n",
            "[Step 26650] Loss: 2.0598\n",
            "[Step 26660] Loss: 2.7451\n",
            "[Step 26670] Loss: 2.2660\n",
            "[Step 26680] Loss: 2.4507\n",
            "[Step 26690] Loss: 2.5367\n",
            "[Step 26700] Loss: 2.6831\n",
            "[Step 26710] Loss: 2.4149\n",
            "[Step 26720] Loss: 2.8363\n",
            "[Step 26730] Loss: 2.6455\n",
            "[Step 26740] Loss: 2.5739\n",
            "[Step 26750] Loss: 2.7619\n",
            "[Step 26760] Loss: 2.8150\n",
            "[Step 26770] Loss: 2.7413\n",
            "[Step 26780] Loss: 2.9920\n",
            "[Step 26790] Loss: 2.5422\n",
            "[Step 26800] Loss: 2.1990\n",
            "[Step 26810] Loss: 2.7245\n",
            "[Step 26820] Loss: 2.3894\n",
            "[Step 26830] Loss: 3.1272\n",
            "[Step 26840] Loss: 2.8516\n",
            "[Step 26850] Loss: 2.6790\n",
            "[Step 26860] Loss: 2.9385\n",
            "[Step 26870] Loss: 2.9536\n",
            "[Step 26880] Loss: 2.6373\n",
            "[Step 26890] Loss: 2.7386\n",
            "[Step 26900] Loss: 2.7503\n",
            "[Step 26910] Loss: 2.8206\n",
            "[Step 26920] Loss: 2.7425\n",
            "[Step 26930] Loss: 1.9961\n",
            "[Step 26940] Loss: 2.3896\n",
            "[Step 26950] Loss: 2.4790\n",
            "[Step 26960] Loss: 2.6991\n",
            "[Step 26970] Loss: 2.4791\n",
            "[Step 26980] Loss: 2.3287\n",
            "[Step 26990] Loss: 2.7453\n",
            "[Step 27000] Loss: 2.4491\n",
            "[Step 27010] Loss: 2.7865\n",
            "[Step 27020] Loss: 2.3160\n",
            "[Step 27030] Loss: 2.7026\n",
            "[Step 27040] Loss: 2.5713\n",
            "[Step 27050] Loss: 2.4714\n",
            "[Step 27060] Loss: 2.5583\n",
            "[Step 27070] Loss: 2.6151\n",
            "[Step 27080] Loss: 2.4772\n",
            "[Step 27090] Loss: 2.0132\n",
            "[Step 27100] Loss: 2.5840\n",
            "[Step 27110] Loss: 2.5007\n",
            "[Step 27120] Loss: 2.8037\n",
            "[Step 27130] Loss: 2.7198\n",
            "[Step 27140] Loss: 2.7269\n",
            "[Step 27150] Loss: 2.6180\n",
            "[Step 27160] Loss: 2.4792\n",
            "[Step 27170] Loss: 2.4697\n",
            "[Step 27180] Loss: 2.5282\n",
            "[Step 27190] Loss: 2.2875\n",
            "[Step 27200] Loss: 2.5610\n",
            "[Step 27210] Loss: 2.6126\n",
            "[Step 27220] Loss: 2.8287\n",
            "[Step 27230] Loss: 2.0540\n",
            "[Step 27240] Loss: 2.4143\n",
            "[Step 27250] Loss: 2.5779\n",
            "[Step 27260] Loss: 2.3844\n",
            "[Step 27270] Loss: 2.5915\n",
            "[Step 27280] Loss: 2.7079\n",
            "[Step 27290] Loss: 2.2052\n",
            "[Step 27300] Loss: 2.7279\n",
            "[Step 27310] Loss: 2.9427\n",
            "[Step 27320] Loss: 2.1183\n",
            "[Step 27330] Loss: 2.4571\n",
            "[Step 27340] Loss: 2.0900\n",
            "[Step 27350] Loss: 2.6980\n",
            "[Step 27360] Loss: 2.7409\n",
            "[Step 27370] Loss: 2.3112\n",
            "[Step 27380] Loss: 2.5377\n",
            "[Step 27390] Loss: 2.0877\n",
            "[Step 27400] Loss: 2.1428\n",
            "[Step 27410] Loss: 2.8132\n",
            "[Step 27420] Loss: 2.9042\n",
            "[Step 27430] Loss: 2.1452\n",
            "[Step 27440] Loss: 2.2215\n",
            "[Step 27450] Loss: 2.1961\n",
            "[Step 27460] Loss: 2.6177\n",
            "[Step 27470] Loss: 2.6748\n",
            "[Step 27480] Loss: 2.8759\n",
            "[Step 27490] Loss: 2.5700\n",
            "[Step 27500] Loss: 2.4592\n",
            "[Step 27510] Loss: 2.3895\n",
            "[Step 27520] Loss: 2.6889\n",
            "[Step 27530] Loss: 2.3040\n",
            "[Step 27540] Loss: 2.0385\n",
            "[Step 27550] Loss: 2.6355\n",
            "[Step 27560] Loss: 2.8724\n",
            "[Step 27570] Loss: 3.0054\n",
            "[Step 27580] Loss: 2.4598\n",
            "[Step 27590] Loss: 2.8313\n",
            "[Step 27600] Loss: 2.3632\n",
            "[Step 27610] Loss: 2.2701\n",
            "[Step 27620] Loss: 2.1833\n",
            "[Step 27630] Loss: 2.5149\n",
            "[Step 27640] Loss: 2.3079\n",
            "[Step 27650] Loss: 2.6189\n",
            "[Step 27660] Loss: 2.2186\n",
            "[Step 27670] Loss: 2.0361\n",
            "[Step 27680] Loss: 2.9771\n",
            "[Step 27690] Loss: 2.5894\n",
            "[Step 27700] Loss: 2.7237\n",
            "[Step 27710] Loss: 3.0120\n",
            "[Step 27720] Loss: 2.3033\n",
            "[Step 27730] Loss: 2.2426\n",
            "[Step 27740] Loss: 2.3074\n",
            "[Step 27750] Loss: 2.6914\n",
            "[Step 27760] Loss: 2.5006\n",
            "[Step 27770] Loss: 2.3783\n",
            "[Step 27780] Loss: 2.4718\n",
            "[Step 27790] Loss: 2.5660\n",
            "[Step 27800] Loss: 2.5305\n",
            "[Step 27810] Loss: 2.7813\n",
            "[Step 27820] Loss: 2.4021\n",
            "[Step 27830] Loss: 2.2514\n",
            "[Step 27840] Loss: 1.9984\n",
            "[Step 27850] Loss: 3.0086\n",
            "[Step 27860] Loss: 1.8858\n",
            "[Step 27870] Loss: 2.4637\n",
            "[Step 27880] Loss: 2.2696\n",
            "[Step 27890] Loss: 2.3402\n",
            "[Step 27900] Loss: 2.5788\n",
            "[Step 27910] Loss: 2.5552\n",
            "[Step 27920] Loss: 2.2188\n",
            "[Step 27930] Loss: 2.5829\n",
            "[Step 27940] Loss: 2.6218\n",
            "[Step 27950] Loss: 2.6390\n",
            "[Step 27960] Loss: 2.1659\n",
            "[Step 27970] Loss: 2.7599\n",
            "[Step 27980] Loss: 2.3961\n",
            "[Step 27990] Loss: 2.2526\n",
            "[Step 28000] Loss: 2.3319\n",
            "[Step 28010] Loss: 2.6361\n",
            "[Step 28020] Loss: 2.5357\n",
            "[Step 28030] Loss: 2.3951\n",
            "[Step 28040] Loss: 2.7566\n",
            "[Step 28050] Loss: 2.3005\n",
            "[Step 28060] Loss: 2.9257\n",
            "[Step 28070] Loss: 2.6483\n",
            "[Step 28080] Loss: 2.6648\n",
            "[Step 28090] Loss: 2.9065\n",
            "[Step 28100] Loss: 2.3495\n",
            "[Step 28110] Loss: 2.6836\n",
            "[Step 28120] Loss: 2.4372\n",
            "[Step 28130] Loss: 2.4804\n",
            "[Step 28140] Loss: 2.5597\n",
            "[Step 28150] Loss: 2.4501\n",
            "[Step 28160] Loss: 2.7587\n",
            "[Step 28170] Loss: 3.1434\n",
            "[Step 28180] Loss: 2.3263\n",
            "[Step 28190] Loss: 2.4583\n",
            "[Step 28200] Loss: 2.3838\n",
            "[Step 28210] Loss: 2.4753\n",
            "[Step 28220] Loss: 2.4695\n",
            "[Step 28230] Loss: 2.0727\n",
            "[Step 28240] Loss: 2.6058\n",
            "[Step 28250] Loss: 2.2306\n",
            "[Step 28260] Loss: 2.5319\n",
            "[Step 28270] Loss: 2.6282\n",
            "[Step 28280] Loss: 2.4281\n",
            "[Step 28290] Loss: 2.5145\n",
            "[Step 28300] Loss: 2.6046\n",
            "[Step 28310] Loss: 2.6785\n",
            "[Step 28320] Loss: 2.3835\n",
            "[Step 28330] Loss: 2.6994\n",
            "[Step 28340] Loss: 2.2059\n",
            "[Step 28350] Loss: 2.6632\n",
            "[Step 28360] Loss: 2.6337\n",
            "[Step 28370] Loss: 2.6185\n",
            "[Step 28380] Loss: 2.0321\n",
            "[Step 28390] Loss: 2.6651\n",
            "[Step 28400] Loss: 2.4582\n",
            "[Step 28410] Loss: 2.4714\n",
            "ðŸ“˜ Epoch 55 - Avg Training Loss: 2.5416\n",
            "ðŸ“Š Final Validation â€” Loss: 2.6078 | Accuracy: 0.3543 | Precision: 0.3341\n",
            "âœ… Continued training complete\n"
          ]
        }
      ],
      "source": [
        "new_training_args = {\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 0.005,\n",
        "    \"num_additional_epochs\": 15,\n",
        "    \"logging_steps\": 10,\n",
        "}\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=new_training_args[\"learning_rate\"],\n",
        "    weight_decay=new_training_args[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "starting_epoch = 40\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(starting_epoch, starting_epoch + new_training_args[\"num_additional_epochs\"]):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % new_training_args[\"logging_steps\"] == 0:\n",
        "            print(f\"[Step {global_step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"ðŸ“˜ Epoch {epoch+1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    evaluate_val(model, val_loader, criterion, device)\n",
        "\n",
        "print(\"âœ… Continued training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhZa5G5hnL4T",
        "outputId": "de0d0465-7b79-4fdf-d83b-23754bbef0f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Saved training checkpoint to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "save_path = '/content/drive/My Drive/NexHack/model_weights_2.pth'\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=True)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model_weights_1 = MyResNet50()\n",
        "\n",
        "torch.save(model_weights_1.state_dict(), 'model_weights_2.pth')\n",
        "torch.save(model_weights_1.state_dict(), save_path)\n",
        "\n",
        "save_path = '/content/drive/My Drive/NexHack/model_full_2.pth'\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=True)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model_full_1 = MyResNet50()\n",
        "\n",
        "torch.save(model_full_1, 'model_full_2.pth')\n",
        "torch.save(model_full_1.state_dict(), save_path)\n",
        "\n",
        "save_path = '/content/drive/My Drive/NexHack/checkpoint_2.pth'\n",
        "\n",
        "\n",
        "checkpoint = {\n",
        "    'epoch': 40,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'training_args': training_args\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, save_path)\n",
        "\n",
        "print(\"âœ… Saved training checkpoint to Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v2Q_P7YlnU6x",
        "outputId": "65a5a688-5e88-4fc4-d9ac-83fdf089ac73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 10] Loss: 3.0896\n",
            "[Step 20] Loss: 3.1614\n",
            "[Step 30] Loss: 2.5643\n",
            "[Step 40] Loss: 3.2348\n",
            "[Step 50] Loss: 2.9746\n",
            "[Step 60] Loss: 3.0579\n",
            "[Step 70] Loss: 2.7880\n",
            "[Step 80] Loss: 3.5696\n",
            "[Step 90] Loss: 2.4390\n",
            "[Step 100] Loss: 2.9733\n",
            "[Step 110] Loss: 3.6060\n",
            "[Step 120] Loss: 2.9335\n",
            "[Step 130] Loss: 3.4842\n",
            "[Step 140] Loss: 2.8087\n",
            "[Step 150] Loss: 3.7450\n",
            "[Step 160] Loss: 2.9844\n",
            "[Step 170] Loss: 2.9476\n",
            "[Step 180] Loss: 2.6357\n",
            "[Step 190] Loss: 3.1413\n",
            "[Step 200] Loss: 3.1753\n",
            "[Step 210] Loss: 2.8750\n",
            "[Step 220] Loss: 2.8709\n",
            "[Step 230] Loss: 2.6737\n",
            "[Step 240] Loss: 2.9442\n",
            "[Step 250] Loss: 3.3000\n",
            "[Step 260] Loss: 2.8154\n",
            "[Step 270] Loss: 3.4085\n",
            "[Step 280] Loss: 2.9416\n",
            "[Step 290] Loss: 2.6335\n",
            "[Step 300] Loss: 3.0595\n",
            "[Step 310] Loss: 2.5250\n",
            "[Step 320] Loss: 2.9407\n",
            "[Step 330] Loss: 3.1740\n",
            "[Step 340] Loss: 3.1796\n",
            "[Step 350] Loss: 3.1487\n",
            "[Step 360] Loss: 3.4645\n",
            "[Step 370] Loss: 2.9010\n",
            "[Step 380] Loss: 2.6087\n",
            "[Step 390] Loss: 3.0878\n",
            "[Step 400] Loss: 2.7550\n",
            "[Step 410] Loss: 2.9412\n",
            "[Step 420] Loss: 2.7641\n",
            "[Step 430] Loss: 2.8554\n",
            "[Step 440] Loss: 2.3956\n",
            "[Step 450] Loss: 2.8360\n",
            "[Step 460] Loss: 2.9711\n",
            "[Step 470] Loss: 3.2385\n",
            "[Step 480] Loss: 3.4354\n",
            "[Step 490] Loss: 3.2728\n",
            "[Step 500] Loss: 3.0486\n",
            "[Step 510] Loss: 3.0766\n",
            "[Step 520] Loss: 3.0053\n",
            "[Step 530] Loss: 3.0531\n",
            "[Step 540] Loss: 2.6518\n",
            "[Step 550] Loss: 2.9794\n",
            "[Step 560] Loss: 2.9663\n",
            "[Step 570] Loss: 3.3826\n",
            "[Step 580] Loss: 2.7984\n",
            "[Step 590] Loss: 2.7277\n",
            "[Step 600] Loss: 3.3676\n",
            "[Step 610] Loss: 3.2453\n",
            "[Step 620] Loss: 2.5850\n",
            "[Step 630] Loss: 3.0571\n",
            "[Step 640] Loss: 2.8809\n",
            "[Step 650] Loss: 3.3760\n",
            "[Step 660] Loss: 2.8976\n",
            "[Step 670] Loss: 3.0596\n",
            "[Step 680] Loss: 2.8598\n",
            "[Step 690] Loss: 2.8897\n",
            "[Step 700] Loss: 2.8246\n",
            "[Step 710] Loss: 2.8145\n",
            "[Step 720] Loss: 3.0603\n",
            "[Step 730] Loss: 3.4092\n",
            "[Step 740] Loss: 2.6985\n",
            "[Step 750] Loss: 2.3237\n",
            "[Step 760] Loss: 2.7046\n",
            "[Step 770] Loss: 2.8756\n",
            "[Step 780] Loss: 2.6016\n",
            "[Step 790] Loss: 3.0990\n",
            "[Step 800] Loss: 2.8914\n",
            "[Step 810] Loss: 3.2572\n",
            "[Step 820] Loss: 2.9645\n",
            "[Step 830] Loss: 3.2680\n",
            "[Step 840] Loss: 2.8065\n",
            "[Step 850] Loss: 3.2677\n",
            "[Step 860] Loss: 2.5870\n",
            "[Step 870] Loss: 2.9043\n",
            "[Step 880] Loss: 3.1392\n",
            "[Step 890] Loss: 3.0874\n",
            "[Step 900] Loss: 2.9904\n",
            "[Step 910] Loss: 2.9088\n",
            "[Step 920] Loss: 2.8445\n",
            "[Step 930] Loss: 2.9389\n",
            "[Step 940] Loss: 3.1232\n",
            "[Step 950] Loss: 3.0673\n",
            "[Step 960] Loss: 2.6712\n",
            "[Step 970] Loss: 2.8203\n",
            "[Step 980] Loss: 2.4738\n",
            "[Step 990] Loss: 3.0042\n",
            "[Step 1000] Loss: 3.3630\n",
            "[Step 1010] Loss: 2.9446\n",
            "[Step 1020] Loss: 3.0837\n",
            "[Step 1030] Loss: 2.6203\n",
            "[Step 1040] Loss: 2.9754\n",
            "[Step 1050] Loss: 3.1542\n",
            "[Step 1060] Loss: 3.1606\n",
            "[Step 1070] Loss: 3.0848\n",
            "[Step 1080] Loss: 2.7986\n",
            "[Step 1090] Loss: 3.2824\n",
            "[Step 1100] Loss: 3.0291\n",
            "[Step 1110] Loss: 2.5573\n",
            "[Step 1120] Loss: 2.5801\n",
            "[Step 1130] Loss: 2.6795\n",
            "[Step 1140] Loss: 3.1141\n",
            "[Step 1150] Loss: 2.9235\n",
            "[Step 1160] Loss: 2.9640\n",
            "[Step 1170] Loss: 2.6254\n",
            "[Step 1180] Loss: 2.6359\n",
            "[Step 1190] Loss: 2.8529\n",
            "[Step 1200] Loss: 2.8908\n",
            "[Step 1210] Loss: 3.2059\n",
            "[Step 1220] Loss: 3.4759\n",
            "[Step 1230] Loss: 2.8882\n",
            "[Step 1240] Loss: 2.7434\n",
            "[Step 1250] Loss: 3.4423\n",
            "[Step 1260] Loss: 3.4913\n",
            "[Step 1270] Loss: 3.1978\n",
            "[Step 1280] Loss: 2.7697\n",
            "[Step 1290] Loss: 2.9749\n",
            "[Step 1300] Loss: 3.3332\n",
            "[Step 1310] Loss: 3.3130\n",
            "[Step 1320] Loss: 3.0538\n",
            "[Step 1330] Loss: 3.0803\n",
            "[Step 1340] Loss: 2.7284\n",
            "[Step 1350] Loss: 3.2085\n",
            "[Step 1360] Loss: 2.9755\n",
            "[Step 1370] Loss: 2.9212\n",
            "[Step 1380] Loss: 2.9463\n",
            "[Step 1390] Loss: 3.1758\n",
            "[Step 1400] Loss: 2.8212\n",
            "[Step 1410] Loss: 3.3600\n",
            "[Step 1420] Loss: 3.0323\n",
            "[Step 1430] Loss: 3.2334\n",
            "[Step 1440] Loss: 3.1303\n",
            "[Step 1450] Loss: 3.3876\n",
            "[Step 1460] Loss: 2.7670\n",
            "[Step 1470] Loss: 2.7464\n",
            "[Step 1480] Loss: 2.7454\n",
            "[Step 1490] Loss: 3.3835\n",
            "[Step 1500] Loss: 2.8292\n",
            "[Step 1510] Loss: 2.8873\n",
            "[Step 1520] Loss: 2.6019\n",
            "[Step 1530] Loss: 3.1973\n",
            "[Step 1540] Loss: 2.8781\n",
            "[Step 1550] Loss: 2.7876\n",
            "[Step 1560] Loss: 2.9529\n",
            "[Step 1570] Loss: 2.6944\n",
            "[Step 1580] Loss: 2.8350\n",
            "[Step 1590] Loss: 2.8390\n",
            "[Step 1600] Loss: 2.8968\n",
            "[Step 1610] Loss: 3.1650\n",
            "[Step 1620] Loss: 2.4250\n",
            "[Step 1630] Loss: 2.7490\n",
            "[Step 1640] Loss: 3.1814\n",
            "[Step 1650] Loss: 3.1030\n",
            "[Step 1660] Loss: 3.1199\n",
            "[Step 1670] Loss: 3.3594\n",
            "[Step 1680] Loss: 3.1551\n",
            "[Step 1690] Loss: 2.7454\n",
            "[Step 1700] Loss: 3.1020\n",
            "[Step 1710] Loss: 3.0438\n",
            "[Step 1720] Loss: 3.0969\n",
            "[Step 1730] Loss: 2.7805\n",
            "[Step 1740] Loss: 3.5752\n",
            "[Step 1750] Loss: 2.6713\n",
            "[Step 1760] Loss: 2.5951\n",
            "[Step 1770] Loss: 2.8579\n",
            "[Step 1780] Loss: 2.7720\n",
            "[Step 1790] Loss: 2.9981\n",
            "[Step 1800] Loss: 2.7748\n",
            "[Step 1810] Loss: 3.1806\n",
            "[Step 1820] Loss: 2.9978\n",
            "[Step 1830] Loss: 3.2151\n",
            "[Step 1840] Loss: 3.4909\n",
            "[Step 1850] Loss: 2.6499\n",
            "[Step 1860] Loss: 2.7555\n",
            "[Step 1870] Loss: 2.7158\n",
            "[Step 1880] Loss: 3.2126\n",
            "[Step 1890] Loss: 2.6675\n",
            "ðŸ“˜ Epoch 56 - Avg Training Loss: 2.9781\n",
            "ðŸ“Š Final Validation â€” Loss: 3.2926 | Accuracy: 0.2115 | Precision: 0.2549\n",
            "[Step 1900] Loss: 2.9559\n",
            "[Step 1910] Loss: 2.9639\n",
            "[Step 1920] Loss: 3.0460\n",
            "[Step 1930] Loss: 2.9476\n",
            "[Step 1940] Loss: 2.7388\n",
            "[Step 1950] Loss: 2.4240\n",
            "[Step 1960] Loss: 2.7234\n",
            "[Step 1970] Loss: 3.0867\n",
            "[Step 1980] Loss: 3.4272\n",
            "[Step 1990] Loss: 3.0912\n",
            "[Step 2000] Loss: 3.2080\n",
            "[Step 2010] Loss: 2.7435\n",
            "[Step 2020] Loss: 3.0921\n",
            "[Step 2030] Loss: 2.7507\n",
            "[Step 2040] Loss: 3.0847\n",
            "[Step 2050] Loss: 2.9849\n",
            "[Step 2060] Loss: 3.2260\n",
            "[Step 2070] Loss: 2.5000\n",
            "[Step 2080] Loss: 2.7973\n",
            "[Step 2090] Loss: 2.8975\n",
            "[Step 2100] Loss: 2.4988\n",
            "[Step 2110] Loss: 2.9597\n",
            "[Step 2120] Loss: 3.2574\n",
            "[Step 2130] Loss: 3.2590\n",
            "[Step 2140] Loss: 3.3041\n",
            "[Step 2150] Loss: 2.9204\n",
            "[Step 2160] Loss: 2.9096\n",
            "[Step 2170] Loss: 2.9014\n",
            "[Step 2180] Loss: 2.6406\n",
            "[Step 2190] Loss: 3.1026\n",
            "[Step 2200] Loss: 2.9756\n",
            "[Step 2210] Loss: 3.0338\n",
            "[Step 2220] Loss: 3.1849\n",
            "[Step 2230] Loss: 3.0072\n",
            "[Step 2240] Loss: 3.1756\n",
            "[Step 2250] Loss: 3.0957\n",
            "[Step 2260] Loss: 2.9319\n",
            "[Step 2270] Loss: 2.7376\n",
            "[Step 2280] Loss: 2.7102\n",
            "[Step 2290] Loss: 2.8881\n",
            "[Step 2300] Loss: 3.2688\n",
            "[Step 2310] Loss: 2.7384\n",
            "[Step 2320] Loss: 2.8594\n",
            "[Step 2330] Loss: 3.2781\n",
            "[Step 2340] Loss: 2.7634\n",
            "[Step 2350] Loss: 3.0622\n",
            "[Step 2360] Loss: 2.8428\n",
            "[Step 2370] Loss: 2.4795\n",
            "[Step 2380] Loss: 2.8983\n",
            "[Step 2390] Loss: 3.3388\n",
            "[Step 2400] Loss: 3.2841\n",
            "[Step 2410] Loss: 3.0575\n",
            "[Step 2420] Loss: 2.4868\n",
            "[Step 2430] Loss: 3.0653\n",
            "[Step 2440] Loss: 3.1014\n",
            "[Step 2450] Loss: 2.8656\n",
            "[Step 2460] Loss: 2.7194\n",
            "[Step 2470] Loss: 2.8928\n",
            "[Step 2480] Loss: 3.0729\n",
            "[Step 2490] Loss: 3.0699\n",
            "[Step 2500] Loss: 2.6239\n",
            "[Step 2510] Loss: 2.8572\n",
            "[Step 2520] Loss: 3.2582\n",
            "[Step 2530] Loss: 2.6632\n",
            "[Step 2540] Loss: 2.5606\n",
            "[Step 2550] Loss: 3.1256\n",
            "[Step 2560] Loss: 2.8995\n",
            "[Step 2570] Loss: 3.5564\n",
            "[Step 2580] Loss: 3.0446\n",
            "[Step 2590] Loss: 3.2963\n",
            "[Step 2600] Loss: 3.3166\n",
            "[Step 2610] Loss: 3.2927\n",
            "[Step 2620] Loss: 3.1244\n",
            "[Step 2630] Loss: 3.2920\n",
            "[Step 2640] Loss: 3.3340\n",
            "[Step 2650] Loss: 2.8534\n",
            "[Step 2660] Loss: 3.1679\n",
            "[Step 2670] Loss: 3.0255\n",
            "[Step 2680] Loss: 2.9395\n",
            "[Step 2690] Loss: 2.6850\n",
            "[Step 2700] Loss: 2.8563\n",
            "[Step 2710] Loss: 3.1071\n",
            "[Step 2720] Loss: 3.1664\n",
            "[Step 2730] Loss: 3.0878\n",
            "[Step 2740] Loss: 2.9562\n",
            "[Step 2750] Loss: 3.3370\n",
            "[Step 2760] Loss: 3.1036\n",
            "[Step 2770] Loss: 2.7030\n",
            "[Step 2780] Loss: 3.0506\n",
            "[Step 2790] Loss: 2.5344\n",
            "[Step 2800] Loss: 2.9662\n",
            "[Step 2810] Loss: 3.0104\n",
            "[Step 2820] Loss: 3.2142\n",
            "[Step 2830] Loss: 3.4934\n",
            "[Step 2840] Loss: 2.8261\n",
            "[Step 2850] Loss: 3.3651\n",
            "[Step 2860] Loss: 3.2052\n",
            "[Step 2870] Loss: 2.8797\n",
            "[Step 2880] Loss: 2.9926\n",
            "[Step 2890] Loss: 3.0614\n",
            "[Step 2900] Loss: 2.7998\n",
            "[Step 2910] Loss: 3.4653\n",
            "[Step 2920] Loss: 2.7797\n",
            "[Step 2930] Loss: 2.4855\n",
            "[Step 2940] Loss: 2.8331\n",
            "[Step 2950] Loss: 3.0395\n",
            "[Step 2960] Loss: 3.1245\n",
            "[Step 2970] Loss: 3.2645\n",
            "[Step 2980] Loss: 2.2504\n",
            "[Step 2990] Loss: 3.2633\n",
            "[Step 3000] Loss: 2.9715\n",
            "[Step 3010] Loss: 2.9243\n",
            "[Step 3020] Loss: 2.6220\n",
            "[Step 3030] Loss: 2.7056\n",
            "[Step 3040] Loss: 3.0980\n",
            "[Step 3050] Loss: 3.2572\n",
            "[Step 3060] Loss: 3.1627\n",
            "[Step 3070] Loss: 3.0699\n",
            "[Step 3080] Loss: 2.8200\n",
            "[Step 3090] Loss: 2.9708\n",
            "[Step 3100] Loss: 2.9067\n",
            "[Step 3110] Loss: 2.6729\n",
            "[Step 3120] Loss: 3.4270\n",
            "[Step 3130] Loss: 2.9400\n",
            "[Step 3140] Loss: 3.1390\n",
            "[Step 3150] Loss: 2.6759\n",
            "[Step 3160] Loss: 2.6576\n",
            "[Step 3170] Loss: 2.6524\n",
            "[Step 3180] Loss: 3.0971\n",
            "[Step 3190] Loss: 2.9802\n",
            "[Step 3200] Loss: 3.1011\n",
            "[Step 3210] Loss: 2.8428\n",
            "[Step 3220] Loss: 2.6148\n",
            "[Step 3230] Loss: 3.0832\n",
            "[Step 3240] Loss: 2.6885\n",
            "[Step 3250] Loss: 2.8697\n",
            "[Step 3260] Loss: 3.0130\n",
            "[Step 3270] Loss: 3.0063\n",
            "[Step 3280] Loss: 3.1604\n",
            "[Step 3290] Loss: 2.8377\n",
            "[Step 3300] Loss: 3.0654\n",
            "[Step 3310] Loss: 3.0743\n",
            "[Step 3320] Loss: 2.9701\n",
            "[Step 3330] Loss: 3.5323\n",
            "[Step 3340] Loss: 2.9014\n",
            "[Step 3350] Loss: 2.9002\n",
            "[Step 3360] Loss: 3.2071\n",
            "[Step 3370] Loss: 2.5483\n",
            "[Step 3380] Loss: 3.3082\n",
            "[Step 3390] Loss: 2.9468\n",
            "[Step 3400] Loss: 3.5057\n",
            "[Step 3410] Loss: 2.6714\n",
            "[Step 3420] Loss: 2.6114\n",
            "[Step 3430] Loss: 2.8601\n",
            "[Step 3440] Loss: 3.0630\n",
            "[Step 3450] Loss: 2.6994\n",
            "[Step 3460] Loss: 3.0533\n",
            "[Step 3470] Loss: 3.0027\n",
            "[Step 3480] Loss: 3.3256\n",
            "[Step 3490] Loss: 2.9058\n",
            "[Step 3500] Loss: 3.4818\n",
            "[Step 3510] Loss: 2.5100\n",
            "[Step 3520] Loss: 2.8733\n",
            "[Step 3530] Loss: 2.6478\n",
            "[Step 3540] Loss: 2.8678\n",
            "[Step 3550] Loss: 2.9555\n",
            "[Step 3560] Loss: 3.0560\n",
            "[Step 3570] Loss: 2.8614\n",
            "[Step 3580] Loss: 3.0852\n",
            "[Step 3590] Loss: 2.8720\n",
            "[Step 3600] Loss: 3.1151\n",
            "[Step 3610] Loss: 2.8983\n",
            "[Step 3620] Loss: 3.6236\n",
            "[Step 3630] Loss: 3.0670\n",
            "[Step 3640] Loss: 3.2357\n",
            "[Step 3650] Loss: 3.3270\n",
            "[Step 3660] Loss: 3.0989\n",
            "[Step 3670] Loss: 2.3989\n",
            "[Step 3680] Loss: 3.5772\n",
            "[Step 3690] Loss: 2.9247\n",
            "[Step 3700] Loss: 2.5788\n",
            "[Step 3710] Loss: 2.7722\n",
            "[Step 3720] Loss: 3.1249\n",
            "[Step 3730] Loss: 2.9794\n",
            "[Step 3740] Loss: 2.6679\n",
            "[Step 3750] Loss: 2.6668\n",
            "[Step 3760] Loss: 3.6394\n",
            "[Step 3770] Loss: 2.5216\n",
            "[Step 3780] Loss: 2.9628\n",
            "ðŸ“˜ Epoch 57 - Avg Training Loss: 2.9903\n",
            "ðŸ“Š Final Validation â€” Loss: 3.2854 | Accuracy: 0.2176 | Precision: 0.2776\n",
            "[Step 3790] Loss: 2.5953\n",
            "[Step 3800] Loss: 2.8214\n",
            "[Step 3810] Loss: 2.9145\n",
            "[Step 3820] Loss: 2.8118\n",
            "[Step 3830] Loss: 3.0800\n",
            "[Step 3840] Loss: 2.7744\n",
            "[Step 3850] Loss: 3.0237\n",
            "[Step 3860] Loss: 2.9864\n",
            "[Step 3870] Loss: 3.1413\n",
            "[Step 3880] Loss: 2.6204\n",
            "[Step 3890] Loss: 3.0524\n",
            "[Step 3900] Loss: 2.7063\n",
            "[Step 3910] Loss: 3.4177\n",
            "[Step 3920] Loss: 2.6709\n",
            "[Step 3930] Loss: 2.6930\n",
            "[Step 3940] Loss: 2.7802\n",
            "[Step 3950] Loss: 3.1234\n",
            "[Step 3960] Loss: 2.8539\n",
            "[Step 3970] Loss: 3.3580\n",
            "[Step 3980] Loss: 3.0688\n",
            "[Step 3990] Loss: 3.0269\n",
            "[Step 4000] Loss: 2.7854\n",
            "[Step 4010] Loss: 2.6972\n",
            "[Step 4020] Loss: 2.5795\n",
            "[Step 4030] Loss: 3.6031\n",
            "[Step 4040] Loss: 2.9793\n",
            "[Step 4050] Loss: 2.8586\n",
            "[Step 4060] Loss: 2.9721\n",
            "[Step 4070] Loss: 3.1415\n",
            "[Step 4080] Loss: 3.3514\n",
            "[Step 4090] Loss: 3.1147\n",
            "[Step 4100] Loss: 3.1335\n",
            "[Step 4110] Loss: 2.7229\n",
            "[Step 4120] Loss: 3.1570\n",
            "[Step 4130] Loss: 2.7880\n",
            "[Step 4140] Loss: 3.0837\n",
            "[Step 4150] Loss: 3.1278\n",
            "[Step 4160] Loss: 3.0642\n",
            "[Step 4170] Loss: 3.3724\n",
            "[Step 4180] Loss: 2.7189\n",
            "[Step 4190] Loss: 2.7955\n",
            "[Step 4200] Loss: 2.8503\n",
            "[Step 4210] Loss: 3.2485\n",
            "[Step 4220] Loss: 3.1748\n",
            "[Step 4230] Loss: 3.2362\n",
            "[Step 4240] Loss: 3.3008\n",
            "[Step 4250] Loss: 2.8011\n",
            "[Step 4260] Loss: 2.9373\n",
            "[Step 4270] Loss: 3.2882\n",
            "[Step 4280] Loss: 2.9347\n",
            "[Step 4290] Loss: 3.5026\n",
            "[Step 4300] Loss: 2.7857\n",
            "[Step 4310] Loss: 3.0902\n",
            "[Step 4320] Loss: 3.3254\n",
            "[Step 4330] Loss: 3.4842\n",
            "[Step 4340] Loss: 3.3414\n",
            "[Step 4350] Loss: 2.4798\n",
            "[Step 4360] Loss: 2.9569\n",
            "[Step 4370] Loss: 2.9535\n",
            "[Step 4380] Loss: 3.0837\n",
            "[Step 4390] Loss: 2.7035\n",
            "[Step 4400] Loss: 3.2555\n",
            "[Step 4410] Loss: 2.8950\n",
            "[Step 4420] Loss: 3.1755\n",
            "[Step 4430] Loss: 3.2038\n",
            "[Step 4440] Loss: 3.1256\n",
            "[Step 4450] Loss: 2.9113\n",
            "[Step 4460] Loss: 2.3934\n",
            "[Step 4470] Loss: 2.9630\n",
            "[Step 4480] Loss: 2.9606\n",
            "[Step 4490] Loss: 3.1102\n",
            "[Step 4500] Loss: 2.4777\n",
            "[Step 4510] Loss: 2.8635\n",
            "[Step 4520] Loss: 2.8397\n",
            "[Step 4530] Loss: 2.9488\n",
            "[Step 4540] Loss: 2.6346\n",
            "[Step 4550] Loss: 2.5878\n",
            "[Step 4560] Loss: 3.1092\n",
            "[Step 4570] Loss: 2.8594\n",
            "[Step 4580] Loss: 2.8940\n",
            "[Step 4590] Loss: 3.0135\n",
            "[Step 4600] Loss: 3.1923\n",
            "[Step 4610] Loss: 2.8743\n",
            "[Step 4620] Loss: 2.8442\n",
            "[Step 4630] Loss: 2.9800\n",
            "[Step 4640] Loss: 2.9387\n",
            "[Step 4650] Loss: 3.0507\n",
            "[Step 4660] Loss: 2.9067\n",
            "[Step 4670] Loss: 3.5348\n",
            "[Step 4680] Loss: 2.9573\n",
            "[Step 4690] Loss: 2.9930\n",
            "[Step 4700] Loss: 3.0946\n",
            "[Step 4710] Loss: 3.3062\n",
            "[Step 4720] Loss: 2.8490\n",
            "[Step 4730] Loss: 2.4716\n",
            "[Step 4740] Loss: 2.9823\n",
            "[Step 4750] Loss: 2.6424\n",
            "[Step 4760] Loss: 2.5781\n",
            "[Step 4770] Loss: 2.7715\n",
            "[Step 4780] Loss: 3.2344\n",
            "[Step 4790] Loss: 2.8564\n",
            "[Step 4800] Loss: 3.2137\n",
            "[Step 4810] Loss: 2.7456\n",
            "[Step 4820] Loss: 2.7477\n",
            "[Step 4830] Loss: 3.1705\n",
            "[Step 4840] Loss: 2.8139\n",
            "[Step 4850] Loss: 2.8912\n",
            "[Step 4860] Loss: 2.8870\n",
            "[Step 4870] Loss: 2.6651\n",
            "[Step 4880] Loss: 3.0591\n",
            "[Step 4890] Loss: 3.2139\n",
            "[Step 4900] Loss: 2.6709\n",
            "[Step 4910] Loss: 2.7947\n",
            "[Step 4920] Loss: 2.9605\n",
            "[Step 4930] Loss: 2.8451\n",
            "[Step 4940] Loss: 2.7580\n",
            "[Step 4950] Loss: 2.7259\n",
            "[Step 4960] Loss: 2.9210\n",
            "[Step 4970] Loss: 3.2309\n",
            "[Step 4980] Loss: 3.0311\n",
            "[Step 4990] Loss: 3.0394\n",
            "[Step 5000] Loss: 2.9574\n",
            "[Step 5010] Loss: 2.6275\n",
            "[Step 5020] Loss: 3.0384\n",
            "[Step 5030] Loss: 3.3606\n",
            "[Step 5040] Loss: 3.3737\n",
            "[Step 5050] Loss: 2.6843\n",
            "[Step 5060] Loss: 3.3353\n",
            "[Step 5070] Loss: 3.0640\n",
            "[Step 5080] Loss: 2.9782\n",
            "[Step 5090] Loss: 2.9364\n",
            "[Step 5100] Loss: 2.4016\n",
            "[Step 5110] Loss: 3.3018\n",
            "[Step 5120] Loss: 2.9444\n",
            "[Step 5130] Loss: 3.1167\n",
            "[Step 5140] Loss: 2.6041\n",
            "[Step 5150] Loss: 2.9054\n",
            "[Step 5160] Loss: 2.5546\n",
            "[Step 5170] Loss: 2.7639\n",
            "[Step 5180] Loss: 2.9410\n",
            "[Step 5190] Loss: 3.4841\n",
            "[Step 5200] Loss: 2.8258\n",
            "[Step 5210] Loss: 2.4880\n",
            "[Step 5220] Loss: 3.0164\n",
            "[Step 5230] Loss: 3.0576\n",
            "[Step 5240] Loss: 3.1323\n",
            "[Step 5250] Loss: 2.6659\n",
            "[Step 5260] Loss: 2.8756\n",
            "[Step 5270] Loss: 3.2047\n",
            "[Step 5280] Loss: 3.0999\n",
            "[Step 5290] Loss: 2.7757\n",
            "[Step 5300] Loss: 2.3739\n",
            "[Step 5310] Loss: 3.0114\n",
            "[Step 5320] Loss: 3.1031\n",
            "[Step 5330] Loss: 2.7647\n",
            "[Step 5340] Loss: 2.5231\n",
            "[Step 5350] Loss: 3.5490\n",
            "[Step 5360] Loss: 3.4226\n",
            "[Step 5370] Loss: 3.2450\n",
            "[Step 5380] Loss: 3.3922\n",
            "[Step 5390] Loss: 2.6764\n",
            "[Step 5400] Loss: 2.3588\n",
            "[Step 5410] Loss: 3.1723\n",
            "[Step 5420] Loss: 2.3619\n",
            "[Step 5430] Loss: 2.8193\n",
            "[Step 5440] Loss: 2.6668\n",
            "[Step 5450] Loss: 2.8432\n",
            "[Step 5460] Loss: 2.9363\n",
            "[Step 5470] Loss: 3.2476\n",
            "[Step 5480] Loss: 2.7541\n",
            "[Step 5490] Loss: 3.2861\n",
            "[Step 5500] Loss: 3.0837\n",
            "[Step 5510] Loss: 2.9362\n",
            "[Step 5520] Loss: 2.8517\n",
            "[Step 5530] Loss: 2.7190\n",
            "[Step 5540] Loss: 3.1036\n",
            "[Step 5550] Loss: 3.0430\n",
            "[Step 5560] Loss: 2.9381\n",
            "[Step 5570] Loss: 2.7030\n",
            "[Step 5580] Loss: 2.6368\n",
            "[Step 5590] Loss: 2.6749\n",
            "[Step 5600] Loss: 3.1443\n",
            "[Step 5610] Loss: 3.3916\n",
            "[Step 5620] Loss: 2.8924\n",
            "[Step 5630] Loss: 3.1793\n",
            "[Step 5640] Loss: 2.6465\n",
            "[Step 5650] Loss: 2.7723\n",
            "[Step 5660] Loss: 2.5255\n",
            "[Step 5670] Loss: 3.4575\n",
            "[Step 5680] Loss: 3.1208\n",
            "ðŸ“˜ Epoch 58 - Avg Training Loss: 2.9634\n",
            "ðŸ“Š Final Validation â€” Loss: 3.1219 | Accuracy: 0.2358 | Precision: 0.2696\n",
            "[Step 5690] Loss: 2.5051\n",
            "[Step 5700] Loss: 2.5297\n",
            "[Step 5710] Loss: 3.0863\n",
            "[Step 5720] Loss: 2.6395\n",
            "[Step 5730] Loss: 2.4559\n",
            "[Step 5740] Loss: 3.1249\n",
            "[Step 5750] Loss: 3.0412\n",
            "[Step 5760] Loss: 3.2077\n",
            "[Step 5770] Loss: 2.7502\n",
            "[Step 5780] Loss: 2.2058\n",
            "[Step 5790] Loss: 2.6566\n",
            "[Step 5800] Loss: 2.9771\n",
            "[Step 5810] Loss: 2.8085\n",
            "[Step 5820] Loss: 3.0055\n",
            "[Step 5830] Loss: 2.9267\n",
            "[Step 5840] Loss: 2.8935\n",
            "[Step 5850] Loss: 2.7556\n",
            "[Step 5860] Loss: 2.7945\n",
            "[Step 5870] Loss: 2.5355\n",
            "[Step 5880] Loss: 2.4439\n",
            "[Step 5890] Loss: 2.9353\n",
            "[Step 5900] Loss: 2.7797\n",
            "[Step 5910] Loss: 2.5051\n",
            "[Step 5920] Loss: 2.7912\n",
            "[Step 5930] Loss: 2.5879\n",
            "[Step 5940] Loss: 2.9423\n",
            "[Step 5950] Loss: 2.4671\n",
            "[Step 5960] Loss: 3.2297\n",
            "[Step 5970] Loss: 2.7797\n",
            "[Step 5980] Loss: 2.7468\n",
            "[Step 5990] Loss: 2.8320\n",
            "[Step 6000] Loss: 3.0087\n",
            "[Step 6010] Loss: 2.8338\n",
            "[Step 6020] Loss: 3.2044\n",
            "[Step 6030] Loss: 2.6627\n",
            "[Step 6040] Loss: 2.9340\n",
            "[Step 6050] Loss: 2.8177\n",
            "[Step 6060] Loss: 2.6800\n",
            "[Step 6070] Loss: 2.8845\n",
            "[Step 6080] Loss: 2.9654\n",
            "[Step 6090] Loss: 2.5514\n",
            "[Step 6100] Loss: 3.0354\n",
            "[Step 6110] Loss: 2.6928\n",
            "[Step 6120] Loss: 2.7233\n",
            "[Step 6130] Loss: 3.1516\n",
            "[Step 6140] Loss: 2.5552\n",
            "[Step 6150] Loss: 2.7504\n",
            "[Step 6160] Loss: 2.8226\n",
            "[Step 6170] Loss: 2.7824\n",
            "[Step 6180] Loss: 3.1033\n",
            "[Step 6190] Loss: 3.1867\n",
            "[Step 6200] Loss: 1.9722\n",
            "[Step 6210] Loss: 2.9897\n",
            "[Step 6220] Loss: 2.8978\n",
            "[Step 6230] Loss: 2.6418\n",
            "[Step 6240] Loss: 3.2851\n",
            "[Step 6250] Loss: 2.8204\n",
            "[Step 6260] Loss: 3.0452\n",
            "[Step 6270] Loss: 2.8614\n",
            "[Step 6280] Loss: 3.0289\n",
            "[Step 6290] Loss: 3.5187\n",
            "[Step 6300] Loss: 3.2033\n",
            "[Step 6310] Loss: 3.1757\n",
            "[Step 6320] Loss: 3.0593\n",
            "[Step 6330] Loss: 2.4837\n",
            "[Step 6340] Loss: 3.2371\n",
            "[Step 6350] Loss: 2.6623\n",
            "[Step 6360] Loss: 2.8264\n",
            "[Step 6370] Loss: 2.5292\n",
            "[Step 6380] Loss: 2.8807\n",
            "[Step 6390] Loss: 2.8553\n",
            "[Step 6400] Loss: 2.4123\n",
            "[Step 6410] Loss: 2.9984\n",
            "[Step 6420] Loss: 3.0346\n",
            "[Step 6430] Loss: 2.7741\n",
            "[Step 6440] Loss: 2.6752\n",
            "[Step 6450] Loss: 3.6860\n",
            "[Step 6460] Loss: 2.8297\n",
            "[Step 6470] Loss: 3.1867\n",
            "[Step 6480] Loss: 3.1481\n",
            "[Step 6490] Loss: 3.0004\n",
            "[Step 6500] Loss: 2.6792\n",
            "[Step 6510] Loss: 3.0089\n",
            "[Step 6520] Loss: 2.9524\n",
            "[Step 6530] Loss: 2.6918\n",
            "[Step 6540] Loss: 3.1584\n",
            "[Step 6550] Loss: 3.1497\n",
            "[Step 6560] Loss: 2.9002\n",
            "[Step 6570] Loss: 2.5139\n",
            "[Step 6580] Loss: 3.1058\n",
            "[Step 6590] Loss: 3.6462\n",
            "[Step 6600] Loss: 3.1280\n",
            "[Step 6610] Loss: 2.7603\n",
            "[Step 6620] Loss: 3.0114\n",
            "[Step 6630] Loss: 2.7558\n",
            "[Step 6640] Loss: 2.6964\n",
            "[Step 6650] Loss: 2.6940\n",
            "[Step 6660] Loss: 2.6558\n",
            "[Step 6670] Loss: 2.8628\n",
            "[Step 6680] Loss: 2.4655\n",
            "[Step 6690] Loss: 3.1441\n",
            "[Step 6700] Loss: 3.2170\n",
            "[Step 6710] Loss: 3.0590\n",
            "[Step 6720] Loss: 2.5387\n",
            "[Step 6730] Loss: 3.3827\n",
            "[Step 6740] Loss: 2.7731\n",
            "[Step 6750] Loss: 2.9848\n",
            "[Step 6760] Loss: 2.8208\n",
            "[Step 6770] Loss: 2.7110\n",
            "[Step 6780] Loss: 2.6342\n",
            "[Step 6790] Loss: 2.6919\n",
            "[Step 6800] Loss: 2.8036\n",
            "[Step 6810] Loss: 2.8595\n",
            "[Step 6820] Loss: 2.4944\n",
            "[Step 6830] Loss: 3.4534\n",
            "[Step 6840] Loss: 2.8717\n",
            "[Step 6850] Loss: 2.8043\n",
            "[Step 6860] Loss: 3.2915\n",
            "[Step 6870] Loss: 3.3230\n",
            "[Step 6880] Loss: 2.5573\n",
            "[Step 6890] Loss: 2.7968\n",
            "[Step 6900] Loss: 3.2162\n",
            "[Step 6910] Loss: 3.0658\n",
            "[Step 6920] Loss: 2.8957\n",
            "[Step 6930] Loss: 2.9713\n",
            "[Step 6940] Loss: 3.1179\n",
            "[Step 6950] Loss: 2.5053\n",
            "[Step 6960] Loss: 2.8125\n",
            "[Step 6970] Loss: 2.8836\n",
            "[Step 6980] Loss: 3.3523\n",
            "[Step 6990] Loss: 2.7754\n",
            "[Step 7000] Loss: 2.9059\n",
            "[Step 7010] Loss: 3.3612\n",
            "[Step 7020] Loss: 3.3490\n",
            "[Step 7030] Loss: 2.7889\n",
            "[Step 7040] Loss: 3.2887\n",
            "[Step 7050] Loss: 2.7583\n",
            "[Step 7060] Loss: 2.7162\n",
            "[Step 7070] Loss: 3.0065\n",
            "[Step 7080] Loss: 2.8993\n",
            "[Step 7090] Loss: 2.9052\n",
            "[Step 7100] Loss: 3.4780\n",
            "[Step 7110] Loss: 2.8516\n",
            "[Step 7120] Loss: 2.8308\n",
            "[Step 7130] Loss: 3.0078\n",
            "[Step 7140] Loss: 2.6985\n",
            "[Step 7150] Loss: 3.0582\n",
            "[Step 7160] Loss: 3.1596\n",
            "[Step 7170] Loss: 2.9187\n",
            "[Step 7180] Loss: 2.6992\n",
            "[Step 7190] Loss: 2.7671\n",
            "[Step 7200] Loss: 3.0504\n",
            "[Step 7210] Loss: 2.9661\n",
            "[Step 7220] Loss: 2.6946\n",
            "[Step 7230] Loss: 3.2431\n",
            "[Step 7240] Loss: 2.7072\n",
            "[Step 7250] Loss: 3.0656\n",
            "[Step 7260] Loss: 2.7889\n",
            "[Step 7270] Loss: 3.2221\n",
            "[Step 7280] Loss: 2.9212\n",
            "[Step 7290] Loss: 2.7092\n",
            "[Step 7300] Loss: 3.0473\n",
            "[Step 7310] Loss: 2.9188\n",
            "[Step 7320] Loss: 2.6280\n",
            "[Step 7330] Loss: 2.6245\n",
            "[Step 7340] Loss: 2.7294\n",
            "[Step 7350] Loss: 3.3095\n",
            "[Step 7360] Loss: 3.0324\n",
            "[Step 7370] Loss: 2.5348\n",
            "[Step 7380] Loss: 3.0456\n",
            "[Step 7390] Loss: 3.7351\n",
            "[Step 7400] Loss: 2.8517\n",
            "[Step 7410] Loss: 3.6817\n",
            "[Step 7420] Loss: 3.4344\n",
            "[Step 7430] Loss: 2.6912\n",
            "[Step 7440] Loss: 2.6216\n",
            "[Step 7450] Loss: 2.7363\n",
            "[Step 7460] Loss: 3.4886\n",
            "[Step 7470] Loss: 3.0491\n",
            "[Step 7480] Loss: 2.5191\n",
            "[Step 7490] Loss: 2.9819\n",
            "[Step 7500] Loss: 2.8684\n",
            "[Step 7510] Loss: 3.0299\n",
            "[Step 7520] Loss: 2.9696\n",
            "[Step 7530] Loss: 2.5234\n",
            "[Step 7540] Loss: 3.1035\n",
            "[Step 7550] Loss: 3.0894\n",
            "[Step 7560] Loss: 2.7612\n",
            "[Step 7570] Loss: 3.1009\n",
            "ðŸ“˜ Epoch 59 - Avg Training Loss: 2.9184\n",
            "ðŸ“Š Final Validation â€” Loss: 3.0294 | Accuracy: 0.2572 | Precision: 0.2802\n",
            "[Step 7580] Loss: 2.6103\n",
            "[Step 7590] Loss: 2.2300\n",
            "[Step 7600] Loss: 2.6088\n",
            "[Step 7610] Loss: 2.7598\n",
            "[Step 7620] Loss: 2.7596\n",
            "[Step 7630] Loss: 2.7341\n",
            "[Step 7640] Loss: 3.2184\n",
            "[Step 7650] Loss: 2.8491\n",
            "[Step 7660] Loss: 3.2097\n",
            "[Step 7670] Loss: 3.1142\n",
            "[Step 7680] Loss: 2.6372\n",
            "[Step 7690] Loss: 2.8146\n",
            "[Step 7700] Loss: 2.7280\n",
            "[Step 7710] Loss: 3.2584\n",
            "[Step 7720] Loss: 2.8397\n",
            "[Step 7730] Loss: 3.1672\n",
            "[Step 7740] Loss: 2.4878\n",
            "[Step 7750] Loss: 2.8294\n",
            "[Step 7760] Loss: 3.5344\n",
            "[Step 7770] Loss: 2.4132\n",
            "[Step 7780] Loss: 2.7666\n",
            "[Step 7790] Loss: 2.8816\n",
            "[Step 7800] Loss: 2.5584\n",
            "[Step 7810] Loss: 2.4661\n",
            "[Step 7820] Loss: 2.5498\n",
            "[Step 7830] Loss: 3.0227\n",
            "[Step 7840] Loss: 2.4409\n",
            "[Step 7850] Loss: 2.5352\n",
            "[Step 7860] Loss: 2.6402\n",
            "[Step 7870] Loss: 2.8004\n",
            "[Step 7880] Loss: 3.0690\n",
            "[Step 7890] Loss: 2.6292\n",
            "[Step 7900] Loss: 2.7521\n",
            "[Step 7910] Loss: 2.9050\n",
            "[Step 7920] Loss: 2.8998\n",
            "[Step 7930] Loss: 2.5633\n",
            "[Step 7940] Loss: 3.2889\n",
            "[Step 7950] Loss: 2.9165\n",
            "[Step 7960] Loss: 3.0869\n",
            "[Step 7970] Loss: 2.8964\n",
            "[Step 7980] Loss: 3.0959\n",
            "[Step 7990] Loss: 3.1576\n",
            "[Step 8000] Loss: 2.8558\n",
            "[Step 8010] Loss: 2.4564\n",
            "[Step 8020] Loss: 2.7996\n",
            "[Step 8030] Loss: 2.7037\n",
            "[Step 8040] Loss: 2.8391\n",
            "[Step 8050] Loss: 2.7560\n",
            "[Step 8060] Loss: 3.0012\n",
            "[Step 8070] Loss: 2.6970\n",
            "[Step 8080] Loss: 2.4622\n",
            "[Step 8090] Loss: 2.7638\n",
            "[Step 8100] Loss: 3.2551\n",
            "[Step 8110] Loss: 2.8462\n",
            "[Step 8120] Loss: 2.9405\n",
            "[Step 8130] Loss: 2.5960\n",
            "[Step 8140] Loss: 2.6191\n",
            "[Step 8150] Loss: 2.7255\n",
            "[Step 8160] Loss: 2.8548\n",
            "[Step 8170] Loss: 2.9334\n",
            "[Step 8180] Loss: 2.2839\n",
            "[Step 8190] Loss: 3.3429\n",
            "[Step 8200] Loss: 2.7892\n",
            "[Step 8210] Loss: 2.5855\n",
            "[Step 8220] Loss: 3.4238\n",
            "[Step 8230] Loss: 2.6253\n",
            "[Step 8240] Loss: 2.3111\n",
            "[Step 8250] Loss: 3.0852\n",
            "[Step 8260] Loss: 2.6759\n",
            "[Step 8270] Loss: 3.0253\n",
            "[Step 8280] Loss: 2.9310\n",
            "[Step 8290] Loss: 2.8569\n",
            "[Step 8300] Loss: 2.9862\n",
            "[Step 8310] Loss: 3.1575\n",
            "[Step 8320] Loss: 2.9600\n",
            "[Step 8330] Loss: 2.6549\n",
            "[Step 8340] Loss: 2.6232\n",
            "[Step 8350] Loss: 2.7183\n",
            "[Step 8360] Loss: 2.5254\n",
            "[Step 8370] Loss: 3.2301\n",
            "[Step 8380] Loss: 3.2336\n",
            "[Step 8390] Loss: 2.9840\n",
            "[Step 8400] Loss: 3.2325\n",
            "[Step 8410] Loss: 3.2031\n",
            "[Step 8420] Loss: 2.9879\n",
            "[Step 8430] Loss: 3.1495\n",
            "[Step 8440] Loss: 3.0384\n",
            "[Step 8450] Loss: 2.3751\n",
            "[Step 8460] Loss: 2.3763\n",
            "[Step 8470] Loss: 2.8454\n",
            "[Step 8480] Loss: 2.8046\n",
            "[Step 8490] Loss: 2.5542\n",
            "[Step 8500] Loss: 2.3856\n",
            "[Step 8510] Loss: 2.8501\n",
            "[Step 8520] Loss: 3.1352\n",
            "[Step 8530] Loss: 2.6927\n",
            "[Step 8540] Loss: 3.0325\n",
            "[Step 8550] Loss: 3.0722\n",
            "[Step 8560] Loss: 2.7122\n",
            "[Step 8570] Loss: 2.5151\n",
            "[Step 8580] Loss: 2.9125\n",
            "[Step 8590] Loss: 2.9865\n",
            "[Step 8600] Loss: 2.9937\n",
            "[Step 8610] Loss: 3.0473\n",
            "[Step 8620] Loss: 2.9334\n",
            "[Step 8630] Loss: 3.3407\n",
            "[Step 8640] Loss: 2.5284\n",
            "[Step 8650] Loss: 2.4653\n",
            "[Step 8660] Loss: 3.2902\n",
            "[Step 8670] Loss: 2.6660\n",
            "[Step 8680] Loss: 2.8854\n",
            "[Step 8690] Loss: 2.4622\n",
            "[Step 8700] Loss: 2.7847\n",
            "[Step 8710] Loss: 2.9671\n",
            "[Step 8720] Loss: 2.8007\n",
            "[Step 8730] Loss: 2.6271\n",
            "[Step 8740] Loss: 3.3451\n",
            "[Step 8750] Loss: 3.2412\n",
            "[Step 8760] Loss: 2.6709\n",
            "[Step 8770] Loss: 2.9995\n",
            "[Step 8780] Loss: 2.5668\n",
            "[Step 8790] Loss: 2.6720\n",
            "[Step 8800] Loss: 2.8597\n",
            "[Step 8810] Loss: 2.4292\n",
            "[Step 8820] Loss: 2.7225\n",
            "[Step 8830] Loss: 2.5567\n",
            "[Step 8840] Loss: 2.6976\n",
            "[Step 8850] Loss: 2.5101\n",
            "[Step 8860] Loss: 2.7387\n",
            "[Step 8870] Loss: 2.8315\n",
            "[Step 8880] Loss: 2.9245\n",
            "[Step 8890] Loss: 2.6654\n",
            "[Step 8900] Loss: 2.8630\n",
            "[Step 8910] Loss: 2.8121\n",
            "[Step 8920] Loss: 2.6697\n",
            "[Step 8930] Loss: 2.4327\n",
            "[Step 8940] Loss: 2.9322\n",
            "[Step 8950] Loss: 2.9336\n",
            "[Step 8960] Loss: 3.2662\n",
            "[Step 8970] Loss: 3.0080\n",
            "[Step 8980] Loss: 2.5720\n",
            "[Step 8990] Loss: 2.7164\n",
            "[Step 9000] Loss: 2.5865\n",
            "[Step 9010] Loss: 2.6232\n",
            "[Step 9020] Loss: 2.9323\n",
            "[Step 9030] Loss: 3.0890\n",
            "[Step 9040] Loss: 3.2285\n",
            "[Step 9050] Loss: 2.7802\n",
            "[Step 9060] Loss: 3.4031\n",
            "[Step 9070] Loss: 2.8611\n",
            "[Step 9080] Loss: 2.9827\n",
            "[Step 9090] Loss: 2.7589\n",
            "[Step 9100] Loss: 3.1309\n",
            "[Step 9110] Loss: 2.9824\n",
            "[Step 9120] Loss: 3.0213\n",
            "[Step 9130] Loss: 3.2205\n",
            "[Step 9140] Loss: 2.8894\n",
            "[Step 9150] Loss: 2.2560\n",
            "[Step 9160] Loss: 3.0740\n",
            "[Step 9170] Loss: 2.9365\n",
            "[Step 9180] Loss: 3.0117\n",
            "[Step 9190] Loss: 2.5820\n",
            "[Step 9200] Loss: 2.8280\n",
            "[Step 9210] Loss: 2.7586\n",
            "[Step 9220] Loss: 2.6193\n",
            "[Step 9230] Loss: 2.9649\n",
            "[Step 9240] Loss: 2.6043\n",
            "[Step 9250] Loss: 2.7229\n",
            "[Step 9260] Loss: 2.7231\n",
            "[Step 9270] Loss: 2.7581\n",
            "[Step 9280] Loss: 3.1884\n",
            "[Step 9290] Loss: 2.8437\n",
            "[Step 9300] Loss: 3.0851\n",
            "[Step 9310] Loss: 2.8155\n",
            "[Step 9320] Loss: 3.1152\n",
            "[Step 9330] Loss: 3.1166\n",
            "[Step 9340] Loss: 3.1791\n",
            "[Step 9350] Loss: 3.0752\n",
            "[Step 9360] Loss: 2.5493\n",
            "[Step 9370] Loss: 2.4076\n",
            "[Step 9380] Loss: 2.5749\n",
            "[Step 9390] Loss: 2.7862\n",
            "[Step 9400] Loss: 3.0507\n",
            "[Step 9410] Loss: 2.6649\n",
            "[Step 9420] Loss: 2.8525\n",
            "[Step 9430] Loss: 2.4978\n",
            "[Step 9440] Loss: 3.0167\n",
            "[Step 9450] Loss: 2.6118\n",
            "[Step 9460] Loss: 3.2594\n",
            "[Step 9470] Loss: 2.6425\n",
            "ðŸ“˜ Epoch 60 - Avg Training Loss: 2.8526\n",
            "ðŸ“Š Final Validation â€” Loss: 3.2144 | Accuracy: 0.2265 | Precision: 0.2605\n",
            "[Step 9480] Loss: 3.3131\n",
            "[Step 9490] Loss: 2.9108\n",
            "[Step 9500] Loss: 2.4807\n",
            "[Step 9510] Loss: 2.9386\n",
            "[Step 9520] Loss: 2.7863\n",
            "[Step 9530] Loss: 2.7158\n",
            "[Step 9540] Loss: 3.0643\n",
            "[Step 9550] Loss: 2.8116\n",
            "[Step 9560] Loss: 2.8125\n",
            "[Step 9570] Loss: 3.0078\n",
            "[Step 9580] Loss: 2.2885\n",
            "[Step 9590] Loss: 2.9061\n",
            "[Step 9600] Loss: 2.7708\n",
            "[Step 9610] Loss: 2.8434\n",
            "[Step 9620] Loss: 3.3395\n",
            "[Step 9630] Loss: 2.2242\n",
            "[Step 9640] Loss: 2.8204\n",
            "[Step 9650] Loss: 2.9649\n",
            "[Step 9660] Loss: 3.3363\n",
            "[Step 9670] Loss: 2.3403\n",
            "[Step 9680] Loss: 2.8408\n",
            "[Step 9690] Loss: 2.6250\n",
            "[Step 9700] Loss: 2.8806\n",
            "[Step 9710] Loss: 3.2351\n",
            "[Step 9720] Loss: 2.8706\n",
            "[Step 9730] Loss: 2.5605\n",
            "[Step 9740] Loss: 3.1978\n",
            "[Step 9750] Loss: 3.0058\n",
            "[Step 9760] Loss: 2.5612\n",
            "[Step 9770] Loss: 3.2205\n",
            "[Step 9780] Loss: 2.9830\n",
            "[Step 9790] Loss: 2.5855\n",
            "[Step 9800] Loss: 3.0630\n",
            "[Step 9810] Loss: 2.8460\n",
            "[Step 9820] Loss: 2.1948\n",
            "[Step 9830] Loss: 2.2468\n",
            "[Step 9840] Loss: 2.9910\n",
            "[Step 9850] Loss: 2.2239\n",
            "[Step 9860] Loss: 3.1221\n",
            "[Step 9870] Loss: 2.2833\n",
            "[Step 9880] Loss: 2.6898\n",
            "[Step 9890] Loss: 2.9386\n",
            "[Step 9900] Loss: 2.8350\n",
            "[Step 9910] Loss: 2.2449\n",
            "[Step 9920] Loss: 3.0279\n",
            "[Step 9930] Loss: 3.0255\n",
            "[Step 9940] Loss: 2.8279\n",
            "[Step 9950] Loss: 2.9582\n",
            "[Step 9960] Loss: 2.3927\n",
            "[Step 9970] Loss: 2.6061\n",
            "[Step 9980] Loss: 2.8289\n",
            "[Step 9990] Loss: 2.6821\n",
            "[Step 10000] Loss: 2.4992\n",
            "[Step 10010] Loss: 2.6472\n",
            "[Step 10020] Loss: 3.0097\n",
            "[Step 10030] Loss: 2.9932\n",
            "[Step 10040] Loss: 3.4176\n",
            "[Step 10050] Loss: 2.5682\n",
            "[Step 10060] Loss: 2.5052\n",
            "[Step 10070] Loss: 3.3110\n",
            "[Step 10080] Loss: 2.8731\n",
            "[Step 10090] Loss: 3.1445\n",
            "[Step 10100] Loss: 2.9195\n",
            "[Step 10110] Loss: 2.7023\n",
            "[Step 10120] Loss: 3.6222\n",
            "[Step 10130] Loss: 2.4697\n",
            "[Step 10140] Loss: 3.3139\n",
            "[Step 10150] Loss: 3.0737\n",
            "[Step 10160] Loss: 2.7566\n",
            "[Step 10170] Loss: 3.1251\n",
            "[Step 10180] Loss: 2.6440\n",
            "[Step 10190] Loss: 2.8841\n",
            "[Step 10200] Loss: 2.4386\n",
            "[Step 10210] Loss: 2.2152\n",
            "[Step 10220] Loss: 2.5555\n",
            "[Step 10230] Loss: 2.3700\n",
            "[Step 10240] Loss: 3.0765\n",
            "[Step 10250] Loss: 3.1079\n",
            "[Step 10260] Loss: 3.1908\n",
            "[Step 10270] Loss: 2.1779\n",
            "[Step 10280] Loss: 2.7961\n",
            "[Step 10290] Loss: 2.7984\n",
            "[Step 10300] Loss: 2.5671\n",
            "[Step 10310] Loss: 2.7178\n",
            "[Step 10320] Loss: 3.2710\n",
            "[Step 10330] Loss: 2.8940\n",
            "[Step 10340] Loss: 2.8299\n",
            "[Step 10350] Loss: 3.0713\n",
            "[Step 10360] Loss: 2.9521\n",
            "[Step 10370] Loss: 2.8087\n",
            "[Step 10380] Loss: 2.4829\n",
            "[Step 10390] Loss: 2.7460\n",
            "[Step 10400] Loss: 2.2692\n",
            "[Step 10410] Loss: 2.9989\n",
            "[Step 10420] Loss: 2.7407\n",
            "[Step 10430] Loss: 2.9715\n",
            "[Step 10440] Loss: 2.8835\n",
            "[Step 10450] Loss: 2.4448\n",
            "[Step 10460] Loss: 2.6269\n",
            "[Step 10470] Loss: 2.3100\n",
            "[Step 10480] Loss: 2.8150\n",
            "[Step 10490] Loss: 2.9301\n",
            "[Step 10500] Loss: 2.8209\n",
            "[Step 10510] Loss: 3.3020\n",
            "[Step 10520] Loss: 2.4399\n",
            "[Step 10530] Loss: 2.8842\n",
            "[Step 10540] Loss: 2.9532\n",
            "[Step 10550] Loss: 2.9371\n",
            "[Step 10560] Loss: 2.4219\n",
            "[Step 10570] Loss: 2.7197\n",
            "[Step 10580] Loss: 2.3096\n",
            "[Step 10590] Loss: 2.4002\n",
            "[Step 10600] Loss: 2.8948\n",
            "[Step 10610] Loss: 2.7752\n",
            "[Step 10620] Loss: 2.8671\n",
            "[Step 10630] Loss: 2.6929\n",
            "[Step 10640] Loss: 3.4030\n",
            "[Step 10650] Loss: 3.1822\n",
            "[Step 10660] Loss: 2.9416\n",
            "[Step 10670] Loss: 2.8573\n",
            "[Step 10680] Loss: 2.3813\n",
            "[Step 10690] Loss: 3.1021\n",
            "[Step 10700] Loss: 2.8835\n",
            "[Step 10710] Loss: 2.7639\n",
            "[Step 10720] Loss: 2.5557\n",
            "[Step 10730] Loss: 2.8980\n",
            "[Step 10740] Loss: 3.1016\n",
            "[Step 10750] Loss: 2.9866\n",
            "[Step 10760] Loss: 2.8224\n",
            "[Step 10770] Loss: 2.4881\n",
            "[Step 10780] Loss: 2.6156\n",
            "[Step 10790] Loss: 3.0706\n",
            "[Step 10800] Loss: 2.9103\n",
            "[Step 10810] Loss: 2.3544\n",
            "[Step 10820] Loss: 2.4919\n",
            "[Step 10830] Loss: 3.1347\n",
            "[Step 10840] Loss: 2.7642\n",
            "[Step 10850] Loss: 3.4314\n",
            "[Step 10860] Loss: 2.9874\n",
            "[Step 10870] Loss: 3.3956\n",
            "[Step 10880] Loss: 2.4909\n",
            "[Step 10890] Loss: 2.5066\n",
            "[Step 10900] Loss: 2.8451\n",
            "[Step 10910] Loss: 2.7430\n",
            "[Step 10920] Loss: 2.9375\n",
            "[Step 10930] Loss: 2.3348\n",
            "[Step 10940] Loss: 2.5072\n",
            "[Step 10950] Loss: 2.6711\n",
            "[Step 10960] Loss: 2.9267\n",
            "[Step 10970] Loss: 2.7934\n",
            "[Step 10980] Loss: 3.3257\n",
            "[Step 10990] Loss: 2.6908\n",
            "[Step 11000] Loss: 2.3247\n",
            "[Step 11010] Loss: 2.3772\n",
            "[Step 11020] Loss: 2.7384\n",
            "[Step 11030] Loss: 2.8510\n",
            "[Step 11040] Loss: 2.7076\n",
            "[Step 11050] Loss: 2.8393\n",
            "[Step 11060] Loss: 2.6279\n",
            "[Step 11070] Loss: 2.8563\n",
            "[Step 11080] Loss: 2.8146\n",
            "[Step 11090] Loss: 2.9881\n",
            "[Step 11100] Loss: 2.5964\n",
            "[Step 11110] Loss: 2.6552\n",
            "[Step 11120] Loss: 3.0641\n",
            "[Step 11130] Loss: 2.7347\n",
            "[Step 11140] Loss: 2.7547\n",
            "[Step 11150] Loss: 2.6018\n",
            "[Step 11160] Loss: 3.4894\n",
            "[Step 11170] Loss: 2.7546\n",
            "[Step 11180] Loss: 3.0525\n",
            "[Step 11190] Loss: 2.6350\n",
            "[Step 11200] Loss: 2.6375\n",
            "[Step 11210] Loss: 3.2628\n",
            "[Step 11220] Loss: 3.2268\n",
            "[Step 11230] Loss: 3.4966\n",
            "[Step 11240] Loss: 3.0664\n",
            "[Step 11250] Loss: 2.6903\n",
            "[Step 11260] Loss: 2.6058\n",
            "[Step 11270] Loss: 2.8865\n",
            "[Step 11280] Loss: 2.5378\n",
            "[Step 11290] Loss: 2.3203\n",
            "[Step 11300] Loss: 2.6923\n",
            "[Step 11310] Loss: 2.6735\n",
            "[Step 11320] Loss: 2.6771\n",
            "[Step 11330] Loss: 2.4728\n",
            "[Step 11340] Loss: 3.1752\n",
            "[Step 11350] Loss: 2.3869\n",
            "[Step 11360] Loss: 2.6736\n",
            "ðŸ“˜ Epoch 61 - Avg Training Loss: 2.7732\n",
            "ðŸ“Š Final Validation â€” Loss: 3.0228 | Accuracy: 0.2571 | Precision: 0.3006\n",
            "[Step 11370] Loss: 2.7667\n",
            "[Step 11380] Loss: 2.8221\n",
            "[Step 11390] Loss: 2.3924\n",
            "[Step 11400] Loss: 2.4431\n",
            "[Step 11410] Loss: 2.6307\n",
            "[Step 11420] Loss: 2.8424\n",
            "[Step 11430] Loss: 2.0926\n",
            "[Step 11440] Loss: 2.5452\n",
            "[Step 11450] Loss: 3.1451\n",
            "[Step 11460] Loss: 2.6015\n",
            "[Step 11470] Loss: 2.3884\n",
            "[Step 11480] Loss: 2.4375\n",
            "[Step 11490] Loss: 2.8171\n",
            "[Step 11500] Loss: 2.8467\n",
            "[Step 11510] Loss: 2.9605\n",
            "[Step 11520] Loss: 2.3340\n",
            "[Step 11530] Loss: 2.6130\n",
            "[Step 11540] Loss: 2.9342\n",
            "[Step 11550] Loss: 2.3940\n",
            "[Step 11560] Loss: 2.6609\n",
            "[Step 11570] Loss: 3.2859\n",
            "[Step 11580] Loss: 2.8999\n",
            "[Step 11590] Loss: 2.8609\n",
            "[Step 11600] Loss: 3.0796\n",
            "[Step 11610] Loss: 2.3204\n",
            "[Step 11620] Loss: 3.0131\n",
            "[Step 11630] Loss: 2.8827\n",
            "[Step 11640] Loss: 2.6944\n",
            "[Step 11650] Loss: 2.8895\n",
            "[Step 11660] Loss: 3.1164\n",
            "[Step 11670] Loss: 2.6661\n",
            "[Step 11680] Loss: 2.3471\n",
            "[Step 11690] Loss: 2.9855\n",
            "[Step 11700] Loss: 3.0653\n",
            "[Step 11710] Loss: 2.5686\n",
            "[Step 11720] Loss: 2.9751\n",
            "[Step 11730] Loss: 2.6942\n",
            "[Step 11740] Loss: 2.7094\n",
            "[Step 11750] Loss: 2.2322\n",
            "[Step 11760] Loss: 2.6873\n",
            "[Step 11770] Loss: 2.7175\n",
            "[Step 11780] Loss: 2.7475\n",
            "[Step 11790] Loss: 2.3482\n",
            "[Step 11800] Loss: 2.8460\n",
            "[Step 11810] Loss: 2.2981\n",
            "[Step 11820] Loss: 2.3623\n",
            "[Step 11830] Loss: 2.8428\n",
            "[Step 11840] Loss: 3.2000\n",
            "[Step 11850] Loss: 3.0249\n",
            "[Step 11860] Loss: 2.5597\n",
            "[Step 11870] Loss: 2.8216\n",
            "[Step 11880] Loss: 2.6509\n",
            "[Step 11890] Loss: 2.1007\n",
            "[Step 11900] Loss: 2.6317\n",
            "[Step 11910] Loss: 2.5857\n",
            "[Step 11920] Loss: 3.0777\n",
            "[Step 11930] Loss: 2.8032\n",
            "[Step 11940] Loss: 2.5462\n",
            "[Step 11950] Loss: 2.7324\n",
            "[Step 11960] Loss: 2.6402\n",
            "[Step 11970] Loss: 3.1815\n",
            "[Step 11980] Loss: 2.7654\n",
            "[Step 11990] Loss: 2.8172\n",
            "[Step 12000] Loss: 2.2723\n",
            "[Step 12010] Loss: 2.7001\n",
            "[Step 12020] Loss: 2.1814\n",
            "[Step 12030] Loss: 3.0452\n",
            "[Step 12040] Loss: 2.6878\n",
            "[Step 12050] Loss: 2.9592\n",
            "[Step 12060] Loss: 2.6359\n",
            "[Step 12070] Loss: 2.5548\n",
            "[Step 12080] Loss: 2.8309\n",
            "[Step 12090] Loss: 2.8577\n",
            "[Step 12100] Loss: 3.0853\n",
            "[Step 12110] Loss: 2.6688\n",
            "[Step 12120] Loss: 2.5541\n",
            "[Step 12130] Loss: 2.7998\n",
            "[Step 12140] Loss: 3.1180\n",
            "[Step 12150] Loss: 2.7124\n",
            "[Step 12160] Loss: 1.9992\n",
            "[Step 12170] Loss: 2.6544\n",
            "[Step 12180] Loss: 3.1943\n",
            "[Step 12190] Loss: 2.5483\n",
            "[Step 12200] Loss: 2.6120\n",
            "[Step 12210] Loss: 2.9127\n",
            "[Step 12220] Loss: 2.9711\n",
            "[Step 12230] Loss: 2.7589\n",
            "[Step 12240] Loss: 2.2358\n",
            "[Step 12250] Loss: 2.5933\n",
            "[Step 12260] Loss: 2.6060\n",
            "[Step 12270] Loss: 2.5124\n",
            "[Step 12280] Loss: 2.9318\n",
            "[Step 12290] Loss: 2.6259\n",
            "[Step 12300] Loss: 2.7815\n",
            "[Step 12310] Loss: 2.3498\n",
            "[Step 12320] Loss: 2.5713\n",
            "[Step 12330] Loss: 2.9706\n",
            "[Step 12340] Loss: 2.6669\n",
            "[Step 12350] Loss: 2.8863\n",
            "[Step 12360] Loss: 2.3993\n",
            "[Step 12370] Loss: 2.7075\n",
            "[Step 12380] Loss: 2.5924\n",
            "[Step 12390] Loss: 2.5229\n",
            "[Step 12400] Loss: 2.8932\n",
            "[Step 12410] Loss: 2.8916\n",
            "[Step 12420] Loss: 2.9102\n",
            "[Step 12430] Loss: 3.5445\n",
            "[Step 12440] Loss: 2.5364\n",
            "[Step 12450] Loss: 2.8824\n",
            "[Step 12460] Loss: 2.8778\n",
            "[Step 12470] Loss: 2.8592\n",
            "[Step 12480] Loss: 2.3879\n",
            "[Step 12490] Loss: 2.4550\n",
            "[Step 12500] Loss: 2.2122\n",
            "[Step 12510] Loss: 2.6878\n",
            "[Step 12520] Loss: 2.5925\n",
            "[Step 12530] Loss: 2.9340\n",
            "[Step 12540] Loss: 2.4396\n",
            "[Step 12550] Loss: 2.5512\n",
            "[Step 12560] Loss: 2.3339\n",
            "[Step 12570] Loss: 2.3918\n",
            "[Step 12580] Loss: 2.5877\n",
            "[Step 12590] Loss: 2.8997\n",
            "[Step 12600] Loss: 2.9626\n",
            "[Step 12610] Loss: 2.5379\n",
            "[Step 12620] Loss: 3.1780\n",
            "[Step 12630] Loss: 2.6708\n",
            "[Step 12640] Loss: 2.6853\n",
            "[Step 12650] Loss: 2.5316\n",
            "[Step 12660] Loss: 2.5166\n",
            "[Step 12670] Loss: 2.7212\n",
            "[Step 12680] Loss: 2.8264\n",
            "[Step 12690] Loss: 2.6597\n",
            "[Step 12700] Loss: 2.3711\n",
            "[Step 12710] Loss: 3.0762\n",
            "[Step 12720] Loss: 2.6453\n",
            "[Step 12730] Loss: 2.7218\n",
            "[Step 12740] Loss: 2.8064\n",
            "[Step 12750] Loss: 2.7144\n",
            "[Step 12760] Loss: 2.8173\n",
            "[Step 12770] Loss: 2.8766\n",
            "[Step 12780] Loss: 2.6092\n",
            "[Step 12790] Loss: 2.5563\n",
            "[Step 12800] Loss: 2.3331\n",
            "[Step 12810] Loss: 2.3938\n",
            "[Step 12820] Loss: 3.4044\n",
            "[Step 12830] Loss: 3.0264\n",
            "[Step 12840] Loss: 2.4379\n",
            "[Step 12850] Loss: 2.6024\n",
            "[Step 12860] Loss: 2.9630\n",
            "[Step 12870] Loss: 2.9635\n",
            "[Step 12880] Loss: 2.3774\n",
            "[Step 12890] Loss: 2.9449\n",
            "[Step 12900] Loss: 2.7419\n",
            "[Step 12910] Loss: 2.8752\n",
            "[Step 12920] Loss: 3.0330\n",
            "[Step 12930] Loss: 2.5215\n",
            "[Step 12940] Loss: 2.8627\n",
            "[Step 12950] Loss: 2.5497\n",
            "[Step 12960] Loss: 2.8262\n",
            "[Step 12970] Loss: 2.4607\n",
            "[Step 12980] Loss: 3.1894\n",
            "[Step 12990] Loss: 2.8040\n",
            "[Step 13000] Loss: 2.4844\n",
            "[Step 13010] Loss: 3.2423\n",
            "[Step 13020] Loss: 2.7388\n",
            "[Step 13030] Loss: 2.5770\n",
            "[Step 13040] Loss: 2.7968\n",
            "[Step 13050] Loss: 2.7258\n",
            "[Step 13060] Loss: 2.7039\n",
            "[Step 13070] Loss: 2.8053\n",
            "[Step 13080] Loss: 2.2787\n",
            "[Step 13090] Loss: 2.6661\n",
            "[Step 13100] Loss: 3.0576\n",
            "[Step 13110] Loss: 2.3952\n",
            "[Step 13120] Loss: 2.7575\n",
            "[Step 13130] Loss: 2.5724\n",
            "[Step 13140] Loss: 3.2415\n",
            "[Step 13150] Loss: 2.4494\n",
            "[Step 13160] Loss: 2.6005\n",
            "[Step 13170] Loss: 2.8695\n",
            "[Step 13180] Loss: 2.5388\n",
            "[Step 13190] Loss: 2.4995\n",
            "[Step 13200] Loss: 2.3379\n",
            "[Step 13210] Loss: 2.7465\n",
            "[Step 13220] Loss: 2.3437\n",
            "[Step 13230] Loss: 2.4350\n",
            "[Step 13240] Loss: 2.7667\n",
            "[Step 13250] Loss: 3.2176\n",
            "ðŸ“˜ Epoch 62 - Avg Training Loss: 2.6847\n",
            "ðŸ“Š Final Validation â€” Loss: 2.7688 | Accuracy: 0.3174 | Precision: 0.3134\n",
            "[Step 13260] Loss: 3.0253\n",
            "[Step 13270] Loss: 2.5535\n",
            "[Step 13280] Loss: 2.3578\n",
            "[Step 13290] Loss: 2.7478\n",
            "[Step 13300] Loss: 2.9030\n",
            "[Step 13310] Loss: 2.3561\n",
            "[Step 13320] Loss: 2.7021\n",
            "[Step 13330] Loss: 2.6624\n",
            "[Step 13340] Loss: 3.1065\n",
            "[Step 13350] Loss: 2.5766\n",
            "[Step 13360] Loss: 2.6392\n",
            "[Step 13370] Loss: 1.9772\n",
            "[Step 13380] Loss: 2.6558\n",
            "[Step 13390] Loss: 2.5612\n",
            "[Step 13400] Loss: 2.3941\n",
            "[Step 13410] Loss: 2.3953\n",
            "[Step 13420] Loss: 2.1760\n",
            "[Step 13430] Loss: 2.4468\n",
            "[Step 13440] Loss: 2.5387\n",
            "[Step 13450] Loss: 2.6563\n",
            "[Step 13460] Loss: 2.6962\n",
            "[Step 13470] Loss: 2.5952\n",
            "[Step 13480] Loss: 2.6236\n",
            "[Step 13490] Loss: 2.7316\n",
            "[Step 13500] Loss: 2.9803\n",
            "[Step 13510] Loss: 2.4195\n",
            "[Step 13520] Loss: 2.3416\n",
            "[Step 13530] Loss: 2.8027\n",
            "[Step 13540] Loss: 2.3085\n",
            "[Step 13550] Loss: 2.7270\n",
            "[Step 13560] Loss: 2.5765\n",
            "[Step 13570] Loss: 2.8150\n",
            "[Step 13580] Loss: 1.8016\n",
            "[Step 13590] Loss: 2.6598\n",
            "[Step 13600] Loss: 2.2876\n",
            "[Step 13610] Loss: 3.0336\n",
            "[Step 13620] Loss: 2.4889\n",
            "[Step 13630] Loss: 2.5164\n",
            "[Step 13640] Loss: 2.5521\n",
            "[Step 13650] Loss: 2.7802\n",
            "[Step 13660] Loss: 2.2092\n",
            "[Step 13670] Loss: 3.2051\n",
            "[Step 13680] Loss: 2.2485\n",
            "[Step 13690] Loss: 2.0797\n",
            "[Step 13700] Loss: 2.6535\n",
            "[Step 13710] Loss: 2.9190\n",
            "[Step 13720] Loss: 2.0521\n",
            "[Step 13730] Loss: 2.5051\n",
            "[Step 13740] Loss: 2.3661\n",
            "[Step 13750] Loss: 3.1181\n",
            "[Step 13760] Loss: 2.6228\n",
            "[Step 13770] Loss: 2.3831\n",
            "[Step 13780] Loss: 2.8190\n",
            "[Step 13790] Loss: 2.4252\n",
            "[Step 13800] Loss: 2.4719\n",
            "[Step 13810] Loss: 2.1807\n",
            "[Step 13820] Loss: 2.4104\n",
            "[Step 13830] Loss: 2.0378\n",
            "[Step 13840] Loss: 3.1089\n",
            "[Step 13850] Loss: 2.6985\n",
            "[Step 13860] Loss: 2.1455\n",
            "[Step 13870] Loss: 2.4605\n",
            "[Step 13880] Loss: 2.6741\n",
            "[Step 13890] Loss: 2.4952\n",
            "[Step 13900] Loss: 2.9651\n",
            "[Step 13910] Loss: 2.0663\n",
            "[Step 13920] Loss: 2.0760\n",
            "[Step 13930] Loss: 2.5192\n",
            "[Step 13940] Loss: 2.5833\n",
            "[Step 13950] Loss: 2.5598\n",
            "[Step 13960] Loss: 2.5562\n",
            "[Step 13970] Loss: 2.4849\n",
            "[Step 13980] Loss: 2.6237\n",
            "[Step 13990] Loss: 2.6807\n",
            "[Step 14000] Loss: 2.4451\n",
            "[Step 14010] Loss: 2.3682\n",
            "[Step 14020] Loss: 2.5105\n",
            "[Step 14030] Loss: 3.0362\n",
            "[Step 14040] Loss: 2.6207\n",
            "[Step 14050] Loss: 2.4564\n",
            "[Step 14060] Loss: 2.2374\n",
            "[Step 14070] Loss: 2.3628\n",
            "[Step 14080] Loss: 2.5374\n",
            "[Step 14090] Loss: 2.0748\n",
            "[Step 14100] Loss: 2.6681\n",
            "[Step 14110] Loss: 2.4976\n",
            "[Step 14120] Loss: 2.8405\n",
            "[Step 14130] Loss: 2.4652\n",
            "[Step 14140] Loss: 2.4259\n",
            "[Step 14150] Loss: 2.6623\n",
            "[Step 14160] Loss: 2.9234\n",
            "[Step 14170] Loss: 2.9072\n",
            "[Step 14180] Loss: 2.5666\n",
            "[Step 14190] Loss: 2.5992\n",
            "[Step 14200] Loss: 2.2330\n",
            "[Step 14210] Loss: 2.4274\n",
            "[Step 14220] Loss: 2.5475\n",
            "[Step 14230] Loss: 2.4569\n",
            "[Step 14240] Loss: 2.4017\n",
            "[Step 14250] Loss: 3.1677\n",
            "[Step 14260] Loss: 2.4678\n",
            "[Step 14270] Loss: 3.0932\n",
            "[Step 14280] Loss: 2.4003\n",
            "[Step 14290] Loss: 2.7576\n",
            "[Step 14300] Loss: 2.5919\n",
            "[Step 14310] Loss: 2.2487\n",
            "[Step 14320] Loss: 2.2464\n",
            "[Step 14330] Loss: 2.9299\n",
            "[Step 14340] Loss: 2.4726\n",
            "[Step 14350] Loss: 2.5801\n",
            "[Step 14360] Loss: 2.9158\n",
            "[Step 14370] Loss: 2.5962\n",
            "[Step 14380] Loss: 2.7400\n",
            "[Step 14390] Loss: 2.6836\n",
            "[Step 14400] Loss: 3.3666\n",
            "[Step 14410] Loss: 2.5120\n",
            "[Step 14420] Loss: 2.4770\n",
            "[Step 14430] Loss: 3.1981\n",
            "[Step 14440] Loss: 2.5729\n",
            "[Step 14450] Loss: 2.7459\n",
            "[Step 14460] Loss: 2.4742\n",
            "[Step 14470] Loss: 2.1721\n",
            "[Step 14480] Loss: 2.7811\n",
            "[Step 14490] Loss: 2.8760\n",
            "[Step 14500] Loss: 2.1819\n",
            "[Step 14510] Loss: 2.9193\n",
            "[Step 14520] Loss: 2.5524\n",
            "[Step 14530] Loss: 2.9729\n",
            "[Step 14540] Loss: 2.2245\n",
            "[Step 14550] Loss: 2.1604\n",
            "[Step 14560] Loss: 2.5921\n",
            "[Step 14570] Loss: 2.7735\n",
            "[Step 14580] Loss: 2.8281\n",
            "[Step 14590] Loss: 2.2022\n",
            "[Step 14600] Loss: 2.5424\n",
            "[Step 14610] Loss: 2.4177\n",
            "[Step 14620] Loss: 2.1580\n",
            "[Step 14630] Loss: 2.2094\n",
            "[Step 14640] Loss: 2.9430\n",
            "[Step 14650] Loss: 2.6411\n",
            "[Step 14660] Loss: 2.6234\n",
            "[Step 14670] Loss: 2.7863\n",
            "[Step 14680] Loss: 2.5680\n",
            "[Step 14690] Loss: 2.5579\n",
            "[Step 14700] Loss: 2.7838\n",
            "[Step 14710] Loss: 2.8520\n",
            "[Step 14720] Loss: 2.7569\n",
            "[Step 14730] Loss: 2.3062\n",
            "[Step 14740] Loss: 2.7039\n",
            "[Step 14750] Loss: 2.4682\n",
            "[Step 14760] Loss: 3.1796\n",
            "[Step 14770] Loss: 2.8667\n",
            "[Step 14780] Loss: 2.7718\n",
            "[Step 14790] Loss: 3.0469\n",
            "[Step 14800] Loss: 2.4287\n",
            "[Step 14810] Loss: 2.4152\n",
            "[Step 14820] Loss: 2.3674\n",
            "[Step 14830] Loss: 2.2029\n",
            "[Step 14840] Loss: 2.1737\n",
            "[Step 14850] Loss: 2.3484\n",
            "[Step 14860] Loss: 2.5719\n",
            "[Step 14870] Loss: 2.2736\n",
            "[Step 14880] Loss: 2.3233\n",
            "[Step 14890] Loss: 3.1010\n",
            "[Step 14900] Loss: 2.5042\n",
            "[Step 14910] Loss: 2.1248\n",
            "[Step 14920] Loss: 2.1592\n",
            "[Step 14930] Loss: 2.6870\n",
            "[Step 14940] Loss: 3.0788\n",
            "[Step 14950] Loss: 2.5037\n",
            "[Step 14960] Loss: 3.1857\n",
            "[Step 14970] Loss: 2.9940\n",
            "[Step 14980] Loss: 2.7104\n",
            "[Step 14990] Loss: 2.8784\n",
            "[Step 15000] Loss: 2.7035\n",
            "[Step 15010] Loss: 2.6583\n",
            "[Step 15020] Loss: 2.9468\n",
            "[Step 15030] Loss: 2.6232\n",
            "[Step 15040] Loss: 2.9053\n",
            "[Step 15050] Loss: 2.5829\n",
            "[Step 15060] Loss: 2.5974\n",
            "[Step 15070] Loss: 2.5247\n",
            "[Step 15080] Loss: 2.3097\n",
            "[Step 15090] Loss: 2.5777\n",
            "[Step 15100] Loss: 2.6784\n",
            "[Step 15110] Loss: 2.6865\n",
            "[Step 15120] Loss: 2.5779\n",
            "[Step 15130] Loss: 2.6040\n",
            "[Step 15140] Loss: 3.1572\n",
            "[Step 15150] Loss: 2.6178\n",
            "ðŸ“˜ Epoch 63 - Avg Training Loss: 2.6001\n",
            "ðŸ“Š Final Validation â€” Loss: 2.7657 | Accuracy: 0.3191 | Precision: 0.3331\n",
            "[Step 15160] Loss: 2.7041\n",
            "[Step 15170] Loss: 2.2426\n",
            "[Step 15180] Loss: 2.9606\n",
            "[Step 15190] Loss: 2.3542\n",
            "[Step 15200] Loss: 2.1385\n",
            "[Step 15210] Loss: 2.2007\n",
            "[Step 15220] Loss: 2.4760\n",
            "[Step 15230] Loss: 2.0901\n",
            "[Step 15240] Loss: 2.2250\n",
            "[Step 15250] Loss: 2.3265\n",
            "[Step 15260] Loss: 2.3022\n",
            "[Step 15270] Loss: 2.5152\n",
            "[Step 15280] Loss: 2.5421\n",
            "[Step 15290] Loss: 2.5136\n",
            "[Step 15300] Loss: 2.3393\n",
            "[Step 15310] Loss: 2.3529\n",
            "[Step 15320] Loss: 2.5675\n",
            "[Step 15330] Loss: 2.3270\n",
            "[Step 15340] Loss: 2.4152\n",
            "[Step 15350] Loss: 3.0531\n",
            "[Step 15360] Loss: 2.3262\n",
            "[Step 15370] Loss: 2.6521\n",
            "[Step 15380] Loss: 2.6027\n",
            "[Step 15390] Loss: 2.3854\n",
            "[Step 15400] Loss: 2.3185\n",
            "[Step 15410] Loss: 2.8433\n",
            "[Step 15420] Loss: 2.2893\n",
            "[Step 15430] Loss: 2.6314\n",
            "[Step 15440] Loss: 2.8892\n",
            "[Step 15450] Loss: 2.2493\n",
            "[Step 15460] Loss: 2.4601\n",
            "[Step 15470] Loss: 2.5084\n",
            "[Step 15480] Loss: 2.6382\n",
            "[Step 15490] Loss: 2.2673\n",
            "[Step 15500] Loss: 1.9988\n",
            "[Step 15510] Loss: 2.6553\n",
            "[Step 15520] Loss: 1.8645\n",
            "[Step 15530] Loss: 2.3846\n",
            "[Step 15540] Loss: 2.4561\n",
            "[Step 15550] Loss: 2.6469\n",
            "[Step 15560] Loss: 2.1252\n",
            "[Step 15570] Loss: 2.5721\n",
            "[Step 15580] Loss: 2.4960\n",
            "[Step 15590] Loss: 2.8679\n",
            "[Step 15600] Loss: 2.6325\n",
            "[Step 15610] Loss: 2.3388\n",
            "[Step 15620] Loss: 3.0555\n",
            "[Step 15630] Loss: 2.4896\n",
            "[Step 15640] Loss: 2.1700\n",
            "[Step 15650] Loss: 2.7515\n",
            "[Step 15660] Loss: 2.5910\n",
            "[Step 15670] Loss: 2.1248\n",
            "[Step 15680] Loss: 2.2972\n",
            "[Step 15690] Loss: 2.2618\n",
            "[Step 15700] Loss: 2.8022\n",
            "[Step 15710] Loss: 2.7764\n",
            "[Step 15720] Loss: 3.0447\n",
            "[Step 15730] Loss: 2.5110\n",
            "[Step 15740] Loss: 2.3481\n",
            "[Step 15750] Loss: 2.1427\n",
            "[Step 15760] Loss: 2.5331\n",
            "[Step 15770] Loss: 2.9094\n",
            "[Step 15780] Loss: 2.5676\n",
            "[Step 15790] Loss: 2.6293\n",
            "[Step 15800] Loss: 2.3348\n",
            "[Step 15810] Loss: 2.5079\n",
            "[Step 15820] Loss: 2.8801\n",
            "[Step 15830] Loss: 2.7089\n",
            "[Step 15840] Loss: 2.5666\n",
            "[Step 15850] Loss: 2.1959\n",
            "[Step 15860] Loss: 2.3983\n",
            "[Step 15870] Loss: 2.6373\n",
            "[Step 15880] Loss: 2.9515\n",
            "[Step 15890] Loss: 2.1033\n",
            "[Step 15900] Loss: 2.5022\n",
            "[Step 15910] Loss: 2.5970\n",
            "[Step 15920] Loss: 2.4907\n",
            "[Step 15930] Loss: 2.6671\n",
            "[Step 15940] Loss: 2.2425\n",
            "[Step 15950] Loss: 2.4558\n",
            "[Step 15960] Loss: 2.5243\n",
            "[Step 15970] Loss: 1.9833\n",
            "[Step 15980] Loss: 2.4054\n",
            "[Step 15990] Loss: 2.3373\n",
            "[Step 16000] Loss: 3.0193\n",
            "[Step 16010] Loss: 2.1502\n",
            "[Step 16020] Loss: 2.2126\n",
            "[Step 16030] Loss: 2.7716\n",
            "[Step 16040] Loss: 2.1008\n",
            "[Step 16050] Loss: 2.7946\n",
            "[Step 16060] Loss: 2.3059\n",
            "[Step 16070] Loss: 2.8023\n",
            "[Step 16080] Loss: 2.4344\n",
            "[Step 16090] Loss: 2.5148\n",
            "[Step 16100] Loss: 2.4162\n",
            "[Step 16110] Loss: 2.1874\n",
            "[Step 16120] Loss: 2.5524\n",
            "[Step 16130] Loss: 2.6545\n",
            "[Step 16140] Loss: 2.1991\n",
            "[Step 16150] Loss: 2.4559\n",
            "[Step 16160] Loss: 1.9638\n",
            "[Step 16170] Loss: 2.6056\n",
            "[Step 16180] Loss: 2.6364\n",
            "[Step 16190] Loss: 2.6457\n",
            "[Step 16200] Loss: 2.4760\n",
            "[Step 16210] Loss: 2.4333\n",
            "[Step 16220] Loss: 2.8096\n",
            "[Step 16230] Loss: 2.5377\n",
            "[Step 16240] Loss: 2.6299\n",
            "[Step 16250] Loss: 2.2545\n",
            "[Step 16260] Loss: 2.8396\n",
            "[Step 16270] Loss: 2.5404\n",
            "[Step 16280] Loss: 2.7080\n",
            "[Step 16290] Loss: 2.4727\n",
            "[Step 16300] Loss: 3.3076\n",
            "[Step 16310] Loss: 2.6244\n",
            "[Step 16320] Loss: 1.9721\n",
            "[Step 16330] Loss: 2.8572\n",
            "[Step 16340] Loss: 2.4780\n",
            "[Step 16350] Loss: 2.7628\n",
            "[Step 16360] Loss: 2.6896\n",
            "[Step 16370] Loss: 2.7777\n",
            "[Step 16380] Loss: 2.7800\n",
            "[Step 16390] Loss: 2.4975\n",
            "[Step 16400] Loss: 2.5350\n",
            "[Step 16410] Loss: 2.8654\n",
            "[Step 16420] Loss: 2.7031\n",
            "[Step 16430] Loss: 3.1129\n",
            "[Step 16440] Loss: 2.3767\n",
            "[Step 16450] Loss: 2.4201\n",
            "[Step 16460] Loss: 2.5143\n",
            "[Step 16470] Loss: 2.9039\n",
            "[Step 16480] Loss: 2.6534\n",
            "[Step 16490] Loss: 2.4695\n",
            "[Step 16500] Loss: 2.6307\n",
            "[Step 16510] Loss: 2.2796\n",
            "[Step 16520] Loss: 2.6777\n",
            "[Step 16530] Loss: 2.8276\n",
            "[Step 16540] Loss: 2.3804\n",
            "[Step 16550] Loss: 2.8585\n",
            "[Step 16560] Loss: 2.2435\n",
            "[Step 16570] Loss: 2.2992\n",
            "[Step 16580] Loss: 2.2682\n",
            "[Step 16590] Loss: 2.7707\n",
            "[Step 16600] Loss: 2.3270\n",
            "[Step 16610] Loss: 2.8902\n",
            "[Step 16620] Loss: 2.2269\n",
            "[Step 16630] Loss: 2.7172\n",
            "[Step 16640] Loss: 3.1144\n",
            "[Step 16650] Loss: 2.8813\n",
            "[Step 16660] Loss: 2.4638\n",
            "[Step 16670] Loss: 2.6953\n",
            "[Step 16680] Loss: 2.5240\n",
            "[Step 16690] Loss: 2.7019\n",
            "[Step 16700] Loss: 2.4667\n",
            "[Step 16710] Loss: 2.4028\n",
            "[Step 16720] Loss: 2.7542\n",
            "[Step 16730] Loss: 2.3216\n",
            "[Step 16740] Loss: 2.4916\n",
            "[Step 16750] Loss: 2.6135\n",
            "[Step 16760] Loss: 2.5658\n",
            "[Step 16770] Loss: 2.2207\n",
            "[Step 16780] Loss: 2.2291\n",
            "[Step 16790] Loss: 2.3149\n",
            "[Step 16800] Loss: 2.4693\n",
            "[Step 16810] Loss: 2.2123\n",
            "[Step 16820] Loss: 2.7591\n",
            "[Step 16830] Loss: 2.7673\n",
            "[Step 16840] Loss: 2.6578\n",
            "[Step 16850] Loss: 2.4992\n",
            "[Step 16860] Loss: 2.7485\n",
            "[Step 16870] Loss: 2.3885\n",
            "[Step 16880] Loss: 2.4642\n",
            "[Step 16890] Loss: 2.9529\n",
            "[Step 16900] Loss: 2.6131\n",
            "[Step 16910] Loss: 2.2135\n",
            "[Step 16920] Loss: 2.5176\n",
            "[Step 16930] Loss: 2.9094\n",
            "[Step 16940] Loss: 2.7364\n",
            "[Step 16950] Loss: 2.7423\n",
            "[Step 16960] Loss: 2.4132\n",
            "[Step 16970] Loss: 2.8387\n",
            "[Step 16980] Loss: 2.8703\n",
            "[Step 16990] Loss: 2.5294\n",
            "[Step 17000] Loss: 2.7066\n",
            "[Step 17010] Loss: 2.4803\n",
            "[Step 17020] Loss: 2.4344\n",
            "[Step 17030] Loss: 2.4013\n",
            "[Step 17040] Loss: 2.7125\n",
            "ðŸ“˜ Epoch 64 - Avg Training Loss: 2.5264\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5682 | Accuracy: 0.3615 | Precision: 0.3463\n",
            "[Step 17050] Loss: 2.5967\n",
            "[Step 17060] Loss: 2.7447\n",
            "[Step 17070] Loss: 2.6759\n",
            "[Step 17080] Loss: 2.7675\n",
            "[Step 17090] Loss: 2.8132\n",
            "[Step 17100] Loss: 2.5399\n",
            "[Step 17110] Loss: 2.4858\n",
            "[Step 17120] Loss: 2.6560\n",
            "[Step 17130] Loss: 3.1270\n",
            "[Step 17140] Loss: 2.4689\n",
            "[Step 17150] Loss: 2.6007\n",
            "[Step 17160] Loss: 2.3284\n",
            "[Step 17170] Loss: 2.8560\n",
            "[Step 17180] Loss: 2.3340\n",
            "[Step 17190] Loss: 2.7601\n",
            "[Step 17200] Loss: 2.4784\n",
            "[Step 17210] Loss: 2.4135\n",
            "[Step 17220] Loss: 2.7348\n",
            "[Step 17230] Loss: 1.6987\n",
            "[Step 17240] Loss: 2.6572\n",
            "[Step 17250] Loss: 2.6091\n",
            "[Step 17260] Loss: 2.3839\n",
            "[Step 17270] Loss: 2.5250\n",
            "[Step 17280] Loss: 2.1796\n",
            "[Step 17290] Loss: 2.9048\n",
            "[Step 17300] Loss: 2.2923\n",
            "[Step 17310] Loss: 2.5034\n",
            "[Step 17320] Loss: 2.6269\n",
            "[Step 17330] Loss: 2.1827\n",
            "[Step 17340] Loss: 2.2724\n",
            "[Step 17350] Loss: 2.3811\n",
            "[Step 17360] Loss: 2.4146\n",
            "[Step 17370] Loss: 2.4847\n",
            "[Step 17380] Loss: 2.8739\n",
            "[Step 17390] Loss: 2.3903\n",
            "[Step 17400] Loss: 2.3054\n",
            "[Step 17410] Loss: 2.3452\n",
            "[Step 17420] Loss: 3.3495\n",
            "[Step 17430] Loss: 2.3846\n",
            "[Step 17440] Loss: 2.5459\n",
            "[Step 17450] Loss: 2.4929\n",
            "[Step 17460] Loss: 2.6569\n",
            "[Step 17470] Loss: 2.5534\n",
            "[Step 17480] Loss: 2.8129\n",
            "[Step 17490] Loss: 2.1281\n",
            "[Step 17500] Loss: 2.2169\n",
            "[Step 17510] Loss: 2.3097\n",
            "[Step 17520] Loss: 2.6373\n",
            "[Step 17530] Loss: 2.6546\n",
            "[Step 17540] Loss: 2.5142\n",
            "[Step 17550] Loss: 2.2299\n",
            "[Step 17560] Loss: 2.5694\n",
            "[Step 17570] Loss: 2.5989\n",
            "[Step 17580] Loss: 2.2884\n",
            "[Step 17590] Loss: 2.7466\n",
            "[Step 17600] Loss: 2.4529\n",
            "[Step 17610] Loss: 2.6688\n",
            "[Step 17620] Loss: 2.0921\n",
            "[Step 17630] Loss: 2.6022\n",
            "[Step 17640] Loss: 2.5859\n",
            "[Step 17650] Loss: 2.7515\n",
            "[Step 17660] Loss: 2.5383\n",
            "[Step 17670] Loss: 2.6410\n",
            "[Step 17680] Loss: 2.4228\n",
            "[Step 17690] Loss: 2.4645\n",
            "[Step 17700] Loss: 2.6454\n",
            "[Step 17710] Loss: 1.9449\n",
            "[Step 17720] Loss: 2.2900\n",
            "[Step 17730] Loss: 2.2201\n",
            "[Step 17740] Loss: 2.5927\n",
            "[Step 17750] Loss: 2.0675\n",
            "[Step 17760] Loss: 2.7303\n",
            "[Step 17770] Loss: 2.5336\n",
            "[Step 17780] Loss: 2.8717\n",
            "[Step 17790] Loss: 2.4206\n",
            "[Step 17800] Loss: 2.5479\n",
            "[Step 17810] Loss: 2.6016\n",
            "[Step 17820] Loss: 2.3998\n",
            "[Step 17830] Loss: 2.4503\n",
            "[Step 17840] Loss: 2.6525\n",
            "[Step 17850] Loss: 2.1354\n",
            "[Step 17860] Loss: 2.5709\n",
            "[Step 17870] Loss: 2.7809\n",
            "[Step 17880] Loss: 2.2572\n",
            "[Step 17890] Loss: 2.5617\n",
            "[Step 17900] Loss: 2.3731\n",
            "[Step 17910] Loss: 2.2921\n",
            "[Step 17920] Loss: 2.2848\n",
            "[Step 17930] Loss: 2.9851\n",
            "[Step 17940] Loss: 2.4017\n",
            "[Step 17950] Loss: 2.2520\n",
            "[Step 17960] Loss: 1.8630\n",
            "[Step 17970] Loss: 2.5027\n",
            "[Step 17980] Loss: 2.4622\n",
            "[Step 17990] Loss: 2.5945\n",
            "[Step 18000] Loss: 2.6088\n",
            "[Step 18010] Loss: 2.5591\n",
            "[Step 18020] Loss: 1.8707\n",
            "[Step 18030] Loss: 2.3909\n",
            "[Step 18040] Loss: 2.2403\n",
            "[Step 18050] Loss: 2.5334\n",
            "[Step 18060] Loss: 2.3819\n",
            "[Step 18070] Loss: 2.3765\n",
            "[Step 18080] Loss: 2.4118\n",
            "[Step 18090] Loss: 2.2631\n",
            "[Step 18100] Loss: 2.6537\n",
            "[Step 18110] Loss: 2.1131\n",
            "[Step 18120] Loss: 2.3297\n",
            "[Step 18130] Loss: 2.2565\n",
            "[Step 18140] Loss: 2.5018\n",
            "[Step 18150] Loss: 1.9985\n",
            "[Step 18160] Loss: 2.4826\n",
            "[Step 18170] Loss: 2.4111\n",
            "[Step 18180] Loss: 2.3166\n",
            "[Step 18190] Loss: 2.7957\n",
            "[Step 18200] Loss: 2.7569\n",
            "[Step 18210] Loss: 2.5716\n",
            "[Step 18220] Loss: 2.5069\n",
            "[Step 18230] Loss: 2.7567\n",
            "[Step 18240] Loss: 2.2328\n",
            "[Step 18250] Loss: 2.3118\n",
            "[Step 18260] Loss: 2.6607\n",
            "[Step 18270] Loss: 2.0146\n",
            "[Step 18280] Loss: 2.5803\n",
            "[Step 18290] Loss: 2.7423\n",
            "[Step 18300] Loss: 2.3195\n",
            "[Step 18310] Loss: 2.5473\n",
            "[Step 18320] Loss: 2.2901\n",
            "[Step 18330] Loss: 2.6429\n",
            "[Step 18340] Loss: 2.4721\n",
            "[Step 18350] Loss: 2.3774\n",
            "[Step 18360] Loss: 2.7623\n",
            "[Step 18370] Loss: 2.3926\n",
            "[Step 18380] Loss: 2.7004\n",
            "[Step 18390] Loss: 2.5461\n",
            "[Step 18400] Loss: 2.6334\n",
            "[Step 18410] Loss: 3.0359\n",
            "[Step 18420] Loss: 2.3537\n",
            "[Step 18430] Loss: 2.4577\n",
            "[Step 18440] Loss: 2.3631\n",
            "[Step 18450] Loss: 2.2778\n",
            "[Step 18460] Loss: 2.0166\n",
            "[Step 18470] Loss: 2.5396\n",
            "[Step 18480] Loss: 2.2111\n",
            "[Step 18490] Loss: 2.4839\n",
            "[Step 18500] Loss: 2.3058\n",
            "[Step 18510] Loss: 2.9059\n",
            "[Step 18520] Loss: 2.8240\n",
            "[Step 18530] Loss: 2.3682\n",
            "[Step 18540] Loss: 2.4085\n",
            "[Step 18550] Loss: 1.8195\n",
            "[Step 18560] Loss: 2.0238\n",
            "[Step 18570] Loss: 2.3676\n",
            "[Step 18580] Loss: 3.0209\n",
            "[Step 18590] Loss: 2.6763\n",
            "[Step 18600] Loss: 2.7972\n",
            "[Step 18610] Loss: 2.5671\n",
            "[Step 18620] Loss: 2.0596\n",
            "[Step 18630] Loss: 2.6304\n",
            "[Step 18640] Loss: 2.4604\n",
            "[Step 18650] Loss: 2.2560\n",
            "[Step 18660] Loss: 2.6262\n",
            "[Step 18670] Loss: 2.5398\n",
            "[Step 18680] Loss: 1.8979\n",
            "[Step 18690] Loss: 2.2184\n",
            "[Step 18700] Loss: 2.0391\n",
            "[Step 18710] Loss: 2.3785\n",
            "[Step 18720] Loss: 2.3410\n",
            "[Step 18730] Loss: 2.4407\n",
            "[Step 18740] Loss: 2.8271\n",
            "[Step 18750] Loss: 2.4292\n",
            "[Step 18760] Loss: 3.0114\n",
            "[Step 18770] Loss: 2.5438\n",
            "[Step 18780] Loss: 2.7767\n",
            "[Step 18790] Loss: 2.0439\n",
            "[Step 18800] Loss: 2.0654\n",
            "[Step 18810] Loss: 2.5857\n",
            "[Step 18820] Loss: 3.0050\n",
            "[Step 18830] Loss: 2.6738\n",
            "[Step 18840] Loss: 2.3758\n",
            "[Step 18850] Loss: 2.4601\n",
            "[Step 18860] Loss: 2.5348\n",
            "[Step 18870] Loss: 2.3871\n",
            "[Step 18880] Loss: 2.6011\n",
            "[Step 18890] Loss: 2.0539\n",
            "[Step 18900] Loss: 2.6739\n",
            "[Step 18910] Loss: 2.5169\n",
            "[Step 18920] Loss: 2.4745\n",
            "[Step 18930] Loss: 2.6231\n",
            "[Step 18940] Loss: 2.5770\n",
            "ðŸ“˜ Epoch 65 - Avg Training Loss: 2.4785\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5336 | Accuracy: 0.3675 | Precision: 0.3458\n",
            "âœ… Continued training complete\n"
          ]
        }
      ],
      "source": [
        "new_training_args = {\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 0.009,\n",
        "    \"num_additional_epochs\": 10,\n",
        "    \"logging_steps\": 10,\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=new_training_args[\"learning_rate\"],\n",
        "    weight_decay=new_training_args[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "starting_epoch = 55\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(starting_epoch, starting_epoch + new_training_args[\"num_additional_epochs\"]):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % new_training_args[\"logging_steps\"] == 0:\n",
        "            print(f\"[Step {global_step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"ðŸ“˜ Epoch {epoch+1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    evaluate_val(model, val_loader, criterion, device)\n",
        "\n",
        "print(\"âœ… Continued training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmoi0NAwneCd"
      },
      "outputs": [],
      "source": [
        "save_path = '/content/drive/My Drive/NexHack/model_weights_3.pth'\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=True)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model_weights_1 = MyResNet50()\n",
        "\n",
        "torch.save(model_weights_1.state_dict(), 'model_weights_3.pth')\n",
        "torch.save(model_weights_1.state_dict(), save_path)\n",
        "\n",
        "save_path = '/content/drive/My Drive/NexHack/model_full_3.pth'\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=True)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model_full_1 = MyResNet50()\n",
        "\n",
        "torch.save(model_full_1, 'model_full_3.pth')\n",
        "torch.save(model_full_1.state_dict(), save_path)\n",
        "\n",
        "save_path = '/content/drive/My Drive/NexHack/checkpoint_3.pth'\n",
        "\n",
        "checkpoint = {\n",
        "    'epoch': 40,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'training_args': training_args\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, save_path)\n",
        "\n",
        "print(\"âœ… Saved training checkpoint to Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "goSYBroNnjtS",
        "outputId": "5e951de9-9fa1-4045-f8d3-c76bccb81ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 10] Loss: 2.4200\n",
            "[Step 20] Loss: 2.1581\n",
            "[Step 30] Loss: 2.6574\n",
            "[Step 40] Loss: 2.4388\n",
            "[Step 50] Loss: 2.5414\n",
            "[Step 60] Loss: 2.4306\n",
            "[Step 70] Loss: 2.7073\n",
            "[Step 80] Loss: 2.3225\n",
            "[Step 90] Loss: 2.5070\n",
            "[Step 100] Loss: 2.6723\n",
            "[Step 110] Loss: 2.8661\n",
            "[Step 120] Loss: 2.5591\n",
            "[Step 130] Loss: 2.6330\n",
            "[Step 140] Loss: 2.5135\n",
            "[Step 150] Loss: 2.2618\n",
            "[Step 160] Loss: 2.7541\n",
            "[Step 170] Loss: 2.9095\n",
            "[Step 180] Loss: 2.4101\n",
            "[Step 190] Loss: 2.7965\n",
            "[Step 200] Loss: 2.6819\n",
            "[Step 210] Loss: 2.2223\n",
            "[Step 220] Loss: 2.6175\n",
            "[Step 230] Loss: 2.2463\n",
            "[Step 240] Loss: 2.2848\n",
            "[Step 250] Loss: 2.5888\n",
            "[Step 260] Loss: 2.3973\n",
            "[Step 270] Loss: 2.1999\n",
            "[Step 280] Loss: 2.6054\n",
            "[Step 290] Loss: 2.6306\n",
            "[Step 300] Loss: 2.7867\n",
            "[Step 310] Loss: 2.6956\n",
            "[Step 320] Loss: 2.1173\n",
            "[Step 330] Loss: 2.7090\n",
            "[Step 340] Loss: 1.9435\n",
            "[Step 350] Loss: 2.7138\n",
            "[Step 360] Loss: 2.5221\n",
            "[Step 370] Loss: 2.3594\n",
            "[Step 380] Loss: 2.3018\n",
            "[Step 390] Loss: 2.9288\n",
            "[Step 400] Loss: 2.4330\n",
            "[Step 410] Loss: 2.3757\n",
            "[Step 420] Loss: 2.9263\n",
            "[Step 430] Loss: 2.6379\n",
            "[Step 440] Loss: 2.6188\n",
            "[Step 450] Loss: 2.6244\n",
            "[Step 460] Loss: 2.8071\n",
            "[Step 470] Loss: 2.4334\n",
            "[Step 480] Loss: 2.6248\n",
            "[Step 490] Loss: 2.3868\n",
            "[Step 500] Loss: 2.5983\n",
            "[Step 510] Loss: 2.7200\n",
            "[Step 520] Loss: 2.2373\n",
            "[Step 530] Loss: 2.5496\n",
            "[Step 540] Loss: 2.6033\n",
            "[Step 550] Loss: 2.2449\n",
            "[Step 560] Loss: 2.0314\n",
            "[Step 570] Loss: 2.5961\n",
            "[Step 580] Loss: 2.7251\n",
            "[Step 590] Loss: 2.6632\n",
            "[Step 600] Loss: 2.6657\n",
            "[Step 610] Loss: 2.6311\n",
            "[Step 620] Loss: 2.3890\n",
            "[Step 630] Loss: 2.3466\n",
            "[Step 640] Loss: 3.0221\n",
            "[Step 650] Loss: 2.2458\n",
            "[Step 660] Loss: 2.6099\n",
            "[Step 670] Loss: 2.2735\n",
            "[Step 680] Loss: 2.2874\n",
            "[Step 690] Loss: 2.4637\n",
            "[Step 700] Loss: 2.1459\n",
            "[Step 710] Loss: 1.9765\n",
            "[Step 720] Loss: 2.5873\n",
            "[Step 730] Loss: 2.0444\n",
            "[Step 740] Loss: 2.8107\n",
            "[Step 750] Loss: 2.3933\n",
            "[Step 760] Loss: 2.7643\n",
            "[Step 770] Loss: 2.2914\n",
            "[Step 780] Loss: 2.3547\n",
            "[Step 790] Loss: 2.6346\n",
            "[Step 800] Loss: 2.7151\n",
            "[Step 810] Loss: 2.6641\n",
            "[Step 820] Loss: 1.9433\n",
            "[Step 830] Loss: 2.1800\n",
            "[Step 840] Loss: 2.2628\n",
            "[Step 850] Loss: 2.6693\n",
            "[Step 860] Loss: 2.5955\n",
            "[Step 870] Loss: 2.7612\n",
            "[Step 880] Loss: 2.2541\n",
            "[Step 890] Loss: 2.2555\n",
            "[Step 900] Loss: 2.5834\n",
            "[Step 910] Loss: 2.4071\n",
            "[Step 920] Loss: 2.7230\n",
            "[Step 930] Loss: 3.0983\n",
            "[Step 940] Loss: 2.1043\n",
            "[Step 950] Loss: 2.9455\n",
            "[Step 960] Loss: 3.5575\n",
            "[Step 970] Loss: 2.4702\n",
            "[Step 980] Loss: 2.5916\n",
            "[Step 990] Loss: 2.6382\n",
            "[Step 1000] Loss: 2.3177\n",
            "[Step 1010] Loss: 2.7988\n",
            "[Step 1020] Loss: 2.5305\n",
            "[Step 1030] Loss: 2.2368\n",
            "[Step 1040] Loss: 2.5781\n",
            "[Step 1050] Loss: 2.2097\n",
            "[Step 1060] Loss: 2.5710\n",
            "[Step 1070] Loss: 2.8739\n",
            "[Step 1080] Loss: 2.3241\n",
            "[Step 1090] Loss: 2.5244\n",
            "[Step 1100] Loss: 2.1150\n",
            "[Step 1110] Loss: 2.4354\n",
            "[Step 1120] Loss: 2.2199\n",
            "[Step 1130] Loss: 2.5152\n",
            "[Step 1140] Loss: 2.3005\n",
            "[Step 1150] Loss: 2.2500\n",
            "[Step 1160] Loss: 2.5440\n",
            "[Step 1170] Loss: 2.8156\n",
            "[Step 1180] Loss: 2.2216\n",
            "[Step 1190] Loss: 2.6046\n",
            "[Step 1200] Loss: 2.5625\n",
            "[Step 1210] Loss: 2.3775\n",
            "[Step 1220] Loss: 2.4286\n",
            "[Step 1230] Loss: 2.8400\n",
            "[Step 1240] Loss: 2.2031\n",
            "[Step 1250] Loss: 2.3583\n",
            "[Step 1260] Loss: 2.6600\n",
            "[Step 1270] Loss: 2.2883\n",
            "[Step 1280] Loss: 2.7292\n",
            "[Step 1290] Loss: 3.1528\n",
            "[Step 1300] Loss: 2.5977\n",
            "[Step 1310] Loss: 2.3968\n",
            "[Step 1320] Loss: 2.4103\n",
            "[Step 1330] Loss: 2.2399\n",
            "[Step 1340] Loss: 2.5577\n",
            "[Step 1350] Loss: 2.2339\n",
            "[Step 1360] Loss: 2.7988\n",
            "[Step 1370] Loss: 2.5083\n",
            "[Step 1380] Loss: 2.4868\n",
            "[Step 1390] Loss: 2.6856\n",
            "[Step 1400] Loss: 1.9583\n",
            "[Step 1410] Loss: 2.4803\n",
            "[Step 1420] Loss: 2.5004\n",
            "[Step 1430] Loss: 2.6809\n",
            "[Step 1440] Loss: 2.4736\n",
            "[Step 1450] Loss: 2.4371\n",
            "[Step 1460] Loss: 2.4951\n",
            "[Step 1470] Loss: 2.6262\n",
            "[Step 1480] Loss: 2.7760\n",
            "[Step 1490] Loss: 2.0766\n",
            "[Step 1500] Loss: 2.2261\n",
            "[Step 1510] Loss: 2.9505\n",
            "[Step 1520] Loss: 2.1446\n",
            "[Step 1530] Loss: 2.5514\n",
            "[Step 1540] Loss: 2.7321\n",
            "[Step 1550] Loss: 2.9009\n",
            "[Step 1560] Loss: 2.4296\n",
            "[Step 1570] Loss: 2.5977\n",
            "[Step 1580] Loss: 2.9350\n",
            "[Step 1590] Loss: 2.5217\n",
            "[Step 1600] Loss: 2.7836\n",
            "[Step 1610] Loss: 2.2600\n",
            "[Step 1620] Loss: 2.5841\n",
            "[Step 1630] Loss: 2.1196\n",
            "[Step 1640] Loss: 2.1866\n",
            "[Step 1650] Loss: 2.2859\n",
            "[Step 1660] Loss: 2.5280\n",
            "[Step 1670] Loss: 3.0797\n",
            "[Step 1680] Loss: 2.9798\n",
            "[Step 1690] Loss: 2.5169\n",
            "[Step 1700] Loss: 2.5269\n",
            "[Step 1710] Loss: 2.9884\n",
            "[Step 1720] Loss: 2.4617\n",
            "[Step 1730] Loss: 2.2880\n",
            "[Step 1740] Loss: 1.7767\n",
            "[Step 1750] Loss: 2.3682\n",
            "[Step 1760] Loss: 2.3079\n",
            "[Step 1770] Loss: 1.9970\n",
            "[Step 1780] Loss: 2.7645\n",
            "[Step 1790] Loss: 2.2364\n",
            "[Step 1800] Loss: 2.6597\n",
            "[Step 1810] Loss: 2.7829\n",
            "[Step 1820] Loss: 1.9377\n",
            "[Step 1830] Loss: 1.8635\n",
            "[Step 1840] Loss: 2.1261\n",
            "[Step 1850] Loss: 2.7034\n",
            "[Step 1860] Loss: 2.4445\n",
            "[Step 1870] Loss: 2.5508\n",
            "[Step 1880] Loss: 2.2876\n",
            "[Step 1890] Loss: 2.7782\n",
            "ðŸ“˜ Epoch 66 - Avg Training Loss: 2.4976\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5553 | Accuracy: 0.3636 | Precision: 0.3501\n",
            "[Step 1900] Loss: 2.2842\n",
            "[Step 1910] Loss: 2.7909\n",
            "[Step 1920] Loss: 2.3297\n",
            "[Step 1930] Loss: 2.5659\n",
            "[Step 1940] Loss: 2.7544\n",
            "[Step 1950] Loss: 2.4759\n",
            "[Step 1960] Loss: 2.8305\n",
            "[Step 1970] Loss: 2.9900\n",
            "[Step 1980] Loss: 2.1082\n",
            "[Step 1990] Loss: 2.5382\n",
            "[Step 2000] Loss: 2.1135\n",
            "[Step 2010] Loss: 2.5398\n",
            "[Step 2020] Loss: 2.3290\n",
            "[Step 2030] Loss: 2.6043\n",
            "[Step 2040] Loss: 2.6988\n",
            "[Step 2050] Loss: 2.7974\n",
            "[Step 2060] Loss: 2.1818\n",
            "[Step 2070] Loss: 2.5792\n",
            "[Step 2080] Loss: 2.3809\n",
            "[Step 2090] Loss: 2.7000\n",
            "[Step 2100] Loss: 2.3624\n",
            "[Step 2110] Loss: 2.2637\n",
            "[Step 2120] Loss: 2.8049\n",
            "[Step 2130] Loss: 2.6637\n",
            "[Step 2140] Loss: 2.1494\n",
            "[Step 2150] Loss: 2.2891\n",
            "[Step 2160] Loss: 2.4171\n",
            "[Step 2170] Loss: 2.4857\n",
            "[Step 2180] Loss: 2.6763\n",
            "[Step 2190] Loss: 2.2517\n",
            "[Step 2200] Loss: 2.4970\n",
            "[Step 2210] Loss: 2.4928\n",
            "[Step 2220] Loss: 2.8374\n",
            "[Step 2230] Loss: 2.4879\n",
            "[Step 2240] Loss: 2.1960\n",
            "[Step 2250] Loss: 2.2082\n",
            "[Step 2260] Loss: 2.4615\n",
            "[Step 2270] Loss: 2.1839\n",
            "[Step 2280] Loss: 2.6483\n",
            "[Step 2290] Loss: 2.9798\n",
            "[Step 2300] Loss: 2.4089\n",
            "[Step 2310] Loss: 2.5295\n",
            "[Step 2320] Loss: 2.8953\n",
            "[Step 2330] Loss: 2.7688\n",
            "[Step 2340] Loss: 2.4049\n",
            "[Step 2350] Loss: 3.0738\n",
            "[Step 2360] Loss: 2.1751\n",
            "[Step 2370] Loss: 2.6792\n",
            "[Step 2380] Loss: 2.7528\n",
            "[Step 2390] Loss: 2.0096\n",
            "[Step 2400] Loss: 2.5112\n",
            "[Step 2410] Loss: 2.4304\n",
            "[Step 2420] Loss: 2.1595\n",
            "[Step 2430] Loss: 2.4951\n",
            "[Step 2440] Loss: 2.2433\n",
            "[Step 2450] Loss: 2.4655\n",
            "[Step 2460] Loss: 2.2524\n",
            "[Step 2470] Loss: 2.4917\n",
            "[Step 2480] Loss: 2.5576\n",
            "[Step 2490] Loss: 2.6044\n",
            "[Step 2500] Loss: 2.5738\n",
            "[Step 2510] Loss: 2.3710\n",
            "[Step 2520] Loss: 2.5797\n",
            "[Step 2530] Loss: 2.7617\n",
            "[Step 2540] Loss: 2.5539\n",
            "[Step 2550] Loss: 1.8215\n",
            "[Step 2560] Loss: 2.4398\n",
            "[Step 2570] Loss: 2.5650\n",
            "[Step 2580] Loss: 2.2971\n",
            "[Step 2590] Loss: 2.9016\n",
            "[Step 2600] Loss: 3.2024\n",
            "[Step 2610] Loss: 2.2902\n",
            "[Step 2620] Loss: 2.7053\n",
            "[Step 2630] Loss: 2.5960\n",
            "[Step 2640] Loss: 2.7449\n",
            "[Step 2650] Loss: 2.2560\n",
            "[Step 2660] Loss: 2.0357\n",
            "[Step 2670] Loss: 2.9441\n",
            "[Step 2680] Loss: 2.3841\n",
            "[Step 2690] Loss: 2.2320\n",
            "[Step 2700] Loss: 2.1817\n",
            "[Step 2710] Loss: 2.2987\n",
            "[Step 2720] Loss: 2.4279\n",
            "[Step 2730] Loss: 2.0276\n",
            "[Step 2740] Loss: 2.4582\n",
            "[Step 2750] Loss: 2.6097\n",
            "[Step 2760] Loss: 2.7409\n",
            "[Step 2770] Loss: 2.5799\n",
            "[Step 2780] Loss: 2.5579\n",
            "[Step 2790] Loss: 2.4904\n",
            "[Step 2800] Loss: 2.0539\n",
            "[Step 2810] Loss: 2.3503\n",
            "[Step 2820] Loss: 2.5831\n",
            "[Step 2830] Loss: 2.4048\n",
            "[Step 2840] Loss: 2.3138\n",
            "[Step 2850] Loss: 2.6304\n",
            "[Step 2860] Loss: 2.4059\n",
            "[Step 2870] Loss: 2.1155\n",
            "[Step 2880] Loss: 2.7531\n",
            "[Step 2890] Loss: 2.1035\n",
            "[Step 2900] Loss: 2.1871\n",
            "[Step 2910] Loss: 2.1238\n",
            "[Step 2920] Loss: 2.9576\n",
            "[Step 2930] Loss: 2.5185\n",
            "[Step 2940] Loss: 2.4995\n",
            "[Step 2950] Loss: 2.8233\n",
            "[Step 2960] Loss: 2.9363\n",
            "[Step 2970] Loss: 2.1915\n",
            "[Step 2980] Loss: 2.8037\n",
            "[Step 2990] Loss: 2.2389\n",
            "[Step 3000] Loss: 2.4154\n",
            "[Step 3010] Loss: 2.7808\n",
            "[Step 3020] Loss: 2.1386\n",
            "[Step 3030] Loss: 2.5235\n",
            "[Step 3040] Loss: 2.2406\n",
            "[Step 3050] Loss: 2.2801\n",
            "[Step 3060] Loss: 2.6188\n",
            "[Step 3070] Loss: 1.9913\n",
            "[Step 3080] Loss: 2.5030\n",
            "[Step 3090] Loss: 1.9318\n",
            "[Step 3100] Loss: 2.2131\n",
            "[Step 3110] Loss: 2.9838\n",
            "[Step 3120] Loss: 2.5664\n",
            "[Step 3130] Loss: 2.5229\n",
            "[Step 3140] Loss: 2.2845\n",
            "[Step 3150] Loss: 2.4277\n",
            "[Step 3160] Loss: 2.9179\n",
            "[Step 3170] Loss: 2.5187\n",
            "[Step 3180] Loss: 2.6652\n",
            "[Step 3190] Loss: 2.5055\n",
            "[Step 3200] Loss: 2.3916\n",
            "[Step 3210] Loss: 2.6338\n",
            "[Step 3220] Loss: 2.1570\n",
            "[Step 3230] Loss: 2.3228\n",
            "[Step 3240] Loss: 2.3179\n",
            "[Step 3250] Loss: 2.8475\n",
            "[Step 3260] Loss: 2.4535\n",
            "[Step 3270] Loss: 2.5737\n",
            "[Step 3280] Loss: 2.3476\n",
            "[Step 3290] Loss: 2.6499\n",
            "[Step 3300] Loss: 2.8481\n",
            "[Step 3310] Loss: 2.7058\n",
            "[Step 3320] Loss: 2.0870\n",
            "[Step 3330] Loss: 2.9234\n",
            "[Step 3340] Loss: 2.6192\n",
            "[Step 3350] Loss: 2.3604\n",
            "[Step 3360] Loss: 2.3774\n",
            "[Step 3370] Loss: 2.7161\n",
            "[Step 3380] Loss: 2.5480\n",
            "[Step 3390] Loss: 2.0686\n",
            "[Step 3400] Loss: 2.2789\n",
            "[Step 3410] Loss: 2.3424\n",
            "[Step 3420] Loss: 2.1996\n",
            "[Step 3430] Loss: 2.3636\n",
            "[Step 3440] Loss: 2.2026\n",
            "[Step 3450] Loss: 2.3283\n",
            "[Step 3460] Loss: 2.6977\n",
            "[Step 3470] Loss: 2.5808\n",
            "[Step 3480] Loss: 2.4749\n",
            "[Step 3490] Loss: 2.5597\n",
            "[Step 3500] Loss: 2.9078\n",
            "[Step 3510] Loss: 2.5659\n",
            "[Step 3520] Loss: 2.9145\n",
            "[Step 3530] Loss: 2.6042\n",
            "[Step 3540] Loss: 2.4855\n",
            "[Step 3550] Loss: 2.6222\n",
            "[Step 3560] Loss: 2.8635\n",
            "[Step 3570] Loss: 2.5910\n",
            "[Step 3580] Loss: 3.2088\n",
            "[Step 3590] Loss: 2.3402\n",
            "[Step 3600] Loss: 2.4229\n",
            "[Step 3610] Loss: 2.4805\n",
            "[Step 3620] Loss: 2.1244\n",
            "[Step 3630] Loss: 2.2978\n",
            "[Step 3640] Loss: 2.6227\n",
            "[Step 3650] Loss: 2.4013\n",
            "[Step 3660] Loss: 1.9878\n",
            "[Step 3670] Loss: 2.9296\n",
            "[Step 3680] Loss: 2.3557\n",
            "[Step 3690] Loss: 2.4280\n",
            "[Step 3700] Loss: 2.3220\n",
            "[Step 3710] Loss: 2.7669\n",
            "[Step 3720] Loss: 2.7656\n",
            "[Step 3730] Loss: 2.7747\n",
            "[Step 3740] Loss: 3.0722\n",
            "[Step 3750] Loss: 2.4181\n",
            "[Step 3760] Loss: 2.4616\n",
            "[Step 3770] Loss: 2.6976\n",
            "[Step 3780] Loss: 2.9577\n",
            "ðŸ“˜ Epoch 67 - Avg Training Loss: 2.4805\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5332 | Accuracy: 0.3687 | Precision: 0.3564\n",
            "[Step 3790] Loss: 2.5754\n",
            "[Step 3800] Loss: 2.7893\n",
            "[Step 3810] Loss: 2.5677\n",
            "[Step 3820] Loss: 2.2981\n",
            "[Step 3830] Loss: 2.0516\n",
            "[Step 3840] Loss: 2.4643\n",
            "[Step 3850] Loss: 2.4595\n",
            "[Step 3860] Loss: 2.9643\n",
            "[Step 3870] Loss: 2.3700\n",
            "[Step 3880] Loss: 2.4669\n",
            "[Step 3890] Loss: 2.6356\n",
            "[Step 3900] Loss: 1.9424\n",
            "[Step 3910] Loss: 2.2161\n",
            "[Step 3920] Loss: 2.2262\n",
            "[Step 3930] Loss: 2.6866\n",
            "[Step 3940] Loss: 2.4916\n",
            "[Step 3950] Loss: 2.5772\n",
            "[Step 3960] Loss: 1.9990\n",
            "[Step 3970] Loss: 2.8589\n",
            "[Step 3980] Loss: 2.6454\n",
            "[Step 3990] Loss: 2.4076\n",
            "[Step 4000] Loss: 2.2453\n",
            "[Step 4010] Loss: 2.1834\n",
            "[Step 4020] Loss: 2.6564\n",
            "[Step 4030] Loss: 2.6290\n",
            "[Step 4040] Loss: 2.4953\n",
            "[Step 4050] Loss: 2.8445\n",
            "[Step 4060] Loss: 2.6499\n",
            "[Step 4070] Loss: 2.6874\n",
            "[Step 4080] Loss: 2.0206\n",
            "[Step 4090] Loss: 2.3703\n",
            "[Step 4100] Loss: 2.3283\n",
            "[Step 4110] Loss: 3.3445\n",
            "[Step 4120] Loss: 2.4379\n",
            "[Step 4130] Loss: 2.4002\n",
            "[Step 4140] Loss: 2.3678\n",
            "[Step 4150] Loss: 2.1649\n",
            "[Step 4160] Loss: 2.3591\n",
            "[Step 4170] Loss: 2.2203\n",
            "[Step 4180] Loss: 2.3451\n",
            "[Step 4190] Loss: 1.9351\n",
            "[Step 4200] Loss: 2.5808\n",
            "[Step 4210] Loss: 2.6859\n",
            "[Step 4220] Loss: 2.0920\n",
            "[Step 4230] Loss: 2.3959\n",
            "[Step 4240] Loss: 2.3334\n",
            "[Step 4250] Loss: 2.4701\n",
            "[Step 4260] Loss: 2.4793\n",
            "[Step 4270] Loss: 2.5458\n",
            "[Step 4280] Loss: 2.7533\n",
            "[Step 4290] Loss: 2.1320\n",
            "[Step 4300] Loss: 2.3011\n",
            "[Step 4310] Loss: 2.6217\n",
            "[Step 4320] Loss: 2.9219\n",
            "[Step 4330] Loss: 2.5343\n",
            "[Step 4340] Loss: 2.3934\n",
            "[Step 4350] Loss: 2.7612\n",
            "[Step 4360] Loss: 2.6021\n",
            "[Step 4370] Loss: 2.4315\n",
            "[Step 4380] Loss: 3.0009\n",
            "[Step 4390] Loss: 2.0393\n",
            "[Step 4400] Loss: 2.6408\n",
            "[Step 4410] Loss: 2.0068\n",
            "[Step 4420] Loss: 2.8426\n",
            "[Step 4430] Loss: 2.8596\n",
            "[Step 4440] Loss: 2.4542\n",
            "[Step 4450] Loss: 2.4226\n",
            "[Step 4460] Loss: 2.5048\n",
            "[Step 4470] Loss: 2.4196\n",
            "[Step 4480] Loss: 2.3397\n",
            "[Step 4490] Loss: 2.7107\n",
            "[Step 4500] Loss: 1.9465\n",
            "[Step 4510] Loss: 2.6975\n",
            "[Step 4520] Loss: 2.3640\n",
            "[Step 4530] Loss: 3.1668\n",
            "[Step 4540] Loss: 2.3384\n",
            "[Step 4550] Loss: 2.7400\n",
            "[Step 4560] Loss: 2.5948\n",
            "[Step 4570] Loss: 2.5992\n",
            "[Step 4580] Loss: 1.9500\n",
            "[Step 4590] Loss: 3.0884\n",
            "[Step 4600] Loss: 1.9535\n",
            "[Step 4610] Loss: 2.5597\n",
            "[Step 4620] Loss: 2.5370\n",
            "[Step 4630] Loss: 2.1726\n",
            "[Step 4640] Loss: 3.0534\n",
            "[Step 4650] Loss: 1.7520\n",
            "[Step 4660] Loss: 2.3576\n",
            "[Step 4670] Loss: 3.0264\n",
            "[Step 4680] Loss: 2.5321\n",
            "[Step 4690] Loss: 2.5415\n",
            "[Step 4700] Loss: 2.4617\n",
            "[Step 4710] Loss: 2.1184\n",
            "[Step 4720] Loss: 2.3077\n",
            "[Step 4730] Loss: 2.5499\n",
            "[Step 4740] Loss: 2.3287\n",
            "[Step 4750] Loss: 2.5380\n",
            "[Step 4760] Loss: 2.3151\n",
            "[Step 4770] Loss: 2.1398\n",
            "[Step 4780] Loss: 1.9927\n",
            "[Step 4790] Loss: 2.3235\n",
            "[Step 4800] Loss: 2.3959\n",
            "[Step 4810] Loss: 2.4831\n",
            "[Step 4820] Loss: 2.5944\n",
            "[Step 4830] Loss: 1.9629\n",
            "[Step 4840] Loss: 2.4384\n",
            "[Step 4850] Loss: 2.1063\n",
            "[Step 4860] Loss: 2.2636\n",
            "[Step 4870] Loss: 2.5730\n",
            "[Step 4880] Loss: 2.3195\n",
            "[Step 4890] Loss: 2.7130\n",
            "[Step 4900] Loss: 2.8743\n",
            "[Step 4910] Loss: 2.5566\n",
            "[Step 4920] Loss: 2.5413\n",
            "[Step 4930] Loss: 2.5575\n",
            "[Step 4940] Loss: 2.8054\n",
            "[Step 4950] Loss: 2.1357\n",
            "[Step 4960] Loss: 2.6054\n",
            "[Step 4970] Loss: 2.2084\n",
            "[Step 4980] Loss: 2.7217\n",
            "[Step 4990] Loss: 2.7069\n",
            "[Step 5000] Loss: 2.0382\n",
            "[Step 5010] Loss: 1.9527\n",
            "[Step 5020] Loss: 2.9555\n",
            "[Step 5030] Loss: 2.3206\n",
            "[Step 5040] Loss: 2.2909\n",
            "[Step 5050] Loss: 2.4611\n",
            "[Step 5060] Loss: 2.2024\n",
            "[Step 5070] Loss: 2.8152\n",
            "[Step 5080] Loss: 2.1486\n",
            "[Step 5090] Loss: 2.5504\n",
            "[Step 5100] Loss: 2.8055\n",
            "[Step 5110] Loss: 2.2628\n",
            "[Step 5120] Loss: 2.4610\n",
            "[Step 5130] Loss: 1.9301\n",
            "[Step 5140] Loss: 2.3144\n",
            "[Step 5150] Loss: 2.2704\n",
            "[Step 5160] Loss: 1.9388\n",
            "[Step 5170] Loss: 2.5160\n",
            "[Step 5180] Loss: 2.0972\n",
            "[Step 5190] Loss: 2.7665\n",
            "[Step 5200] Loss: 2.9225\n",
            "[Step 5210] Loss: 2.6620\n",
            "[Step 5220] Loss: 2.6193\n",
            "[Step 5230] Loss: 2.6487\n",
            "[Step 5240] Loss: 1.9053\n",
            "[Step 5250] Loss: 2.4499\n",
            "[Step 5260] Loss: 2.5139\n",
            "[Step 5270] Loss: 2.9569\n",
            "[Step 5280] Loss: 2.8844\n",
            "[Step 5290] Loss: 2.3696\n",
            "[Step 5300] Loss: 2.0246\n",
            "[Step 5310] Loss: 3.2803\n",
            "[Step 5320] Loss: 2.6212\n",
            "[Step 5330] Loss: 2.2278\n",
            "[Step 5340] Loss: 2.1262\n",
            "[Step 5350] Loss: 2.1608\n",
            "[Step 5360] Loss: 2.3810\n",
            "[Step 5370] Loss: 1.9637\n",
            "[Step 5380] Loss: 2.5341\n",
            "[Step 5390] Loss: 2.7680\n",
            "[Step 5400] Loss: 2.6529\n",
            "[Step 5410] Loss: 2.4423\n",
            "[Step 5420] Loss: 2.4244\n",
            "[Step 5430] Loss: 2.0841\n",
            "[Step 5440] Loss: 2.6941\n",
            "[Step 5450] Loss: 2.0294\n",
            "[Step 5460] Loss: 2.5192\n",
            "[Step 5470] Loss: 2.9099\n",
            "[Step 5480] Loss: 2.6192\n",
            "[Step 5490] Loss: 2.3534\n",
            "[Step 5500] Loss: 2.3863\n",
            "[Step 5510] Loss: 2.2514\n",
            "[Step 5520] Loss: 2.6015\n",
            "[Step 5530] Loss: 2.0921\n",
            "[Step 5540] Loss: 2.3753\n",
            "[Step 5550] Loss: 2.1876\n",
            "[Step 5560] Loss: 1.9290\n",
            "[Step 5570] Loss: 2.6949\n",
            "[Step 5580] Loss: 2.2235\n",
            "[Step 5590] Loss: 3.0411\n",
            "[Step 5600] Loss: 2.2202\n",
            "[Step 5610] Loss: 2.2703\n",
            "[Step 5620] Loss: 2.5516\n",
            "[Step 5630] Loss: 2.2034\n",
            "[Step 5640] Loss: 2.4476\n",
            "[Step 5650] Loss: 2.0802\n",
            "[Step 5660] Loss: 2.4951\n",
            "[Step 5670] Loss: 3.2808\n",
            "[Step 5680] Loss: 2.6475\n",
            "ðŸ“˜ Epoch 68 - Avg Training Loss: 2.4623\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5488 | Accuracy: 0.3651 | Precision: 0.3511\n",
            "[Step 5690] Loss: 2.4490\n",
            "[Step 5700] Loss: 2.1589\n",
            "[Step 5710] Loss: 2.7626\n",
            "[Step 5720] Loss: 2.5336\n",
            "[Step 5730] Loss: 2.6590\n",
            "[Step 5740] Loss: 2.0041\n",
            "[Step 5750] Loss: 2.4047\n",
            "[Step 5760] Loss: 2.5522\n",
            "[Step 5770] Loss: 2.4686\n",
            "[Step 5780] Loss: 2.4026\n",
            "[Step 5790] Loss: 2.6951\n",
            "[Step 5800] Loss: 2.8599\n",
            "[Step 5810] Loss: 2.7261\n",
            "[Step 5820] Loss: 2.8509\n",
            "[Step 5830] Loss: 2.4236\n",
            "[Step 5840] Loss: 2.7026\n",
            "[Step 5850] Loss: 2.3550\n",
            "[Step 5860] Loss: 2.1601\n",
            "[Step 5870] Loss: 2.2426\n",
            "[Step 5880] Loss: 1.9313\n",
            "[Step 5890] Loss: 3.5828\n",
            "[Step 5900] Loss: 2.5300\n",
            "[Step 5910] Loss: 1.9554\n",
            "[Step 5920] Loss: 2.5375\n",
            "[Step 5930] Loss: 2.5712\n",
            "[Step 5940] Loss: 2.4797\n",
            "[Step 5950] Loss: 2.4500\n",
            "[Step 5960] Loss: 2.3071\n",
            "[Step 5970] Loss: 2.3875\n",
            "[Step 5980] Loss: 2.6420\n",
            "[Step 5990] Loss: 2.4862\n",
            "[Step 6000] Loss: 2.1256\n",
            "[Step 6010] Loss: 2.6847\n",
            "[Step 6020] Loss: 2.7222\n",
            "[Step 6030] Loss: 2.0954\n",
            "[Step 6040] Loss: 2.6484\n",
            "[Step 6050] Loss: 2.4752\n",
            "[Step 6060] Loss: 2.5701\n",
            "[Step 6070] Loss: 1.8143\n",
            "[Step 6080] Loss: 2.5110\n",
            "[Step 6090] Loss: 2.3501\n",
            "[Step 6100] Loss: 2.4557\n",
            "[Step 6110] Loss: 2.3087\n",
            "[Step 6120] Loss: 2.7812\n",
            "[Step 6130] Loss: 2.4940\n",
            "[Step 6140] Loss: 2.4386\n",
            "[Step 6150] Loss: 2.7309\n",
            "[Step 6160] Loss: 2.3572\n",
            "[Step 6170] Loss: 1.7891\n",
            "[Step 6180] Loss: 2.6605\n",
            "[Step 6190] Loss: 2.5426\n",
            "[Step 6200] Loss: 2.1489\n",
            "[Step 6210] Loss: 2.5622\n",
            "[Step 6220] Loss: 2.5452\n",
            "[Step 6230] Loss: 2.3655\n",
            "[Step 6240] Loss: 2.1972\n",
            "[Step 6250] Loss: 2.3021\n",
            "[Step 6260] Loss: 2.7506\n",
            "[Step 6270] Loss: 2.6030\n",
            "[Step 6280] Loss: 2.5930\n",
            "[Step 6290] Loss: 2.6790\n",
            "[Step 6300] Loss: 2.2589\n",
            "[Step 6310] Loss: 1.8443\n",
            "[Step 6320] Loss: 2.2479\n",
            "[Step 6330] Loss: 2.2789\n",
            "[Step 6340] Loss: 2.5213\n",
            "[Step 6350] Loss: 2.1822\n",
            "[Step 6360] Loss: 2.3305\n",
            "[Step 6370] Loss: 3.0547\n",
            "[Step 6380] Loss: 2.1574\n",
            "[Step 6390] Loss: 2.4340\n",
            "[Step 6400] Loss: 2.5706\n",
            "[Step 6410] Loss: 2.7607\n",
            "[Step 6420] Loss: 2.4135\n",
            "[Step 6430] Loss: 2.0597\n",
            "[Step 6440] Loss: 2.1858\n",
            "[Step 6450] Loss: 2.7829\n",
            "[Step 6460] Loss: 2.4313\n",
            "[Step 6470] Loss: 2.6074\n",
            "[Step 6480] Loss: 2.3498\n",
            "[Step 6490] Loss: 2.6102\n",
            "[Step 6500] Loss: 2.5862\n",
            "[Step 6510] Loss: 2.0010\n",
            "[Step 6520] Loss: 2.7679\n",
            "[Step 6530] Loss: 2.7238\n",
            "[Step 6540] Loss: 2.6070\n",
            "[Step 6550] Loss: 2.4661\n",
            "[Step 6560] Loss: 3.2637\n",
            "[Step 6570] Loss: 2.1537\n",
            "[Step 6580] Loss: 2.0336\n",
            "[Step 6590] Loss: 2.1602\n",
            "[Step 6600] Loss: 3.2139\n",
            "[Step 6610] Loss: 2.3214\n",
            "[Step 6620] Loss: 2.4542\n",
            "[Step 6630] Loss: 2.4966\n",
            "[Step 6640] Loss: 3.1707\n",
            "[Step 6650] Loss: 2.4648\n",
            "[Step 6660] Loss: 2.2711\n",
            "[Step 6670] Loss: 2.2310\n",
            "[Step 6680] Loss: 2.4543\n",
            "[Step 6690] Loss: 2.2281\n",
            "[Step 6700] Loss: 2.1531\n",
            "[Step 6710] Loss: 3.3155\n",
            "[Step 6720] Loss: 2.5262\n",
            "[Step 6730] Loss: 2.2853\n",
            "[Step 6740] Loss: 2.2677\n",
            "[Step 6750] Loss: 2.9628\n",
            "[Step 6760] Loss: 2.1701\n",
            "[Step 6770] Loss: 2.1586\n",
            "[Step 6780] Loss: 2.1347\n",
            "[Step 6790] Loss: 2.2989\n",
            "[Step 6800] Loss: 2.0633\n",
            "[Step 6810] Loss: 1.9697\n",
            "[Step 6820] Loss: 2.3208\n",
            "[Step 6830] Loss: 2.4508\n",
            "[Step 6840] Loss: 2.4995\n",
            "[Step 6850] Loss: 2.5825\n",
            "[Step 6860] Loss: 2.5327\n",
            "[Step 6870] Loss: 2.9440\n",
            "[Step 6880] Loss: 2.1857\n",
            "[Step 6890] Loss: 2.6715\n",
            "[Step 6900] Loss: 2.6276\n",
            "[Step 6910] Loss: 2.0882\n",
            "[Step 6920] Loss: 2.3507\n",
            "[Step 6930] Loss: 2.2708\n",
            "[Step 6940] Loss: 2.6676\n",
            "[Step 6950] Loss: 2.5231\n",
            "[Step 6960] Loss: 1.9222\n",
            "[Step 6970] Loss: 1.9821\n",
            "[Step 6980] Loss: 2.6691\n",
            "[Step 6990] Loss: 2.5702\n",
            "[Step 7000] Loss: 2.5448\n",
            "[Step 7010] Loss: 2.1579\n",
            "[Step 7020] Loss: 2.3815\n",
            "[Step 7030] Loss: 2.0642\n",
            "[Step 7040] Loss: 3.0931\n",
            "[Step 7050] Loss: 2.2643\n",
            "[Step 7060] Loss: 2.3246\n",
            "[Step 7070] Loss: 2.2945\n",
            "[Step 7080] Loss: 2.4336\n",
            "[Step 7090] Loss: 2.5739\n",
            "[Step 7100] Loss: 2.7033\n",
            "[Step 7110] Loss: 2.4338\n",
            "[Step 7120] Loss: 2.5957\n",
            "[Step 7130] Loss: 2.5714\n",
            "[Step 7140] Loss: 2.4135\n",
            "[Step 7150] Loss: 2.3387\n",
            "[Step 7160] Loss: 2.2800\n",
            "[Step 7170] Loss: 2.0660\n",
            "[Step 7180] Loss: 2.4117\n",
            "[Step 7190] Loss: 2.3866\n",
            "[Step 7200] Loss: 2.0290\n",
            "[Step 7210] Loss: 2.7052\n",
            "[Step 7220] Loss: 2.3477\n",
            "[Step 7230] Loss: 2.3153\n",
            "[Step 7240] Loss: 2.4709\n",
            "[Step 7250] Loss: 2.7456\n",
            "[Step 7260] Loss: 3.1379\n",
            "[Step 7270] Loss: 2.6061\n",
            "[Step 7280] Loss: 2.2860\n",
            "[Step 7290] Loss: 2.7873\n",
            "[Step 7300] Loss: 2.4969\n",
            "[Step 7310] Loss: 2.3174\n",
            "[Step 7320] Loss: 2.6170\n",
            "[Step 7330] Loss: 2.0490\n",
            "[Step 7340] Loss: 1.9572\n",
            "[Step 7350] Loss: 2.6995\n",
            "[Step 7360] Loss: 2.7644\n",
            "[Step 7370] Loss: 1.8028\n",
            "[Step 7380] Loss: 2.1717\n",
            "[Step 7390] Loss: 2.1860\n",
            "[Step 7400] Loss: 2.2540\n",
            "[Step 7410] Loss: 2.7282\n",
            "[Step 7420] Loss: 1.9524\n",
            "[Step 7430] Loss: 2.9038\n",
            "[Step 7440] Loss: 2.4431\n",
            "[Step 7450] Loss: 2.3459\n",
            "[Step 7460] Loss: 2.4765\n",
            "[Step 7470] Loss: 2.3951\n",
            "[Step 7480] Loss: 2.1953\n",
            "[Step 7490] Loss: 2.4840\n",
            "[Step 7500] Loss: 1.9286\n",
            "[Step 7510] Loss: 2.6485\n",
            "[Step 7520] Loss: 2.9444\n",
            "[Step 7530] Loss: 2.4188\n",
            "[Step 7540] Loss: 2.5378\n",
            "[Step 7550] Loss: 2.4925\n",
            "[Step 7560] Loss: 2.5477\n",
            "[Step 7570] Loss: 2.7106\n",
            "ðŸ“˜ Epoch 69 - Avg Training Loss: 2.4451\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5186 | Accuracy: 0.3715 | Precision: 0.3533\n",
            "[Step 7580] Loss: 2.2056\n",
            "[Step 7590] Loss: 2.2528\n",
            "[Step 7600] Loss: 2.8128\n",
            "[Step 7610] Loss: 2.1674\n",
            "[Step 7620] Loss: 2.8339\n",
            "[Step 7630] Loss: 2.0870\n",
            "[Step 7640] Loss: 2.4912\n",
            "[Step 7650] Loss: 1.9960\n",
            "[Step 7660] Loss: 2.4231\n",
            "[Step 7670] Loss: 2.6673\n",
            "[Step 7680] Loss: 2.1432\n",
            "[Step 7690] Loss: 2.7989\n",
            "[Step 7700] Loss: 2.6089\n",
            "[Step 7710] Loss: 2.6082\n",
            "[Step 7720] Loss: 1.8704\n",
            "[Step 7730] Loss: 2.1207\n",
            "[Step 7740] Loss: 2.4163\n",
            "[Step 7750] Loss: 2.6681\n",
            "[Step 7760] Loss: 2.4209\n",
            "[Step 7770] Loss: 2.5323\n",
            "[Step 7780] Loss: 2.7669\n",
            "[Step 7790] Loss: 2.7103\n",
            "[Step 7800] Loss: 2.4567\n",
            "[Step 7810] Loss: 2.2722\n",
            "[Step 7820] Loss: 2.3219\n",
            "[Step 7830] Loss: 2.9482\n",
            "[Step 7840] Loss: 2.4072\n",
            "[Step 7850] Loss: 2.3319\n",
            "[Step 7860] Loss: 2.0685\n",
            "[Step 7870] Loss: 2.4600\n",
            "[Step 7880] Loss: 2.2095\n",
            "[Step 7890] Loss: 2.1167\n",
            "[Step 7900] Loss: 2.6239\n",
            "[Step 7910] Loss: 2.6268\n",
            "[Step 7920] Loss: 2.6883\n",
            "[Step 7930] Loss: 2.4381\n",
            "[Step 7940] Loss: 2.4808\n",
            "[Step 7950] Loss: 2.0659\n",
            "[Step 7960] Loss: 2.3365\n",
            "[Step 7970] Loss: 2.3217\n",
            "[Step 7980] Loss: 2.5739\n",
            "[Step 7990] Loss: 2.2111\n",
            "[Step 8000] Loss: 2.3811\n",
            "[Step 8010] Loss: 2.1317\n",
            "[Step 8020] Loss: 2.3095\n",
            "[Step 8030] Loss: 2.1546\n",
            "[Step 8040] Loss: 2.4496\n",
            "[Step 8050] Loss: 2.2247\n",
            "[Step 8060] Loss: 1.8579\n",
            "[Step 8070] Loss: 2.1924\n",
            "[Step 8080] Loss: 2.4153\n",
            "[Step 8090] Loss: 2.2253\n",
            "[Step 8100] Loss: 2.5070\n",
            "[Step 8110] Loss: 2.5799\n",
            "[Step 8120] Loss: 2.8539\n",
            "[Step 8130] Loss: 2.5090\n",
            "[Step 8140] Loss: 2.2126\n",
            "[Step 8150] Loss: 2.4329\n",
            "[Step 8160] Loss: 2.3949\n",
            "[Step 8170] Loss: 2.7485\n",
            "[Step 8180] Loss: 2.4003\n",
            "[Step 8190] Loss: 2.0405\n",
            "[Step 8200] Loss: 2.5334\n",
            "[Step 8210] Loss: 2.4583\n",
            "[Step 8220] Loss: 2.6026\n",
            "[Step 8230] Loss: 2.3060\n",
            "[Step 8240] Loss: 2.6153\n",
            "[Step 8250] Loss: 2.5147\n",
            "[Step 8260] Loss: 2.8287\n",
            "[Step 8270] Loss: 2.4829\n",
            "[Step 8280] Loss: 2.1479\n",
            "[Step 8290] Loss: 2.4246\n",
            "[Step 8300] Loss: 2.6322\n",
            "[Step 8310] Loss: 2.3629\n",
            "[Step 8320] Loss: 2.7070\n",
            "[Step 8330] Loss: 2.6005\n",
            "[Step 8340] Loss: 2.4621\n",
            "[Step 8350] Loss: 2.4815\n",
            "[Step 8360] Loss: 2.6608\n",
            "[Step 8370] Loss: 2.4351\n",
            "[Step 8380] Loss: 2.4061\n",
            "[Step 8390] Loss: 2.4694\n",
            "[Step 8400] Loss: 2.5647\n",
            "[Step 8410] Loss: 2.5673\n",
            "[Step 8420] Loss: 2.5054\n",
            "[Step 8430] Loss: 2.5561\n",
            "[Step 8440] Loss: 2.2233\n",
            "[Step 8450] Loss: 2.8874\n",
            "[Step 8460] Loss: 2.2865\n",
            "[Step 8470] Loss: 2.3026\n",
            "[Step 8480] Loss: 2.7158\n",
            "[Step 8490] Loss: 2.6817\n",
            "[Step 8500] Loss: 2.1269\n",
            "[Step 8510] Loss: 3.0646\n",
            "[Step 8520] Loss: 2.6823\n",
            "[Step 8530] Loss: 2.5153\n",
            "[Step 8540] Loss: 2.2022\n",
            "[Step 8550] Loss: 2.3771\n",
            "[Step 8560] Loss: 2.5874\n",
            "[Step 8570] Loss: 2.0113\n",
            "[Step 8580] Loss: 2.0738\n",
            "[Step 8590] Loss: 2.7609\n",
            "[Step 8600] Loss: 2.3775\n",
            "[Step 8610] Loss: 2.6615\n",
            "[Step 8620] Loss: 1.9450\n",
            "[Step 8630] Loss: 2.8755\n",
            "[Step 8640] Loss: 2.4240\n",
            "[Step 8650] Loss: 2.1758\n",
            "[Step 8660] Loss: 2.6603\n",
            "[Step 8670] Loss: 2.2784\n",
            "[Step 8680] Loss: 2.1407\n",
            "[Step 8690] Loss: 2.9578\n",
            "[Step 8700] Loss: 2.3815\n",
            "[Step 8710] Loss: 2.3018\n",
            "[Step 8720] Loss: 2.2586\n",
            "[Step 8730] Loss: 2.2007\n",
            "[Step 8740] Loss: 3.3592\n",
            "[Step 8750] Loss: 2.0795\n",
            "[Step 8760] Loss: 2.0328\n",
            "[Step 8770] Loss: 2.4494\n",
            "[Step 8780] Loss: 2.6031\n",
            "[Step 8790] Loss: 2.1231\n",
            "[Step 8800] Loss: 2.7079\n",
            "[Step 8810] Loss: 2.3540\n",
            "[Step 8820] Loss: 2.9737\n",
            "[Step 8830] Loss: 2.5878\n",
            "[Step 8840] Loss: 2.2470\n",
            "[Step 8850] Loss: 2.3885\n",
            "[Step 8860] Loss: 2.7748\n",
            "[Step 8870] Loss: 2.2635\n",
            "[Step 8880] Loss: 2.3074\n",
            "[Step 8890] Loss: 2.8449\n",
            "[Step 8900] Loss: 2.3999\n",
            "[Step 8910] Loss: 2.6179\n",
            "[Step 8920] Loss: 2.6388\n",
            "[Step 8930] Loss: 2.2194\n",
            "[Step 8940] Loss: 2.2888\n",
            "[Step 8950] Loss: 1.8686\n",
            "[Step 8960] Loss: 2.5454\n",
            "[Step 8970] Loss: 2.5754\n",
            "[Step 8980] Loss: 1.9277\n",
            "[Step 8990] Loss: 2.1783\n",
            "[Step 9000] Loss: 2.9053\n",
            "[Step 9010] Loss: 2.4669\n",
            "[Step 9020] Loss: 2.1174\n",
            "[Step 9030] Loss: 2.4780\n",
            "[Step 9040] Loss: 2.5298\n",
            "[Step 9050] Loss: 2.0565\n",
            "[Step 9060] Loss: 2.8833\n",
            "[Step 9070] Loss: 2.1777\n",
            "[Step 9080] Loss: 2.6548\n",
            "[Step 9090] Loss: 2.6711\n",
            "[Step 9100] Loss: 2.4975\n",
            "[Step 9110] Loss: 2.2125\n",
            "[Step 9120] Loss: 2.6348\n",
            "[Step 9130] Loss: 2.4556\n",
            "[Step 9140] Loss: 2.4442\n",
            "[Step 9150] Loss: 2.5528\n",
            "[Step 9160] Loss: 2.0697\n",
            "[Step 9170] Loss: 2.5320\n",
            "[Step 9180] Loss: 2.2338\n",
            "[Step 9190] Loss: 2.3761\n",
            "[Step 9200] Loss: 3.0238\n",
            "[Step 9210] Loss: 2.2772\n",
            "[Step 9220] Loss: 1.8122\n",
            "[Step 9230] Loss: 3.0457\n",
            "[Step 9240] Loss: 2.4370\n",
            "[Step 9250] Loss: 2.4227\n",
            "[Step 9260] Loss: 2.1604\n",
            "[Step 9270] Loss: 2.7198\n",
            "[Step 9280] Loss: 2.5312\n",
            "[Step 9290] Loss: 2.5791\n",
            "[Step 9300] Loss: 2.4618\n",
            "[Step 9310] Loss: 2.4973\n",
            "[Step 9320] Loss: 2.7219\n",
            "[Step 9330] Loss: 2.2363\n",
            "[Step 9340] Loss: 2.0373\n",
            "[Step 9350] Loss: 2.1740\n",
            "[Step 9360] Loss: 2.5347\n",
            "[Step 9370] Loss: 2.3530\n",
            "[Step 9380] Loss: 2.3778\n",
            "[Step 9390] Loss: 2.4048\n",
            "[Step 9400] Loss: 2.7201\n",
            "[Step 9410] Loss: 3.0791\n",
            "[Step 9420] Loss: 2.3550\n",
            "[Step 9430] Loss: 2.2441\n",
            "[Step 9440] Loss: 2.2660\n",
            "[Step 9450] Loss: 2.8015\n",
            "[Step 9460] Loss: 2.7621\n",
            "[Step 9470] Loss: 2.8510\n",
            "ðŸ“˜ Epoch 70 - Avg Training Loss: 2.4275\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5080 | Accuracy: 0.3719 | Precision: 0.3569\n",
            "[Step 9480] Loss: 2.7088\n",
            "[Step 9490] Loss: 2.1640\n",
            "[Step 9500] Loss: 2.2441\n",
            "[Step 9510] Loss: 2.5269\n",
            "[Step 9520] Loss: 2.2760\n",
            "[Step 9530] Loss: 2.0788\n",
            "[Step 9540] Loss: 2.1119\n",
            "[Step 9550] Loss: 1.9972\n",
            "[Step 9560] Loss: 2.3395\n",
            "[Step 9570] Loss: 2.7341\n",
            "[Step 9580] Loss: 2.0720\n",
            "[Step 9590] Loss: 2.0974\n",
            "[Step 9600] Loss: 2.7417\n",
            "[Step 9610] Loss: 2.4421\n",
            "[Step 9620] Loss: 2.7251\n",
            "[Step 9630] Loss: 2.3835\n",
            "[Step 9640] Loss: 2.7090\n",
            "[Step 9650] Loss: 2.6445\n",
            "[Step 9660] Loss: 2.4157\n",
            "[Step 9670] Loss: 2.0588\n",
            "[Step 9680] Loss: 2.0888\n",
            "[Step 9690] Loss: 2.1134\n",
            "[Step 9700] Loss: 2.8844\n",
            "[Step 9710] Loss: 2.1375\n",
            "[Step 9720] Loss: 1.8588\n",
            "[Step 9730] Loss: 2.0995\n",
            "[Step 9740] Loss: 2.4448\n",
            "[Step 9750] Loss: 2.6506\n",
            "[Step 9760] Loss: 2.1987\n",
            "[Step 9770] Loss: 2.7570\n",
            "[Step 9780] Loss: 2.1639\n",
            "[Step 9790] Loss: 2.1031\n",
            "[Step 9800] Loss: 2.5023\n",
            "[Step 9810] Loss: 2.3379\n",
            "[Step 9820] Loss: 2.4875\n",
            "[Step 9830] Loss: 2.2680\n",
            "[Step 9840] Loss: 2.6403\n",
            "[Step 9850] Loss: 2.5496\n",
            "[Step 9860] Loss: 2.5196\n",
            "[Step 9870] Loss: 2.0473\n",
            "[Step 9880] Loss: 2.4264\n",
            "[Step 9890] Loss: 2.1496\n",
            "[Step 9900] Loss: 2.7879\n",
            "[Step 9910] Loss: 1.7973\n",
            "[Step 9920] Loss: 2.7866\n",
            "[Step 9930] Loss: 2.7468\n",
            "[Step 9940] Loss: 2.7909\n",
            "[Step 9950] Loss: 2.1662\n",
            "[Step 9960] Loss: 1.9052\n",
            "[Step 9970] Loss: 2.4991\n",
            "[Step 9980] Loss: 2.3098\n",
            "[Step 9990] Loss: 2.7990\n",
            "[Step 10000] Loss: 2.3770\n",
            "[Step 10010] Loss: 2.0921\n",
            "[Step 10020] Loss: 2.3594\n",
            "[Step 10030] Loss: 2.4431\n",
            "[Step 10040] Loss: 2.5888\n",
            "[Step 10050] Loss: 2.9071\n",
            "[Step 10060] Loss: 2.2919\n",
            "[Step 10070] Loss: 1.9861\n",
            "[Step 10080] Loss: 1.9018\n",
            "[Step 10090] Loss: 2.2277\n",
            "[Step 10100] Loss: 2.2634\n",
            "[Step 10110] Loss: 2.8680\n",
            "[Step 10120] Loss: 3.0326\n",
            "[Step 10130] Loss: 2.5944\n",
            "[Step 10140] Loss: 2.4197\n",
            "[Step 10150] Loss: 2.3869\n",
            "[Step 10160] Loss: 2.5062\n",
            "[Step 10170] Loss: 2.4662\n",
            "[Step 10180] Loss: 2.2070\n",
            "[Step 10190] Loss: 1.9970\n",
            "[Step 10200] Loss: 2.3157\n",
            "[Step 10210] Loss: 2.0883\n",
            "[Step 10220] Loss: 2.4514\n",
            "[Step 10230] Loss: 2.1014\n",
            "[Step 10240] Loss: 2.8775\n",
            "[Step 10250] Loss: 2.4328\n",
            "[Step 10260] Loss: 2.3715\n",
            "[Step 10270] Loss: 2.5785\n",
            "[Step 10280] Loss: 2.2527\n",
            "[Step 10290] Loss: 2.3370\n",
            "[Step 10300] Loss: 2.3743\n",
            "[Step 10310] Loss: 2.6295\n",
            "[Step 10320] Loss: 1.9162\n",
            "[Step 10330] Loss: 1.7956\n",
            "[Step 10340] Loss: 2.3564\n",
            "[Step 10350] Loss: 2.5201\n",
            "[Step 10360] Loss: 2.3555\n",
            "[Step 10370] Loss: 2.4935\n",
            "[Step 10380] Loss: 2.1401\n",
            "[Step 10390] Loss: 2.4130\n",
            "[Step 10400] Loss: 2.7000\n",
            "[Step 10410] Loss: 2.0150\n",
            "[Step 10420] Loss: 2.2794\n",
            "[Step 10430] Loss: 2.2531\n",
            "[Step 10440] Loss: 2.3561\n",
            "[Step 10450] Loss: 2.2599\n",
            "[Step 10460] Loss: 2.1782\n",
            "[Step 10470] Loss: 2.4992\n",
            "[Step 10480] Loss: 2.1951\n",
            "[Step 10490] Loss: 2.8878\n",
            "[Step 10500] Loss: 2.3040\n",
            "[Step 10510] Loss: 2.4235\n",
            "[Step 10520] Loss: 2.3240\n",
            "[Step 10530] Loss: 2.7986\n",
            "[Step 10540] Loss: 2.5791\n",
            "[Step 10550] Loss: 2.8883\n",
            "[Step 10560] Loss: 2.1332\n",
            "[Step 10570] Loss: 2.3383\n",
            "[Step 10580] Loss: 2.1180\n",
            "[Step 10590] Loss: 2.1285\n",
            "[Step 10600] Loss: 2.3982\n",
            "[Step 10610] Loss: 2.4086\n",
            "[Step 10620] Loss: 2.3922\n",
            "[Step 10630] Loss: 1.9721\n",
            "[Step 10640] Loss: 2.0664\n",
            "[Step 10650] Loss: 2.4765\n",
            "[Step 10660] Loss: 2.1617\n",
            "[Step 10670] Loss: 2.6729\n",
            "[Step 10680] Loss: 2.0276\n",
            "[Step 10690] Loss: 2.9365\n",
            "[Step 10700] Loss: 2.5919\n",
            "[Step 10710] Loss: 2.3992\n",
            "[Step 10720] Loss: 2.6189\n",
            "[Step 10730] Loss: 1.8628\n",
            "[Step 10740] Loss: 2.8321\n",
            "[Step 10750] Loss: 2.4839\n",
            "[Step 10760] Loss: 2.1022\n",
            "[Step 10770] Loss: 2.4020\n",
            "[Step 10780] Loss: 2.3574\n",
            "[Step 10790] Loss: 2.1038\n",
            "[Step 10800] Loss: 2.0570\n",
            "[Step 10810] Loss: 2.3572\n",
            "[Step 10820] Loss: 2.4448\n",
            "[Step 10830] Loss: 2.6610\n",
            "[Step 10840] Loss: 2.6225\n",
            "[Step 10850] Loss: 2.2334\n",
            "[Step 10860] Loss: 1.9479\n",
            "[Step 10870] Loss: 2.5210\n",
            "[Step 10880] Loss: 2.0769\n",
            "[Step 10890] Loss: 2.7108\n",
            "[Step 10900] Loss: 2.6310\n",
            "[Step 10910] Loss: 2.2896\n",
            "[Step 10920] Loss: 2.4303\n",
            "[Step 10930] Loss: 2.6824\n",
            "[Step 10940] Loss: 2.4236\n",
            "[Step 10950] Loss: 2.5669\n",
            "[Step 10960] Loss: 2.4171\n",
            "[Step 10970] Loss: 2.1572\n",
            "[Step 10980] Loss: 2.3629\n",
            "[Step 10990] Loss: 2.7339\n",
            "[Step 11000] Loss: 2.3439\n",
            "[Step 11010] Loss: 3.0847\n",
            "[Step 11020] Loss: 2.6487\n",
            "[Step 11030] Loss: 2.3400\n",
            "[Step 11040] Loss: 2.7108\n",
            "[Step 11050] Loss: 2.5969\n",
            "[Step 11060] Loss: 2.6854\n",
            "[Step 11070] Loss: 2.3672\n",
            "[Step 11080] Loss: 2.5879\n",
            "[Step 11090] Loss: 2.9087\n",
            "[Step 11100] Loss: 2.3788\n",
            "[Step 11110] Loss: 2.7203\n",
            "[Step 11120] Loss: 2.4719\n",
            "[Step 11130] Loss: 2.5612\n",
            "[Step 11140] Loss: 2.2476\n",
            "[Step 11150] Loss: 2.2548\n",
            "[Step 11160] Loss: 2.1996\n",
            "[Step 11170] Loss: 2.2732\n",
            "[Step 11180] Loss: 2.3463\n",
            "[Step 11190] Loss: 2.0877\n",
            "[Step 11200] Loss: 2.7426\n",
            "[Step 11210] Loss: 2.1283\n",
            "[Step 11220] Loss: 2.2181\n",
            "[Step 11230] Loss: 2.6952\n",
            "[Step 11240] Loss: 2.2056\n",
            "[Step 11250] Loss: 2.6573\n",
            "[Step 11260] Loss: 2.1341\n",
            "[Step 11270] Loss: 2.2924\n",
            "[Step 11280] Loss: 2.1394\n",
            "[Step 11290] Loss: 2.3768\n",
            "[Step 11300] Loss: 2.8801\n",
            "[Step 11310] Loss: 2.5035\n",
            "[Step 11320] Loss: 2.4616\n",
            "[Step 11330] Loss: 2.4748\n",
            "[Step 11340] Loss: 2.3155\n",
            "[Step 11350] Loss: 2.5184\n",
            "[Step 11360] Loss: 2.2013\n",
            "ðŸ“˜ Epoch 71 - Avg Training Loss: 2.4062\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4991 | Accuracy: 0.3774 | Precision: 0.3604\n",
            "[Step 11370] Loss: 1.9810\n",
            "[Step 11380] Loss: 2.0571\n",
            "[Step 11390] Loss: 2.6873\n",
            "[Step 11400] Loss: 2.2877\n",
            "[Step 11410] Loss: 2.2495\n",
            "[Step 11420] Loss: 2.1865\n",
            "[Step 11430] Loss: 2.1373\n",
            "[Step 11440] Loss: 3.1519\n",
            "[Step 11450] Loss: 2.3813\n",
            "[Step 11460] Loss: 2.0576\n",
            "[Step 11470] Loss: 2.7022\n",
            "[Step 11480] Loss: 2.7189\n",
            "[Step 11490] Loss: 2.4786\n",
            "[Step 11500] Loss: 2.7896\n",
            "[Step 11510] Loss: 1.7428\n",
            "[Step 11520] Loss: 2.2867\n",
            "[Step 11530] Loss: 2.0556\n",
            "[Step 11540] Loss: 2.7319\n",
            "[Step 11550] Loss: 2.5798\n",
            "[Step 11560] Loss: 2.3294\n",
            "[Step 11570] Loss: 2.3287\n",
            "[Step 11580] Loss: 2.5458\n",
            "[Step 11590] Loss: 2.1220\n",
            "[Step 11600] Loss: 2.1700\n",
            "[Step 11610] Loss: 2.4371\n",
            "[Step 11620] Loss: 2.3792\n",
            "[Step 11630] Loss: 2.6011\n",
            "[Step 11640] Loss: 2.0792\n",
            "[Step 11650] Loss: 2.4156\n",
            "[Step 11660] Loss: 2.2858\n",
            "[Step 11670] Loss: 2.6848\n",
            "[Step 11680] Loss: 2.1751\n",
            "[Step 11690] Loss: 2.4171\n",
            "[Step 11700] Loss: 1.9773\n",
            "[Step 11710] Loss: 2.8090\n",
            "[Step 11720] Loss: 2.5696\n",
            "[Step 11730] Loss: 2.0183\n",
            "[Step 11740] Loss: 2.1831\n",
            "[Step 11750] Loss: 2.8715\n",
            "[Step 11760] Loss: 2.2148\n",
            "[Step 11770] Loss: 2.0305\n",
            "[Step 11780] Loss: 3.1560\n",
            "[Step 11790] Loss: 2.5461\n",
            "[Step 11800] Loss: 2.5984\n",
            "[Step 11810] Loss: 2.5839\n",
            "[Step 11820] Loss: 2.4623\n",
            "[Step 11830] Loss: 2.3239\n",
            "[Step 11840] Loss: 2.6168\n",
            "[Step 11850] Loss: 2.6595\n",
            "[Step 11860] Loss: 2.6402\n",
            "[Step 11870] Loss: 2.5835\n",
            "[Step 11880] Loss: 3.4740\n",
            "[Step 11890] Loss: 2.1016\n",
            "[Step 11900] Loss: 2.8236\n",
            "[Step 11910] Loss: 2.3589\n",
            "[Step 11920] Loss: 2.4397\n",
            "[Step 11930] Loss: 2.2786\n",
            "[Step 11940] Loss: 2.3950\n",
            "[Step 11950] Loss: 2.2679\n",
            "[Step 11960] Loss: 2.3869\n",
            "[Step 11970] Loss: 2.4756\n",
            "[Step 11980] Loss: 2.2870\n",
            "[Step 11990] Loss: 2.3262\n",
            "[Step 12000] Loss: 2.8733\n",
            "[Step 12010] Loss: 2.6902\n",
            "[Step 12020] Loss: 2.1437\n",
            "[Step 12030] Loss: 2.5060\n",
            "[Step 12040] Loss: 2.2693\n",
            "[Step 12050] Loss: 2.3131\n",
            "[Step 12060] Loss: 2.4764\n",
            "[Step 12070] Loss: 2.1509\n",
            "[Step 12080] Loss: 2.5690\n",
            "[Step 12090] Loss: 2.3290\n",
            "[Step 12100] Loss: 2.2950\n",
            "[Step 12110] Loss: 2.2894\n",
            "[Step 12120] Loss: 1.9792\n",
            "[Step 12130] Loss: 2.4651\n",
            "[Step 12140] Loss: 2.0621\n",
            "[Step 12150] Loss: 2.5610\n",
            "[Step 12160] Loss: 2.1001\n",
            "[Step 12170] Loss: 2.3704\n",
            "[Step 12180] Loss: 2.4947\n",
            "[Step 12190] Loss: 2.3619\n",
            "[Step 12200] Loss: 2.4341\n",
            "[Step 12210] Loss: 2.3958\n",
            "[Step 12220] Loss: 2.7616\n",
            "[Step 12230] Loss: 2.5570\n",
            "[Step 12240] Loss: 2.5491\n",
            "[Step 12250] Loss: 2.0197\n",
            "[Step 12260] Loss: 1.7717\n",
            "[Step 12270] Loss: 2.1958\n",
            "[Step 12280] Loss: 3.1322\n",
            "[Step 12290] Loss: 2.4294\n",
            "[Step 12300] Loss: 2.1698\n",
            "[Step 12310] Loss: 2.5671\n",
            "[Step 12320] Loss: 2.4370\n",
            "[Step 12330] Loss: 1.7724\n",
            "[Step 12340] Loss: 2.2544\n",
            "[Step 12350] Loss: 2.7079\n",
            "[Step 12360] Loss: 2.4104\n",
            "[Step 12370] Loss: 2.3856\n",
            "[Step 12380] Loss: 2.3919\n",
            "[Step 12390] Loss: 2.7086\n",
            "[Step 12400] Loss: 2.9048\n",
            "[Step 12410] Loss: 2.0185\n",
            "[Step 12420] Loss: 2.0518\n",
            "[Step 12430] Loss: 2.0407\n",
            "[Step 12440] Loss: 2.1717\n",
            "[Step 12450] Loss: 2.4808\n",
            "[Step 12460] Loss: 2.4705\n",
            "[Step 12470] Loss: 2.5285\n",
            "[Step 12480] Loss: 2.4484\n",
            "[Step 12490] Loss: 2.2723\n",
            "[Step 12500] Loss: 2.5087\n",
            "[Step 12510] Loss: 2.2634\n",
            "[Step 12520] Loss: 2.6068\n",
            "[Step 12530] Loss: 2.4573\n",
            "[Step 12540] Loss: 2.4336\n",
            "[Step 12550] Loss: 2.2677\n",
            "[Step 12560] Loss: 2.7747\n",
            "[Step 12570] Loss: 2.2774\n",
            "[Step 12580] Loss: 1.9674\n",
            "[Step 12590] Loss: 2.5368\n",
            "[Step 12600] Loss: 1.9717\n",
            "[Step 12610] Loss: 2.5397\n",
            "[Step 12620] Loss: 2.0822\n",
            "[Step 12630] Loss: 2.7052\n",
            "[Step 12640] Loss: 2.5510\n",
            "[Step 12650] Loss: 2.1069\n",
            "[Step 12660] Loss: 2.6125\n",
            "[Step 12670] Loss: 2.3813\n",
            "[Step 12680] Loss: 2.3836\n",
            "[Step 12690] Loss: 2.4346\n",
            "[Step 12700] Loss: 2.5969\n",
            "[Step 12710] Loss: 2.2024\n",
            "[Step 12720] Loss: 2.3387\n",
            "[Step 12730] Loss: 2.3882\n",
            "[Step 12740] Loss: 2.4928\n",
            "[Step 12750] Loss: 2.7520\n",
            "[Step 12760] Loss: 2.3649\n",
            "[Step 12770] Loss: 2.4861\n",
            "[Step 12780] Loss: 2.5452\n",
            "[Step 12790] Loss: 2.1641\n",
            "[Step 12800] Loss: 2.4954\n",
            "[Step 12810] Loss: 2.4685\n",
            "[Step 12820] Loss: 2.7710\n",
            "[Step 12830] Loss: 2.0048\n",
            "[Step 12840] Loss: 2.3718\n",
            "[Step 12850] Loss: 2.2990\n",
            "[Step 12860] Loss: 2.5246\n",
            "[Step 12870] Loss: 1.8754\n",
            "[Step 12880] Loss: 2.2294\n",
            "[Step 12890] Loss: 2.9430\n",
            "[Step 12900] Loss: 3.0555\n",
            "[Step 12910] Loss: 2.2773\n",
            "[Step 12920] Loss: 2.3032\n",
            "[Step 12930] Loss: 2.4033\n",
            "[Step 12940] Loss: 2.4343\n",
            "[Step 12950] Loss: 2.5539\n",
            "[Step 12960] Loss: 2.4134\n",
            "[Step 12970] Loss: 2.4916\n",
            "[Step 12980] Loss: 2.5926\n",
            "[Step 12990] Loss: 2.6348\n",
            "[Step 13000] Loss: 2.7040\n",
            "[Step 13010] Loss: 2.6233\n",
            "[Step 13020] Loss: 2.1618\n",
            "[Step 13030] Loss: 2.2463\n",
            "[Step 13040] Loss: 2.0769\n",
            "[Step 13050] Loss: 2.2645\n",
            "[Step 13060] Loss: 2.4712\n",
            "[Step 13070] Loss: 2.5636\n",
            "[Step 13080] Loss: 2.4359\n",
            "[Step 13090] Loss: 2.5043\n",
            "[Step 13100] Loss: 2.3711\n",
            "[Step 13110] Loss: 1.7617\n",
            "[Step 13120] Loss: 2.1407\n",
            "[Step 13130] Loss: 2.3930\n",
            "[Step 13140] Loss: 2.6236\n",
            "[Step 13150] Loss: 2.2083\n",
            "[Step 13160] Loss: 2.0013\n",
            "[Step 13170] Loss: 2.1603\n",
            "[Step 13180] Loss: 3.0013\n",
            "[Step 13190] Loss: 2.4289\n",
            "[Step 13200] Loss: 2.6887\n",
            "[Step 13210] Loss: 2.3182\n",
            "[Step 13220] Loss: 2.6263\n",
            "[Step 13230] Loss: 2.3310\n",
            "[Step 13240] Loss: 2.2707\n",
            "[Step 13250] Loss: 2.1967\n",
            "ðŸ“˜ Epoch 72 - Avg Training Loss: 2.3892\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4903 | Accuracy: 0.3760 | Precision: 0.3548\n",
            "[Step 13260] Loss: 2.1091\n",
            "[Step 13270] Loss: 2.1074\n",
            "[Step 13280] Loss: 2.8360\n",
            "[Step 13290] Loss: 2.7861\n",
            "[Step 13300] Loss: 2.1056\n",
            "[Step 13310] Loss: 2.6094\n",
            "[Step 13320] Loss: 2.4978\n",
            "[Step 13330] Loss: 2.4448\n",
            "[Step 13340] Loss: 2.5868\n",
            "[Step 13350] Loss: 2.5194\n",
            "[Step 13360] Loss: 2.5198\n",
            "[Step 13370] Loss: 2.2073\n",
            "[Step 13380] Loss: 2.5184\n",
            "[Step 13390] Loss: 2.5058\n",
            "[Step 13400] Loss: 2.3481\n",
            "[Step 13410] Loss: 2.3315\n",
            "[Step 13420] Loss: 2.1877\n",
            "[Step 13430] Loss: 2.5173\n",
            "[Step 13440] Loss: 2.5303\n",
            "[Step 13450] Loss: 2.3433\n",
            "[Step 13460] Loss: 2.6175\n",
            "[Step 13470] Loss: 2.8119\n",
            "[Step 13480] Loss: 2.3294\n",
            "[Step 13490] Loss: 1.9635\n",
            "[Step 13500] Loss: 2.3777\n",
            "[Step 13510] Loss: 3.1500\n",
            "[Step 13520] Loss: 2.1774\n",
            "[Step 13530] Loss: 2.3041\n",
            "[Step 13540] Loss: 2.7007\n",
            "[Step 13550] Loss: 2.2171\n",
            "[Step 13560] Loss: 2.5202\n",
            "[Step 13570] Loss: 2.5074\n",
            "[Step 13580] Loss: 1.9769\n",
            "[Step 13590] Loss: 2.2646\n",
            "[Step 13600] Loss: 2.3247\n",
            "[Step 13610] Loss: 2.1152\n",
            "[Step 13620] Loss: 2.3892\n",
            "[Step 13630] Loss: 2.0580\n",
            "[Step 13640] Loss: 3.1630\n",
            "[Step 13650] Loss: 2.4499\n",
            "[Step 13660] Loss: 2.4126\n",
            "[Step 13670] Loss: 2.6036\n",
            "[Step 13680] Loss: 2.5052\n",
            "[Step 13690] Loss: 2.3078\n",
            "[Step 13700] Loss: 1.9609\n",
            "[Step 13710] Loss: 2.4956\n",
            "[Step 13720] Loss: 2.2512\n",
            "[Step 13730] Loss: 2.3927\n",
            "[Step 13740] Loss: 2.7776\n",
            "[Step 13750] Loss: 2.5501\n",
            "[Step 13760] Loss: 2.2584\n",
            "[Step 13770] Loss: 2.2294\n",
            "[Step 13780] Loss: 2.0069\n",
            "[Step 13790] Loss: 1.9006\n",
            "[Step 13800] Loss: 2.3571\n",
            "[Step 13810] Loss: 2.6480\n",
            "[Step 13820] Loss: 2.6243\n",
            "[Step 13830] Loss: 3.3012\n",
            "[Step 13840] Loss: 2.3773\n",
            "[Step 13850] Loss: 2.4884\n",
            "[Step 13860] Loss: 2.5164\n",
            "[Step 13870] Loss: 2.3674\n",
            "[Step 13880] Loss: 2.0926\n",
            "[Step 13890] Loss: 2.4394\n",
            "[Step 13900] Loss: 2.7257\n",
            "[Step 13910] Loss: 2.2711\n",
            "[Step 13920] Loss: 2.0694\n",
            "[Step 13930] Loss: 2.3096\n",
            "[Step 13940] Loss: 1.9195\n",
            "[Step 13950] Loss: 1.9516\n",
            "[Step 13960] Loss: 2.7462\n",
            "[Step 13970] Loss: 2.7398\n",
            "[Step 13980] Loss: 2.0525\n",
            "[Step 13990] Loss: 2.2164\n",
            "[Step 14000] Loss: 2.1573\n",
            "[Step 14010] Loss: 2.7416\n",
            "[Step 14020] Loss: 2.3407\n",
            "[Step 14030] Loss: 2.6254\n",
            "[Step 14040] Loss: 2.0654\n",
            "[Step 14050] Loss: 2.3365\n",
            "[Step 14060] Loss: 2.2859\n",
            "[Step 14070] Loss: 2.8085\n",
            "[Step 14080] Loss: 2.2171\n",
            "[Step 14090] Loss: 2.3637\n",
            "[Step 14100] Loss: 2.4395\n",
            "[Step 14110] Loss: 2.3979\n",
            "[Step 14120] Loss: 2.4542\n",
            "[Step 14130] Loss: 2.9885\n",
            "[Step 14140] Loss: 2.1357\n",
            "[Step 14150] Loss: 2.2043\n",
            "[Step 14160] Loss: 2.3534\n",
            "[Step 14170] Loss: 2.2531\n",
            "[Step 14180] Loss: 2.1908\n",
            "[Step 14190] Loss: 2.9604\n",
            "[Step 14200] Loss: 2.3681\n",
            "[Step 14210] Loss: 2.2513\n",
            "[Step 14220] Loss: 1.9552\n",
            "[Step 14230] Loss: 1.8433\n",
            "[Step 14240] Loss: 2.2888\n",
            "[Step 14250] Loss: 2.5360\n",
            "[Step 14260] Loss: 2.5677\n",
            "[Step 14270] Loss: 2.5561\n",
            "[Step 14280] Loss: 1.8262\n",
            "[Step 14290] Loss: 2.6727\n",
            "[Step 14300] Loss: 1.7183\n",
            "[Step 14310] Loss: 1.9626\n",
            "[Step 14320] Loss: 2.4250\n",
            "[Step 14330] Loss: 2.3686\n",
            "[Step 14340] Loss: 1.9038\n",
            "[Step 14350] Loss: 2.5493\n",
            "[Step 14360] Loss: 2.8435\n",
            "[Step 14370] Loss: 2.1786\n",
            "[Step 14380] Loss: 2.4641\n",
            "[Step 14390] Loss: 2.2350\n",
            "[Step 14400] Loss: 2.7599\n",
            "[Step 14410] Loss: 2.4009\n",
            "[Step 14420] Loss: 2.2987\n",
            "[Step 14430] Loss: 3.0040\n",
            "[Step 14440] Loss: 2.6104\n",
            "[Step 14450] Loss: 2.4498\n",
            "[Step 14460] Loss: 2.4367\n",
            "[Step 14470] Loss: 2.2694\n",
            "[Step 14480] Loss: 2.1310\n",
            "[Step 14490] Loss: 2.0606\n",
            "[Step 14500] Loss: 2.5476\n",
            "[Step 14510] Loss: 2.2330\n",
            "[Step 14520] Loss: 2.2266\n",
            "[Step 14530] Loss: 2.5732\n",
            "[Step 14540] Loss: 2.2496\n",
            "[Step 14550] Loss: 2.0893\n",
            "[Step 14560] Loss: 2.4395\n",
            "[Step 14570] Loss: 2.3938\n",
            "[Step 14580] Loss: 1.9166\n",
            "[Step 14590] Loss: 2.5445\n",
            "[Step 14600] Loss: 2.7646\n",
            "[Step 14610] Loss: 2.3524\n",
            "[Step 14620] Loss: 2.3450\n",
            "[Step 14630] Loss: 2.6078\n",
            "[Step 14640] Loss: 2.5062\n",
            "[Step 14650] Loss: 2.1661\n",
            "[Step 14660] Loss: 2.3323\n",
            "[Step 14670] Loss: 2.3947\n",
            "[Step 14680] Loss: 1.6988\n",
            "[Step 14690] Loss: 2.0058\n",
            "[Step 14700] Loss: 2.2014\n",
            "[Step 14710] Loss: 2.1167\n",
            "[Step 14720] Loss: 2.4770\n",
            "[Step 14730] Loss: 2.3126\n",
            "[Step 14740] Loss: 1.9829\n",
            "[Step 14750] Loss: 2.5016\n",
            "[Step 14760] Loss: 2.3684\n",
            "[Step 14770] Loss: 2.2879\n",
            "[Step 14780] Loss: 2.1701\n",
            "[Step 14790] Loss: 2.6201\n",
            "[Step 14800] Loss: 1.9000\n",
            "[Step 14810] Loss: 1.9938\n",
            "[Step 14820] Loss: 2.4129\n",
            "[Step 14830] Loss: 1.7914\n",
            "[Step 14840] Loss: 2.1885\n",
            "[Step 14850] Loss: 2.1060\n",
            "[Step 14860] Loss: 2.7936\n",
            "[Step 14870] Loss: 1.9498\n",
            "[Step 14880] Loss: 2.8302\n",
            "[Step 14890] Loss: 2.3782\n",
            "[Step 14900] Loss: 2.2755\n",
            "[Step 14910] Loss: 2.3554\n",
            "[Step 14920] Loss: 2.8727\n",
            "[Step 14930] Loss: 2.7975\n",
            "[Step 14940] Loss: 2.0194\n",
            "[Step 14950] Loss: 2.2188\n",
            "[Step 14960] Loss: 2.3680\n",
            "[Step 14970] Loss: 2.6610\n",
            "[Step 14980] Loss: 2.0677\n",
            "[Step 14990] Loss: 2.4135\n",
            "[Step 15000] Loss: 2.6685\n",
            "[Step 15010] Loss: 2.7337\n",
            "[Step 15020] Loss: 2.2758\n",
            "[Step 15030] Loss: 3.0471\n",
            "[Step 15040] Loss: 2.4855\n",
            "[Step 15050] Loss: 1.9729\n",
            "[Step 15060] Loss: 2.0713\n",
            "[Step 15070] Loss: 2.2136\n",
            "[Step 15080] Loss: 2.4026\n",
            "[Step 15090] Loss: 1.8976\n",
            "[Step 15100] Loss: 2.6998\n",
            "[Step 15110] Loss: 2.2379\n",
            "[Step 15120] Loss: 2.0732\n",
            "[Step 15130] Loss: 3.0810\n",
            "[Step 15140] Loss: 2.2461\n",
            "[Step 15150] Loss: 2.5331\n",
            "ðŸ“˜ Epoch 73 - Avg Training Loss: 2.3774\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4831 | Accuracy: 0.3762 | Precision: 0.3560\n",
            "[Step 15160] Loss: 2.1502\n",
            "[Step 15170] Loss: 2.4799\n",
            "[Step 15180] Loss: 2.3447\n",
            "[Step 15190] Loss: 2.2347\n",
            "[Step 15200] Loss: 2.2855\n",
            "[Step 15210] Loss: 2.2260\n",
            "[Step 15220] Loss: 2.1207\n",
            "[Step 15230] Loss: 2.2512\n",
            "[Step 15240] Loss: 2.5814\n",
            "[Step 15250] Loss: 1.8897\n",
            "[Step 15260] Loss: 2.3852\n",
            "[Step 15270] Loss: 2.5251\n",
            "[Step 15280] Loss: 2.5516\n",
            "[Step 15290] Loss: 2.5236\n",
            "[Step 15300] Loss: 1.9052\n",
            "[Step 15310] Loss: 2.6247\n",
            "[Step 15320] Loss: 2.0716\n",
            "[Step 15330] Loss: 2.4910\n",
            "[Step 15340] Loss: 2.2019\n",
            "[Step 15350] Loss: 2.2309\n",
            "[Step 15360] Loss: 1.8941\n",
            "[Step 15370] Loss: 2.0228\n",
            "[Step 15380] Loss: 2.3046\n",
            "[Step 15390] Loss: 2.0537\n",
            "[Step 15400] Loss: 2.7012\n",
            "[Step 15410] Loss: 2.0907\n",
            "[Step 15420] Loss: 2.3800\n",
            "[Step 15430] Loss: 2.7730\n",
            "[Step 15440] Loss: 2.8573\n",
            "[Step 15450] Loss: 2.5723\n",
            "[Step 15460] Loss: 2.4842\n",
            "[Step 15470] Loss: 2.2889\n",
            "[Step 15480] Loss: 2.2967\n",
            "[Step 15490] Loss: 2.4174\n",
            "[Step 15500] Loss: 3.2249\n",
            "[Step 15510] Loss: 2.4696\n",
            "[Step 15520] Loss: 3.0387\n",
            "[Step 15530] Loss: 2.1677\n",
            "[Step 15540] Loss: 2.4034\n",
            "[Step 15550] Loss: 2.3736\n",
            "[Step 15560] Loss: 2.6665\n",
            "[Step 15570] Loss: 2.5145\n",
            "[Step 15580] Loss: 2.5318\n",
            "[Step 15590] Loss: 2.3075\n",
            "[Step 15600] Loss: 2.0240\n",
            "[Step 15610] Loss: 2.1626\n",
            "[Step 15620] Loss: 2.2603\n",
            "[Step 15630] Loss: 2.1247\n",
            "[Step 15640] Loss: 2.4606\n",
            "[Step 15650] Loss: 2.3625\n",
            "[Step 15660] Loss: 2.3400\n",
            "[Step 15670] Loss: 2.5456\n",
            "[Step 15680] Loss: 2.4011\n",
            "[Step 15690] Loss: 2.2933\n",
            "[Step 15700] Loss: 1.5355\n",
            "[Step 15710] Loss: 2.2263\n",
            "[Step 15720] Loss: 2.5993\n",
            "[Step 15730] Loss: 2.2985\n",
            "[Step 15740] Loss: 2.6726\n",
            "[Step 15750] Loss: 2.8245\n",
            "[Step 15760] Loss: 2.1047\n",
            "[Step 15770] Loss: 2.3472\n",
            "[Step 15780] Loss: 2.2578\n",
            "[Step 15790] Loss: 2.2990\n",
            "[Step 15800] Loss: 2.0256\n",
            "[Step 15810] Loss: 2.2274\n",
            "[Step 15820] Loss: 2.2726\n",
            "[Step 15830] Loss: 1.9972\n",
            "[Step 15840] Loss: 2.8283\n",
            "[Step 15850] Loss: 1.9623\n",
            "[Step 15860] Loss: 2.4988\n",
            "[Step 15870] Loss: 2.2863\n",
            "[Step 15880] Loss: 1.9785\n",
            "[Step 15890] Loss: 2.4143\n",
            "[Step 15900] Loss: 2.2473\n",
            "[Step 15910] Loss: 3.3951\n",
            "[Step 15920] Loss: 2.3104\n",
            "[Step 15930] Loss: 2.3775\n",
            "[Step 15940] Loss: 1.8587\n",
            "[Step 15950] Loss: 1.9683\n",
            "[Step 15960] Loss: 2.5741\n",
            "[Step 15970] Loss: 2.2315\n",
            "[Step 15980] Loss: 1.9993\n",
            "[Step 15990] Loss: 1.9297\n",
            "[Step 16000] Loss: 2.3172\n",
            "[Step 16010] Loss: 2.1797\n",
            "[Step 16020] Loss: 2.1360\n",
            "[Step 16030] Loss: 2.0904\n",
            "[Step 16040] Loss: 1.9900\n",
            "[Step 16050] Loss: 2.5810\n",
            "[Step 16060] Loss: 2.4662\n",
            "[Step 16070] Loss: 2.1382\n",
            "[Step 16080] Loss: 2.7428\n",
            "[Step 16090] Loss: 2.3954\n",
            "[Step 16100] Loss: 2.4784\n",
            "[Step 16110] Loss: 2.9937\n",
            "[Step 16120] Loss: 2.2407\n",
            "[Step 16130] Loss: 2.4774\n",
            "[Step 16140] Loss: 2.0417\n",
            "[Step 16150] Loss: 2.4688\n",
            "[Step 16160] Loss: 2.5036\n",
            "[Step 16170] Loss: 1.9640\n",
            "[Step 16180] Loss: 2.2691\n",
            "[Step 16190] Loss: 2.2659\n",
            "[Step 16200] Loss: 2.5215\n",
            "[Step 16210] Loss: 2.4103\n",
            "[Step 16220] Loss: 2.4573\n",
            "[Step 16230] Loss: 2.4869\n",
            "[Step 16240] Loss: 2.5965\n",
            "[Step 16250] Loss: 2.2462\n",
            "[Step 16260] Loss: 3.0094\n",
            "[Step 16270] Loss: 2.5054\n",
            "[Step 16280] Loss: 2.7306\n",
            "[Step 16290] Loss: 2.6311\n",
            "[Step 16300] Loss: 2.0462\n",
            "[Step 16310] Loss: 2.2795\n",
            "[Step 16320] Loss: 2.3334\n",
            "[Step 16330] Loss: 2.3615\n",
            "[Step 16340] Loss: 2.6767\n",
            "[Step 16350] Loss: 2.2161\n",
            "[Step 16360] Loss: 2.3918\n",
            "[Step 16370] Loss: 2.5259\n",
            "[Step 16380] Loss: 2.0133\n",
            "[Step 16390] Loss: 2.3507\n",
            "[Step 16400] Loss: 2.8395\n",
            "[Step 16410] Loss: 2.3805\n",
            "[Step 16420] Loss: 2.2276\n",
            "[Step 16430] Loss: 2.5598\n",
            "[Step 16440] Loss: 2.4875\n",
            "[Step 16450] Loss: 2.3920\n",
            "[Step 16460] Loss: 2.0541\n",
            "[Step 16470] Loss: 2.3465\n",
            "[Step 16480] Loss: 2.3766\n",
            "[Step 16490] Loss: 2.3170\n",
            "[Step 16500] Loss: 2.5107\n",
            "[Step 16510] Loss: 2.3127\n",
            "[Step 16520] Loss: 2.3966\n",
            "[Step 16530] Loss: 2.7902\n",
            "[Step 16540] Loss: 2.6376\n",
            "[Step 16550] Loss: 2.1714\n",
            "[Step 16560] Loss: 2.2135\n",
            "[Step 16570] Loss: 2.4087\n",
            "[Step 16580] Loss: 2.3025\n",
            "[Step 16590] Loss: 2.4109\n",
            "[Step 16600] Loss: 2.3013\n",
            "[Step 16610] Loss: 2.0326\n",
            "[Step 16620] Loss: 2.4943\n",
            "[Step 16630] Loss: 2.3573\n",
            "[Step 16640] Loss: 2.3463\n",
            "[Step 16650] Loss: 2.4360\n",
            "[Step 16660] Loss: 2.3054\n",
            "[Step 16670] Loss: 2.6156\n",
            "[Step 16680] Loss: 2.7874\n",
            "[Step 16690] Loss: 2.1233\n",
            "[Step 16700] Loss: 2.5098\n",
            "[Step 16710] Loss: 2.5462\n",
            "[Step 16720] Loss: 2.4229\n",
            "[Step 16730] Loss: 2.2830\n",
            "[Step 16740] Loss: 2.0988\n",
            "[Step 16750] Loss: 2.5897\n",
            "[Step 16760] Loss: 1.9581\n",
            "[Step 16770] Loss: 1.9454\n",
            "[Step 16780] Loss: 1.9291\n",
            "[Step 16790] Loss: 2.3710\n",
            "[Step 16800] Loss: 2.3619\n",
            "[Step 16810] Loss: 2.7767\n",
            "[Step 16820] Loss: 2.9835\n",
            "[Step 16830] Loss: 1.6865\n",
            "[Step 16840] Loss: 2.3801\n",
            "[Step 16850] Loss: 2.2573\n",
            "[Step 16860] Loss: 2.1801\n",
            "[Step 16870] Loss: 2.9325\n",
            "[Step 16880] Loss: 2.3223\n",
            "[Step 16890] Loss: 2.4608\n",
            "[Step 16900] Loss: 1.9587\n",
            "[Step 16910] Loss: 2.5692\n",
            "[Step 16920] Loss: 2.0566\n",
            "[Step 16930] Loss: 2.1324\n",
            "[Step 16940] Loss: 2.1281\n",
            "[Step 16950] Loss: 2.2655\n",
            "[Step 16960] Loss: 2.6149\n",
            "[Step 16970] Loss: 2.3777\n",
            "[Step 16980] Loss: 2.4264\n",
            "[Step 16990] Loss: 1.9950\n",
            "[Step 17000] Loss: 2.3628\n",
            "[Step 17010] Loss: 2.5199\n",
            "[Step 17020] Loss: 2.2034\n",
            "[Step 17030] Loss: 2.5753\n",
            "[Step 17040] Loss: 2.2511\n",
            "ðŸ“˜ Epoch 74 - Avg Training Loss: 2.3641\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4770 | Accuracy: 0.3799 | Precision: 0.3597\n",
            "[Step 17050] Loss: 2.4168\n",
            "[Step 17060] Loss: 2.5993\n",
            "[Step 17070] Loss: 2.1593\n",
            "[Step 17080] Loss: 2.2256\n",
            "[Step 17090] Loss: 2.0863\n",
            "[Step 17100] Loss: 2.3674\n",
            "[Step 17110] Loss: 2.4173\n",
            "[Step 17120] Loss: 2.0733\n",
            "[Step 17130] Loss: 2.5071\n",
            "[Step 17140] Loss: 2.2529\n",
            "[Step 17150] Loss: 2.1431\n",
            "[Step 17160] Loss: 2.2276\n",
            "[Step 17170] Loss: 2.5536\n",
            "[Step 17180] Loss: 2.6522\n",
            "[Step 17190] Loss: 2.2594\n",
            "[Step 17200] Loss: 2.1436\n",
            "[Step 17210] Loss: 2.2405\n",
            "[Step 17220] Loss: 2.0520\n",
            "[Step 17230] Loss: 2.3569\n",
            "[Step 17240] Loss: 2.5367\n",
            "[Step 17250] Loss: 2.4762\n",
            "[Step 17260] Loss: 2.2187\n",
            "[Step 17270] Loss: 2.8630\n",
            "[Step 17280] Loss: 2.2622\n",
            "[Step 17290] Loss: 2.3937\n",
            "[Step 17300] Loss: 2.3814\n",
            "[Step 17310] Loss: 1.9988\n",
            "[Step 17320] Loss: 2.2266\n",
            "[Step 17330] Loss: 2.1835\n",
            "[Step 17340] Loss: 2.2377\n",
            "[Step 17350] Loss: 2.7339\n",
            "[Step 17360] Loss: 2.5301\n",
            "[Step 17370] Loss: 2.3914\n",
            "[Step 17380] Loss: 2.2967\n",
            "[Step 17390] Loss: 2.3270\n",
            "[Step 17400] Loss: 2.2388\n",
            "[Step 17410] Loss: 2.3889\n",
            "[Step 17420] Loss: 2.4809\n",
            "[Step 17430] Loss: 2.3425\n",
            "[Step 17440] Loss: 2.6220\n",
            "[Step 17450] Loss: 2.3224\n",
            "[Step 17460] Loss: 2.5551\n",
            "[Step 17470] Loss: 2.8919\n",
            "[Step 17480] Loss: 2.3053\n",
            "[Step 17490] Loss: 1.9160\n",
            "[Step 17500] Loss: 2.6243\n",
            "[Step 17510] Loss: 2.6129\n",
            "[Step 17520] Loss: 3.0465\n",
            "[Step 17530] Loss: 2.4367\n",
            "[Step 17540] Loss: 2.0767\n",
            "[Step 17550] Loss: 2.2017\n",
            "[Step 17560] Loss: 2.0730\n",
            "[Step 17570] Loss: 2.4963\n",
            "[Step 17580] Loss: 2.3474\n",
            "[Step 17590] Loss: 2.4914\n",
            "[Step 17600] Loss: 2.0979\n",
            "[Step 17610] Loss: 1.9055\n",
            "[Step 17620] Loss: 2.2273\n",
            "[Step 17630] Loss: 2.3528\n",
            "[Step 17640] Loss: 2.2141\n",
            "[Step 17650] Loss: 2.1383\n",
            "[Step 17660] Loss: 2.0687\n",
            "[Step 17670] Loss: 2.3571\n",
            "[Step 17680] Loss: 2.4481\n",
            "[Step 17690] Loss: 2.0069\n",
            "[Step 17700] Loss: 2.4072\n",
            "[Step 17710] Loss: 2.2436\n",
            "[Step 17720] Loss: 2.3313\n",
            "[Step 17730] Loss: 2.5636\n",
            "[Step 17740] Loss: 2.1010\n",
            "[Step 17750] Loss: 2.1459\n",
            "[Step 17760] Loss: 2.2686\n",
            "[Step 17770] Loss: 2.4873\n",
            "[Step 17780] Loss: 2.3841\n",
            "[Step 17790] Loss: 2.7743\n",
            "[Step 17800] Loss: 1.9197\n",
            "[Step 17810] Loss: 2.4354\n",
            "[Step 17820] Loss: 2.4393\n",
            "[Step 17830] Loss: 2.0778\n",
            "[Step 17840] Loss: 2.4836\n",
            "[Step 17850] Loss: 2.3048\n",
            "[Step 17860] Loss: 2.1550\n",
            "[Step 17870] Loss: 2.7039\n",
            "[Step 17880] Loss: 2.9590\n",
            "[Step 17890] Loss: 2.6088\n",
            "[Step 17900] Loss: 2.2191\n",
            "[Step 17910] Loss: 2.3838\n",
            "[Step 17920] Loss: 2.8281\n",
            "[Step 17930] Loss: 2.3971\n",
            "[Step 17940] Loss: 2.7319\n",
            "[Step 17950] Loss: 2.1746\n",
            "[Step 17960] Loss: 2.0890\n",
            "[Step 17970] Loss: 2.3024\n",
            "[Step 17980] Loss: 2.2363\n",
            "[Step 17990] Loss: 2.3928\n",
            "[Step 18000] Loss: 2.0627\n",
            "[Step 18010] Loss: 2.3560\n",
            "[Step 18020] Loss: 2.5649\n",
            "[Step 18030] Loss: 2.3880\n",
            "[Step 18040] Loss: 2.5901\n",
            "[Step 18050] Loss: 2.6430\n",
            "[Step 18060] Loss: 2.4037\n",
            "[Step 18070] Loss: 1.9423\n",
            "[Step 18080] Loss: 2.2989\n",
            "[Step 18090] Loss: 2.4106\n",
            "[Step 18100] Loss: 2.2558\n",
            "[Step 18110] Loss: 3.1129\n",
            "[Step 18120] Loss: 2.0330\n",
            "[Step 18130] Loss: 2.5925\n",
            "[Step 18140] Loss: 2.2278\n",
            "[Step 18150] Loss: 2.1673\n",
            "[Step 18160] Loss: 2.6831\n",
            "[Step 18170] Loss: 2.4674\n",
            "[Step 18180] Loss: 2.3759\n",
            "[Step 18190] Loss: 2.4143\n",
            "[Step 18200] Loss: 2.5124\n",
            "[Step 18210] Loss: 2.9004\n",
            "[Step 18220] Loss: 2.3575\n",
            "[Step 18230] Loss: 2.6002\n",
            "[Step 18240] Loss: 2.7376\n",
            "[Step 18250] Loss: 2.2596\n",
            "[Step 18260] Loss: 2.7234\n",
            "[Step 18270] Loss: 2.7358\n",
            "[Step 18280] Loss: 2.2247\n",
            "[Step 18290] Loss: 2.2072\n",
            "[Step 18300] Loss: 2.5986\n",
            "[Step 18310] Loss: 2.3189\n",
            "[Step 18320] Loss: 2.6916\n",
            "[Step 18330] Loss: 2.1589\n",
            "[Step 18340] Loss: 2.3285\n",
            "[Step 18350] Loss: 2.4974\n",
            "[Step 18360] Loss: 2.5136\n",
            "[Step 18370] Loss: 2.2610\n",
            "[Step 18380] Loss: 1.9276\n",
            "[Step 18390] Loss: 2.3008\n",
            "[Step 18400] Loss: 2.5198\n",
            "[Step 18410] Loss: 2.1832\n",
            "[Step 18420] Loss: 2.1954\n",
            "[Step 18430] Loss: 2.5259\n",
            "[Step 18440] Loss: 2.5497\n",
            "[Step 18450] Loss: 2.1592\n",
            "[Step 18460] Loss: 2.1970\n",
            "[Step 18470] Loss: 2.4916\n",
            "[Step 18480] Loss: 2.5741\n",
            "[Step 18490] Loss: 2.4260\n",
            "[Step 18500] Loss: 2.3956\n",
            "[Step 18510] Loss: 2.1538\n",
            "[Step 18520] Loss: 1.8715\n",
            "[Step 18530] Loss: 2.0250\n",
            "[Step 18540] Loss: 2.8574\n",
            "[Step 18550] Loss: 2.2911\n",
            "[Step 18560] Loss: 2.0858\n",
            "[Step 18570] Loss: 2.4466\n",
            "[Step 18580] Loss: 2.1590\n",
            "[Step 18590] Loss: 2.1856\n",
            "[Step 18600] Loss: 2.1769\n",
            "[Step 18610] Loss: 2.1150\n",
            "[Step 18620] Loss: 3.0224\n",
            "[Step 18630] Loss: 2.2431\n",
            "[Step 18640] Loss: 2.5549\n",
            "[Step 18650] Loss: 2.9611\n",
            "[Step 18660] Loss: 2.0147\n",
            "[Step 18670] Loss: 2.4254\n",
            "[Step 18680] Loss: 2.4318\n",
            "[Step 18690] Loss: 2.4680\n",
            "[Step 18700] Loss: 2.0867\n",
            "[Step 18710] Loss: 2.8980\n",
            "[Step 18720] Loss: 2.5262\n",
            "[Step 18730] Loss: 2.2133\n",
            "[Step 18740] Loss: 2.1102\n",
            "[Step 18750] Loss: 2.2756\n",
            "[Step 18760] Loss: 2.4791\n",
            "[Step 18770] Loss: 2.5363\n",
            "[Step 18780] Loss: 2.4997\n",
            "[Step 18790] Loss: 2.3506\n",
            "[Step 18800] Loss: 2.8459\n",
            "[Step 18810] Loss: 2.1968\n",
            "[Step 18820] Loss: 2.3335\n",
            "[Step 18830] Loss: 2.7644\n",
            "[Step 18840] Loss: 2.2876\n",
            "[Step 18850] Loss: 2.4101\n",
            "[Step 18860] Loss: 2.9381\n",
            "[Step 18870] Loss: 1.8921\n",
            "[Step 18880] Loss: 2.2703\n",
            "[Step 18890] Loss: 1.7838\n",
            "[Step 18900] Loss: 2.1727\n",
            "[Step 18910] Loss: 1.8201\n",
            "[Step 18920] Loss: 2.1795\n",
            "[Step 18930] Loss: 2.2886\n",
            "[Step 18940] Loss: 2.6026\n",
            "ðŸ“˜ Epoch 75 - Avg Training Loss: 2.3590\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4770 | Accuracy: 0.3777 | Precision: 0.3576\n",
            "[Step 18950] Loss: 2.1545\n",
            "[Step 18960] Loss: 2.4139\n",
            "[Step 18970] Loss: 2.4292\n",
            "[Step 18980] Loss: 2.3003\n",
            "[Step 18990] Loss: 2.4165\n",
            "[Step 19000] Loss: 2.4116\n",
            "[Step 19010] Loss: 2.2027\n",
            "[Step 19020] Loss: 2.1457\n",
            "[Step 19030] Loss: 3.0657\n",
            "[Step 19040] Loss: 2.0787\n",
            "[Step 19050] Loss: 2.0875\n",
            "[Step 19060] Loss: 1.9901\n",
            "[Step 19070] Loss: 2.5464\n",
            "[Step 19080] Loss: 2.3464\n",
            "[Step 19090] Loss: 2.6037\n",
            "[Step 19100] Loss: 2.5328\n",
            "[Step 19110] Loss: 2.0008\n",
            "[Step 19120] Loss: 1.7860\n",
            "[Step 19130] Loss: 2.8573\n",
            "[Step 19140] Loss: 2.3662\n",
            "[Step 19150] Loss: 2.1659\n",
            "[Step 19160] Loss: 2.4725\n",
            "[Step 19170] Loss: 2.1732\n",
            "[Step 19180] Loss: 2.0125\n",
            "[Step 19190] Loss: 2.6330\n",
            "[Step 19200] Loss: 2.7804\n",
            "[Step 19210] Loss: 2.6433\n",
            "[Step 19220] Loss: 2.2676\n",
            "[Step 19230] Loss: 2.4854\n",
            "[Step 19240] Loss: 2.1710\n",
            "[Step 19250] Loss: 2.7939\n",
            "[Step 19260] Loss: 2.3476\n",
            "[Step 19270] Loss: 2.8583\n",
            "[Step 19280] Loss: 2.2179\n",
            "[Step 19290] Loss: 2.3214\n",
            "[Step 19300] Loss: 2.3140\n",
            "[Step 19310] Loss: 2.5523\n",
            "[Step 19320] Loss: 2.7408\n",
            "[Step 19330] Loss: 2.1442\n",
            "[Step 19340] Loss: 2.0873\n",
            "[Step 19350] Loss: 2.2118\n",
            "[Step 19360] Loss: 2.2596\n",
            "[Step 19370] Loss: 2.2724\n",
            "[Step 19380] Loss: 1.7848\n",
            "[Step 19390] Loss: 1.8071\n",
            "[Step 19400] Loss: 2.1220\n",
            "[Step 19410] Loss: 1.9795\n",
            "[Step 19420] Loss: 2.4084\n",
            "[Step 19430] Loss: 2.7635\n",
            "[Step 19440] Loss: 2.8740\n",
            "[Step 19450] Loss: 2.7089\n",
            "[Step 19460] Loss: 2.4263\n",
            "[Step 19470] Loss: 2.0926\n",
            "[Step 19480] Loss: 2.3556\n",
            "[Step 19490] Loss: 2.3696\n",
            "[Step 19500] Loss: 2.5200\n",
            "[Step 19510] Loss: 2.5559\n",
            "[Step 19520] Loss: 1.9126\n",
            "[Step 19530] Loss: 2.3042\n",
            "[Step 19540] Loss: 2.4694\n",
            "[Step 19550] Loss: 2.1510\n",
            "[Step 19560] Loss: 2.0008\n",
            "[Step 19570] Loss: 2.3313\n",
            "[Step 19580] Loss: 2.3517\n",
            "[Step 19590] Loss: 2.1542\n",
            "[Step 19600] Loss: 2.9559\n",
            "[Step 19610] Loss: 2.1385\n",
            "[Step 19620] Loss: 2.3216\n",
            "[Step 19630] Loss: 1.7053\n",
            "[Step 19640] Loss: 2.2099\n",
            "[Step 19650] Loss: 2.1295\n",
            "[Step 19660] Loss: 2.1191\n",
            "[Step 19670] Loss: 2.2606\n",
            "[Step 19680] Loss: 2.3191\n",
            "[Step 19690] Loss: 2.5620\n",
            "[Step 19700] Loss: 2.6797\n",
            "[Step 19710] Loss: 2.0226\n",
            "[Step 19720] Loss: 2.8691\n",
            "[Step 19730] Loss: 2.2333\n",
            "[Step 19740] Loss: 2.3910\n",
            "[Step 19750] Loss: 2.2704\n",
            "[Step 19760] Loss: 2.2061\n",
            "[Step 19770] Loss: 2.2719\n",
            "[Step 19780] Loss: 2.7371\n",
            "[Step 19790] Loss: 2.4145\n",
            "[Step 19800] Loss: 2.1623\n",
            "[Step 19810] Loss: 2.5318\n",
            "[Step 19820] Loss: 2.0279\n",
            "[Step 19830] Loss: 2.4343\n",
            "[Step 19840] Loss: 2.4886\n",
            "[Step 19850] Loss: 2.9032\n",
            "[Step 19860] Loss: 2.7198\n",
            "[Step 19870] Loss: 2.1677\n",
            "[Step 19880] Loss: 2.7739\n",
            "[Step 19890] Loss: 1.9700\n",
            "[Step 19900] Loss: 2.2602\n",
            "[Step 19910] Loss: 2.1094\n",
            "[Step 19920] Loss: 3.0414\n",
            "[Step 19930] Loss: 2.7498\n",
            "[Step 19940] Loss: 2.7840\n",
            "[Step 19950] Loss: 1.9840\n",
            "[Step 19960] Loss: 2.3662\n",
            "[Step 19970] Loss: 2.2214\n",
            "[Step 19980] Loss: 2.1108\n",
            "[Step 19990] Loss: 2.4031\n",
            "[Step 20000] Loss: 2.6673\n",
            "[Step 20010] Loss: 2.0165\n",
            "[Step 20020] Loss: 2.3575\n",
            "[Step 20030] Loss: 1.9901\n",
            "[Step 20040] Loss: 2.5529\n",
            "[Step 20050] Loss: 2.3071\n",
            "[Step 20060] Loss: 2.1088\n",
            "[Step 20070] Loss: 2.1152\n",
            "[Step 20080] Loss: 2.7218\n",
            "[Step 20090] Loss: 2.9475\n",
            "[Step 20100] Loss: 2.2973\n",
            "[Step 20110] Loss: 2.2453\n",
            "[Step 20120] Loss: 2.1218\n",
            "[Step 20130] Loss: 2.2036\n",
            "[Step 20140] Loss: 2.2305\n",
            "[Step 20150] Loss: 2.4670\n",
            "[Step 20160] Loss: 2.1920\n",
            "[Step 20170] Loss: 2.4229\n",
            "[Step 20180] Loss: 2.3826\n",
            "[Step 20190] Loss: 2.2473\n",
            "[Step 20200] Loss: 2.7519\n",
            "[Step 20210] Loss: 2.5188\n",
            "[Step 20220] Loss: 2.6927\n",
            "[Step 20230] Loss: 2.1027\n",
            "[Step 20240] Loss: 2.2787\n",
            "[Step 20250] Loss: 2.5362\n",
            "[Step 20260] Loss: 2.5727\n",
            "[Step 20270] Loss: 2.5990\n",
            "[Step 20280] Loss: 1.8643\n",
            "[Step 20290] Loss: 2.4520\n",
            "[Step 20300] Loss: 2.2878\n",
            "[Step 20310] Loss: 2.0783\n",
            "[Step 20320] Loss: 2.1816\n",
            "[Step 20330] Loss: 2.6061\n",
            "[Step 20340] Loss: 2.5164\n",
            "[Step 20350] Loss: 2.3053\n",
            "[Step 20360] Loss: 2.1484\n",
            "[Step 20370] Loss: 2.1594\n",
            "[Step 20380] Loss: 2.3528\n",
            "[Step 20390] Loss: 2.3194\n",
            "[Step 20400] Loss: 2.4899\n",
            "[Step 20410] Loss: 2.4264\n",
            "[Step 20420] Loss: 2.3458\n",
            "[Step 20430] Loss: 1.8885\n",
            "[Step 20440] Loss: 2.5793\n",
            "[Step 20450] Loss: 2.3104\n",
            "[Step 20460] Loss: 2.4141\n",
            "[Step 20470] Loss: 2.1900\n",
            "[Step 20480] Loss: 2.0358\n",
            "[Step 20490] Loss: 2.0204\n",
            "[Step 20500] Loss: 2.2145\n",
            "[Step 20510] Loss: 2.1887\n",
            "[Step 20520] Loss: 2.1379\n",
            "[Step 20530] Loss: 2.5379\n",
            "[Step 20540] Loss: 2.6140\n",
            "[Step 20550] Loss: 1.9755\n",
            "[Step 20560] Loss: 2.5579\n",
            "[Step 20570] Loss: 2.5606\n",
            "[Step 20580] Loss: 2.3086\n",
            "[Step 20590] Loss: 2.2394\n",
            "[Step 20600] Loss: 2.6590\n",
            "[Step 20610] Loss: 1.7677\n",
            "[Step 20620] Loss: 2.0660\n",
            "[Step 20630] Loss: 2.0372\n",
            "[Step 20640] Loss: 2.0766\n",
            "[Step 20650] Loss: 2.6313\n",
            "[Step 20660] Loss: 2.6784\n",
            "[Step 20670] Loss: 2.4506\n",
            "[Step 20680] Loss: 2.1839\n",
            "[Step 20690] Loss: 2.1465\n",
            "[Step 20700] Loss: 1.6221\n",
            "[Step 20710] Loss: 2.1492\n",
            "[Step 20720] Loss: 2.1522\n",
            "[Step 20730] Loss: 3.0887\n",
            "[Step 20740] Loss: 2.3031\n",
            "[Step 20750] Loss: 2.4863\n",
            "[Step 20760] Loss: 2.0222\n",
            "[Step 20770] Loss: 2.3943\n",
            "[Step 20780] Loss: 2.3481\n",
            "[Step 20790] Loss: 2.8539\n",
            "[Step 20800] Loss: 2.2990\n",
            "[Step 20810] Loss: 2.0651\n",
            "[Step 20820] Loss: 2.4816\n",
            "[Step 20830] Loss: 2.5019\n",
            "ðŸ“˜ Epoch 76 - Avg Training Loss: 2.3561\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4756 | Accuracy: 0.3786 | Precision: 0.3578\n",
            "[Step 20840] Loss: 2.4040\n",
            "[Step 20850] Loss: 2.2477\n",
            "[Step 20860] Loss: 2.3263\n",
            "[Step 20870] Loss: 2.0811\n",
            "[Step 20880] Loss: 2.7311\n",
            "[Step 20890] Loss: 2.3115\n",
            "[Step 20900] Loss: 2.5706\n",
            "[Step 20910] Loss: 2.3604\n",
            "[Step 20920] Loss: 2.3323\n",
            "[Step 20930] Loss: 2.4099\n",
            "[Step 20940] Loss: 2.5554\n",
            "[Step 20950] Loss: 2.5412\n",
            "[Step 20960] Loss: 2.3515\n",
            "[Step 20970] Loss: 2.3085\n",
            "[Step 20980] Loss: 2.0855\n",
            "[Step 20990] Loss: 2.4771\n",
            "[Step 21000] Loss: 2.8264\n",
            "[Step 21010] Loss: 2.5655\n",
            "[Step 21020] Loss: 2.3027\n",
            "[Step 21030] Loss: 2.4682\n",
            "[Step 21040] Loss: 2.5985\n",
            "[Step 21050] Loss: 2.0240\n",
            "[Step 21060] Loss: 2.0373\n",
            "[Step 21070] Loss: 2.1790\n",
            "[Step 21080] Loss: 2.3936\n",
            "[Step 21090] Loss: 1.9995\n",
            "[Step 21100] Loss: 1.8872\n",
            "[Step 21110] Loss: 2.3403\n",
            "[Step 21120] Loss: 2.8938\n",
            "[Step 21130] Loss: 2.3769\n",
            "[Step 21140] Loss: 2.7929\n",
            "[Step 21150] Loss: 2.0149\n",
            "[Step 21160] Loss: 2.7371\n",
            "[Step 21170] Loss: 1.9972\n",
            "[Step 21180] Loss: 2.4532\n",
            "[Step 21190] Loss: 2.3035\n",
            "[Step 21200] Loss: 2.5662\n",
            "[Step 21210] Loss: 2.3823\n",
            "[Step 21220] Loss: 2.3037\n",
            "[Step 21230] Loss: 2.4306\n",
            "[Step 21240] Loss: 2.1795\n",
            "[Step 21250] Loss: 2.1707\n",
            "[Step 21260] Loss: 2.4223\n",
            "[Step 21270] Loss: 2.2251\n",
            "[Step 21280] Loss: 1.9387\n",
            "[Step 21290] Loss: 2.3155\n",
            "[Step 21300] Loss: 2.3470\n",
            "[Step 21310] Loss: 2.0660\n",
            "[Step 21320] Loss: 2.5883\n",
            "[Step 21330] Loss: 1.7135\n",
            "[Step 21340] Loss: 2.2392\n",
            "[Step 21350] Loss: 2.1230\n",
            "[Step 21360] Loss: 2.3456\n",
            "[Step 21370] Loss: 2.0917\n",
            "[Step 21380] Loss: 2.4781\n",
            "[Step 21390] Loss: 1.8847\n",
            "[Step 21400] Loss: 2.1288\n",
            "[Step 21410] Loss: 2.3521\n",
            "[Step 21420] Loss: 2.0801\n",
            "[Step 21430] Loss: 2.4623\n",
            "[Step 21440] Loss: 2.4416\n",
            "[Step 21450] Loss: 2.6235\n",
            "[Step 21460] Loss: 2.4554\n",
            "[Step 21470] Loss: 2.4088\n",
            "[Step 21480] Loss: 1.6653\n",
            "[Step 21490] Loss: 2.5298\n",
            "[Step 21500] Loss: 2.7446\n",
            "[Step 21510] Loss: 2.2835\n",
            "[Step 21520] Loss: 2.0577\n",
            "[Step 21530] Loss: 1.9960\n",
            "[Step 21540] Loss: 2.2545\n",
            "[Step 21550] Loss: 2.3151\n",
            "[Step 21560] Loss: 2.4680\n",
            "[Step 21570] Loss: 2.1255\n",
            "[Step 21580] Loss: 2.4313\n",
            "[Step 21590] Loss: 2.1729\n",
            "[Step 21600] Loss: 2.6106\n",
            "[Step 21610] Loss: 2.1405\n",
            "[Step 21620] Loss: 2.1900\n",
            "[Step 21630] Loss: 2.8311\n",
            "[Step 21640] Loss: 2.3391\n",
            "[Step 21650] Loss: 2.1995\n",
            "[Step 21660] Loss: 2.0944\n",
            "[Step 21670] Loss: 2.0715\n",
            "[Step 21680] Loss: 2.0409\n",
            "[Step 21690] Loss: 2.0423\n",
            "[Step 21700] Loss: 2.5513\n",
            "[Step 21710] Loss: 2.1134\n",
            "[Step 21720] Loss: 2.4110\n",
            "[Step 21730] Loss: 2.6480\n",
            "[Step 21740] Loss: 2.2259\n",
            "[Step 21750] Loss: 2.0814\n",
            "[Step 21760] Loss: 2.6018\n",
            "[Step 21770] Loss: 2.3465\n",
            "[Step 21780] Loss: 2.3574\n",
            "[Step 21790] Loss: 2.7659\n",
            "[Step 21800] Loss: 2.0753\n",
            "[Step 21810] Loss: 2.8199\n",
            "[Step 21820] Loss: 2.2233\n",
            "[Step 21830] Loss: 2.2060\n",
            "[Step 21840] Loss: 2.0281\n",
            "[Step 21850] Loss: 2.0554\n",
            "[Step 21860] Loss: 2.4150\n",
            "[Step 21870] Loss: 2.3700\n",
            "[Step 21880] Loss: 2.0693\n",
            "[Step 21890] Loss: 2.1852\n",
            "[Step 21900] Loss: 2.4775\n",
            "[Step 21910] Loss: 2.1528\n",
            "[Step 21920] Loss: 2.5484\n",
            "[Step 21930] Loss: 2.9077\n",
            "[Step 21940] Loss: 2.3592\n",
            "[Step 21950] Loss: 2.0963\n",
            "[Step 21960] Loss: 2.4965\n",
            "[Step 21970] Loss: 2.6159\n",
            "[Step 21980] Loss: 2.4162\n",
            "[Step 21990] Loss: 2.5401\n",
            "[Step 22000] Loss: 2.1496\n",
            "[Step 22010] Loss: 2.5061\n",
            "[Step 22020] Loss: 1.7846\n",
            "[Step 22030] Loss: 2.3462\n",
            "[Step 22040] Loss: 2.3495\n",
            "[Step 22050] Loss: 2.1986\n",
            "[Step 22060] Loss: 2.0252\n",
            "[Step 22070] Loss: 2.2644\n",
            "[Step 22080] Loss: 2.3795\n",
            "[Step 22090] Loss: 2.3342\n",
            "[Step 22100] Loss: 1.9956\n",
            "[Step 22110] Loss: 2.1506\n",
            "[Step 22120] Loss: 2.0803\n",
            "[Step 22130] Loss: 2.4224\n",
            "[Step 22140] Loss: 2.4239\n",
            "[Step 22150] Loss: 2.3896\n",
            "[Step 22160] Loss: 2.0987\n",
            "[Step 22170] Loss: 1.8750\n",
            "[Step 22180] Loss: 3.3506\n",
            "[Step 22190] Loss: 2.0764\n",
            "[Step 22200] Loss: 2.3430\n",
            "[Step 22210] Loss: 2.4370\n",
            "[Step 22220] Loss: 2.8484\n",
            "[Step 22230] Loss: 2.4349\n",
            "[Step 22240] Loss: 2.2508\n",
            "[Step 22250] Loss: 2.3891\n",
            "[Step 22260] Loss: 2.5191\n",
            "[Step 22270] Loss: 2.2362\n",
            "[Step 22280] Loss: 2.0867\n",
            "[Step 22290] Loss: 2.5292\n",
            "[Step 22300] Loss: 2.5226\n",
            "[Step 22310] Loss: 2.1699\n",
            "[Step 22320] Loss: 2.3056\n",
            "[Step 22330] Loss: 2.4547\n",
            "[Step 22340] Loss: 2.9321\n",
            "[Step 22350] Loss: 1.7468\n",
            "[Step 22360] Loss: 2.2694\n",
            "[Step 22370] Loss: 2.2165\n",
            "[Step 22380] Loss: 1.9675\n",
            "[Step 22390] Loss: 2.1021\n",
            "[Step 22400] Loss: 2.1152\n",
            "[Step 22410] Loss: 2.5003\n",
            "[Step 22420] Loss: 2.4527\n",
            "[Step 22430] Loss: 2.7283\n",
            "[Step 22440] Loss: 2.5732\n",
            "[Step 22450] Loss: 2.1936\n",
            "[Step 22460] Loss: 2.3401\n",
            "[Step 22470] Loss: 2.6622\n",
            "[Step 22480] Loss: 2.6960\n",
            "[Step 22490] Loss: 1.7535\n",
            "[Step 22500] Loss: 1.8570\n",
            "[Step 22510] Loss: 2.3142\n",
            "[Step 22520] Loss: 1.8768\n",
            "[Step 22530] Loss: 2.5413\n",
            "[Step 22540] Loss: 2.7848\n",
            "[Step 22550] Loss: 2.1295\n",
            "[Step 22560] Loss: 2.2732\n",
            "[Step 22570] Loss: 2.3403\n",
            "[Step 22580] Loss: 2.2493\n",
            "[Step 22590] Loss: 2.9336\n",
            "[Step 22600] Loss: 2.2507\n",
            "[Step 22610] Loss: 2.6832\n",
            "[Step 22620] Loss: 2.0056\n",
            "[Step 22630] Loss: 2.7046\n",
            "[Step 22640] Loss: 2.0799\n",
            "[Step 22650] Loss: 2.9486\n",
            "[Step 22660] Loss: 2.1155\n",
            "[Step 22670] Loss: 2.7625\n",
            "[Step 22680] Loss: 2.6391\n",
            "[Step 22690] Loss: 2.2412\n",
            "[Step 22700] Loss: 2.6856\n",
            "[Step 22710] Loss: 2.4230\n",
            "[Step 22720] Loss: 2.5944\n",
            "ðŸ“˜ Epoch 77 - Avg Training Loss: 2.3566\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4808 | Accuracy: 0.3793 | Precision: 0.3591\n",
            "[Step 22730] Loss: 2.2891\n",
            "[Step 22740] Loss: 2.4723\n",
            "[Step 22750] Loss: 2.3389\n",
            "[Step 22760] Loss: 2.4739\n",
            "[Step 22770] Loss: 2.4945\n",
            "[Step 22780] Loss: 2.5078\n",
            "[Step 22790] Loss: 1.9013\n",
            "[Step 22800] Loss: 2.3507\n",
            "[Step 22810] Loss: 2.4443\n",
            "[Step 22820] Loss: 2.1085\n",
            "[Step 22830] Loss: 2.2287\n",
            "[Step 22840] Loss: 2.3589\n",
            "[Step 22850] Loss: 2.1701\n",
            "[Step 22860] Loss: 2.1757\n",
            "[Step 22870] Loss: 2.2178\n",
            "[Step 22880] Loss: 2.4637\n",
            "[Step 22890] Loss: 2.3524\n",
            "[Step 22900] Loss: 2.2836\n",
            "[Step 22910] Loss: 2.1730\n",
            "[Step 22920] Loss: 2.1164\n",
            "[Step 22930] Loss: 2.8500\n",
            "[Step 22940] Loss: 2.0928\n",
            "[Step 22950] Loss: 2.0309\n",
            "[Step 22960] Loss: 2.7795\n",
            "[Step 22970] Loss: 2.3428\n",
            "[Step 22980] Loss: 2.3796\n",
            "[Step 22990] Loss: 2.4654\n",
            "[Step 23000] Loss: 2.5909\n",
            "[Step 23010] Loss: 2.4424\n",
            "[Step 23020] Loss: 2.2099\n",
            "[Step 23030] Loss: 2.2831\n",
            "[Step 23040] Loss: 2.3861\n",
            "[Step 23050] Loss: 2.0956\n",
            "[Step 23060] Loss: 2.4645\n",
            "[Step 23070] Loss: 2.2551\n",
            "[Step 23080] Loss: 2.3222\n",
            "[Step 23090] Loss: 2.0397\n",
            "[Step 23100] Loss: 2.1171\n",
            "[Step 23110] Loss: 2.2161\n",
            "[Step 23120] Loss: 2.3002\n",
            "[Step 23130] Loss: 2.3216\n",
            "[Step 23140] Loss: 2.4119\n",
            "[Step 23150] Loss: 2.5632\n",
            "[Step 23160] Loss: 2.2383\n",
            "[Step 23170] Loss: 1.9609\n",
            "[Step 23180] Loss: 2.2242\n",
            "[Step 23190] Loss: 2.2182\n",
            "[Step 23200] Loss: 2.6565\n",
            "[Step 23210] Loss: 2.7943\n",
            "[Step 23220] Loss: 2.0412\n",
            "[Step 23230] Loss: 2.3450\n",
            "[Step 23240] Loss: 2.4631\n",
            "[Step 23250] Loss: 2.1641\n",
            "[Step 23260] Loss: 2.7463\n",
            "[Step 23270] Loss: 2.4206\n",
            "[Step 23280] Loss: 2.8812\n",
            "[Step 23290] Loss: 1.9883\n",
            "[Step 23300] Loss: 2.4698\n",
            "[Step 23310] Loss: 2.3607\n",
            "[Step 23320] Loss: 2.4046\n",
            "[Step 23330] Loss: 2.1930\n",
            "[Step 23340] Loss: 1.9361\n",
            "[Step 23350] Loss: 1.9736\n",
            "[Step 23360] Loss: 2.5435\n",
            "[Step 23370] Loss: 2.1177\n",
            "[Step 23380] Loss: 2.8562\n",
            "[Step 23390] Loss: 2.2458\n",
            "[Step 23400] Loss: 2.4214\n",
            "[Step 23410] Loss: 2.1889\n",
            "[Step 23420] Loss: 2.3794\n",
            "[Step 23430] Loss: 2.5788\n",
            "[Step 23440] Loss: 2.4055\n",
            "[Step 23450] Loss: 2.2810\n",
            "[Step 23460] Loss: 2.6141\n",
            "[Step 23470] Loss: 2.5745\n",
            "[Step 23480] Loss: 2.1991\n",
            "[Step 23490] Loss: 2.4445\n",
            "[Step 23500] Loss: 2.3222\n",
            "[Step 23510] Loss: 2.4016\n",
            "[Step 23520] Loss: 2.7641\n",
            "[Step 23530] Loss: 1.8865\n",
            "[Step 23540] Loss: 2.3727\n",
            "[Step 23550] Loss: 2.7008\n",
            "[Step 23560] Loss: 2.2168\n",
            "[Step 23570] Loss: 2.3573\n",
            "[Step 23580] Loss: 1.9947\n",
            "[Step 23590] Loss: 2.3303\n",
            "[Step 23600] Loss: 2.4002\n",
            "[Step 23610] Loss: 1.7948\n",
            "[Step 23620] Loss: 1.8774\n",
            "[Step 23630] Loss: 2.2638\n",
            "[Step 23640] Loss: 2.4035\n",
            "[Step 23650] Loss: 2.5642\n",
            "[Step 23660] Loss: 2.1110\n",
            "[Step 23670] Loss: 2.1733\n",
            "[Step 23680] Loss: 2.7345\n",
            "[Step 23690] Loss: 2.9882\n",
            "[Step 23700] Loss: 3.0247\n",
            "[Step 23710] Loss: 1.9387\n",
            "[Step 23720] Loss: 3.0932\n",
            "[Step 23730] Loss: 2.7098\n",
            "[Step 23740] Loss: 2.6202\n",
            "[Step 23750] Loss: 2.0782\n",
            "[Step 23760] Loss: 2.4265\n",
            "[Step 23770] Loss: 2.2799\n",
            "[Step 23780] Loss: 2.4444\n",
            "[Step 23790] Loss: 2.2941\n",
            "[Step 23800] Loss: 2.0145\n",
            "[Step 23810] Loss: 2.2903\n",
            "[Step 23820] Loss: 2.3268\n",
            "[Step 23830] Loss: 2.5004\n",
            "[Step 23840] Loss: 2.1355\n",
            "[Step 23850] Loss: 2.4248\n",
            "[Step 23860] Loss: 2.1600\n",
            "[Step 23870] Loss: 2.6909\n",
            "[Step 23880] Loss: 1.9646\n",
            "[Step 23890] Loss: 2.2290\n",
            "[Step 23900] Loss: 2.3490\n",
            "[Step 23910] Loss: 2.2304\n",
            "[Step 23920] Loss: 2.1759\n",
            "[Step 23930] Loss: 2.0376\n",
            "[Step 23940] Loss: 2.5421\n",
            "[Step 23950] Loss: 2.4098\n",
            "[Step 23960] Loss: 2.1908\n",
            "[Step 23970] Loss: 2.3653\n",
            "[Step 23980] Loss: 2.3326\n",
            "[Step 23990] Loss: 2.2864\n",
            "[Step 24000] Loss: 2.4900\n",
            "[Step 24010] Loss: 2.8612\n",
            "[Step 24020] Loss: 2.4930\n",
            "[Step 24030] Loss: 2.5089\n",
            "[Step 24040] Loss: 2.2492\n",
            "[Step 24050] Loss: 1.7713\n",
            "[Step 24060] Loss: 2.0457\n",
            "[Step 24070] Loss: 2.5724\n",
            "[Step 24080] Loss: 2.5613\n",
            "[Step 24090] Loss: 2.5369\n",
            "[Step 24100] Loss: 2.4552\n",
            "[Step 24110] Loss: 2.7269\n",
            "[Step 24120] Loss: 2.2927\n",
            "[Step 24130] Loss: 2.4255\n",
            "[Step 24140] Loss: 2.9398\n",
            "[Step 24150] Loss: 2.0257\n",
            "[Step 24160] Loss: 1.9343\n",
            "[Step 24170] Loss: 2.3674\n",
            "[Step 24180] Loss: 2.4461\n",
            "[Step 24190] Loss: 1.9553\n",
            "[Step 24200] Loss: 1.9354\n",
            "[Step 24210] Loss: 2.4981\n",
            "[Step 24220] Loss: 2.4470\n",
            "[Step 24230] Loss: 2.5182\n",
            "[Step 24240] Loss: 2.1131\n",
            "[Step 24250] Loss: 2.7848\n",
            "[Step 24260] Loss: 1.9020\n",
            "[Step 24270] Loss: 2.2900\n",
            "[Step 24280] Loss: 2.2188\n",
            "[Step 24290] Loss: 2.2673\n",
            "[Step 24300] Loss: 2.0621\n",
            "[Step 24310] Loss: 2.0258\n",
            "[Step 24320] Loss: 2.2809\n",
            "[Step 24330] Loss: 2.2240\n",
            "[Step 24340] Loss: 2.1110\n",
            "[Step 24350] Loss: 2.8473\n",
            "[Step 24360] Loss: 2.2453\n",
            "[Step 24370] Loss: 2.5105\n",
            "[Step 24380] Loss: 2.6491\n",
            "[Step 24390] Loss: 2.2280\n",
            "[Step 24400] Loss: 1.7011\n",
            "[Step 24410] Loss: 2.0908\n",
            "[Step 24420] Loss: 2.1744\n",
            "[Step 24430] Loss: 2.3275\n",
            "[Step 24440] Loss: 2.2573\n",
            "[Step 24450] Loss: 2.3109\n",
            "[Step 24460] Loss: 2.4385\n",
            "[Step 24470] Loss: 2.6116\n",
            "[Step 24480] Loss: 2.7731\n",
            "[Step 24490] Loss: 2.5021\n",
            "[Step 24500] Loss: 2.0511\n",
            "[Step 24510] Loss: 2.0752\n",
            "[Step 24520] Loss: 2.2301\n",
            "[Step 24530] Loss: 2.5469\n",
            "[Step 24540] Loss: 2.7506\n",
            "[Step 24550] Loss: 2.3260\n",
            "[Step 24560] Loss: 3.0424\n",
            "[Step 24570] Loss: 2.3330\n",
            "[Step 24580] Loss: 2.4179\n",
            "[Step 24590] Loss: 2.4644\n",
            "[Step 24600] Loss: 2.3975\n",
            "[Step 24610] Loss: 2.4424\n",
            "[Step 24620] Loss: 1.7167\n",
            "ðŸ“˜ Epoch 78 - Avg Training Loss: 2.3632\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4817 | Accuracy: 0.3776 | Precision: 0.3595\n",
            "[Step 24630] Loss: 2.1223\n",
            "[Step 24640] Loss: 2.2847\n",
            "[Step 24650] Loss: 2.1193\n",
            "[Step 24660] Loss: 1.9919\n",
            "[Step 24670] Loss: 2.8030\n",
            "[Step 24680] Loss: 2.2896\n",
            "[Step 24690] Loss: 2.3841\n",
            "[Step 24700] Loss: 2.7021\n",
            "[Step 24710] Loss: 2.6339\n",
            "[Step 24720] Loss: 2.8146\n",
            "[Step 24730] Loss: 2.3629\n",
            "[Step 24740] Loss: 2.3585\n",
            "[Step 24750] Loss: 2.0449\n",
            "[Step 24760] Loss: 1.8182\n",
            "[Step 24770] Loss: 2.2486\n",
            "[Step 24780] Loss: 2.1439\n",
            "[Step 24790] Loss: 2.2621\n",
            "[Step 24800] Loss: 2.0633\n",
            "[Step 24810] Loss: 2.1109\n",
            "[Step 24820] Loss: 1.8794\n",
            "[Step 24830] Loss: 2.4822\n",
            "[Step 24840] Loss: 2.5699\n",
            "[Step 24850] Loss: 2.3710\n",
            "[Step 24860] Loss: 2.3919\n",
            "[Step 24870] Loss: 2.4123\n",
            "[Step 24880] Loss: 2.3715\n",
            "[Step 24890] Loss: 2.3185\n",
            "[Step 24900] Loss: 2.6680\n",
            "[Step 24910] Loss: 2.1749\n",
            "[Step 24920] Loss: 2.4096\n",
            "[Step 24930] Loss: 2.0548\n",
            "[Step 24940] Loss: 2.2196\n",
            "[Step 24950] Loss: 2.1230\n",
            "[Step 24960] Loss: 2.3067\n",
            "[Step 24970] Loss: 2.3022\n",
            "[Step 24980] Loss: 2.2249\n",
            "[Step 24990] Loss: 2.5966\n",
            "[Step 25000] Loss: 2.4546\n",
            "[Step 25010] Loss: 2.8665\n",
            "[Step 25020] Loss: 2.2672\n",
            "[Step 25030] Loss: 2.2796\n",
            "[Step 25040] Loss: 2.1211\n",
            "[Step 25050] Loss: 2.2302\n",
            "[Step 25060] Loss: 2.8933\n",
            "[Step 25070] Loss: 2.4953\n",
            "[Step 25080] Loss: 2.2706\n",
            "[Step 25090] Loss: 2.3586\n",
            "[Step 25100] Loss: 2.6489\n",
            "[Step 25110] Loss: 2.5847\n",
            "[Step 25120] Loss: 2.2171\n",
            "[Step 25130] Loss: 2.5052\n",
            "[Step 25140] Loss: 2.1198\n",
            "[Step 25150] Loss: 2.1738\n",
            "[Step 25160] Loss: 2.4393\n",
            "[Step 25170] Loss: 2.6001\n",
            "[Step 25180] Loss: 2.2491\n",
            "[Step 25190] Loss: 2.2748\n",
            "[Step 25200] Loss: 2.5586\n",
            "[Step 25210] Loss: 2.2320\n",
            "[Step 25220] Loss: 1.9817\n",
            "[Step 25230] Loss: 2.4880\n",
            "[Step 25240] Loss: 2.1563\n",
            "[Step 25250] Loss: 2.3759\n",
            "[Step 25260] Loss: 1.9933\n",
            "[Step 25270] Loss: 1.9630\n",
            "[Step 25280] Loss: 2.9574\n",
            "[Step 25290] Loss: 2.6522\n",
            "[Step 25300] Loss: 2.4114\n",
            "[Step 25310] Loss: 1.9815\n",
            "[Step 25320] Loss: 2.2705\n",
            "[Step 25330] Loss: 2.1536\n",
            "[Step 25340] Loss: 2.5208\n",
            "[Step 25350] Loss: 2.5344\n",
            "[Step 25360] Loss: 2.6706\n",
            "[Step 25370] Loss: 2.5160\n",
            "[Step 25380] Loss: 2.1185\n",
            "[Step 25390] Loss: 2.4411\n",
            "[Step 25400] Loss: 2.0735\n",
            "[Step 25410] Loss: 2.3083\n",
            "[Step 25420] Loss: 2.2467\n",
            "[Step 25430] Loss: 2.1086\n",
            "[Step 25440] Loss: 2.3796\n",
            "[Step 25450] Loss: 2.4522\n",
            "[Step 25460] Loss: 2.4343\n",
            "[Step 25470] Loss: 2.1878\n",
            "[Step 25480] Loss: 2.2794\n",
            "[Step 25490] Loss: 2.7452\n",
            "[Step 25500] Loss: 2.3745\n",
            "[Step 25510] Loss: 2.7173\n",
            "[Step 25520] Loss: 2.8064\n",
            "[Step 25530] Loss: 2.0236\n",
            "[Step 25540] Loss: 2.6444\n",
            "[Step 25550] Loss: 1.9169\n",
            "[Step 25560] Loss: 1.9000\n",
            "[Step 25570] Loss: 2.6769\n",
            "[Step 25580] Loss: 2.3797\n",
            "[Step 25590] Loss: 2.0739\n",
            "[Step 25600] Loss: 2.6249\n",
            "[Step 25610] Loss: 2.5031\n",
            "[Step 25620] Loss: 2.1581\n",
            "[Step 25630] Loss: 2.3786\n",
            "[Step 25640] Loss: 2.2817\n",
            "[Step 25650] Loss: 2.4225\n",
            "[Step 25660] Loss: 2.4199\n",
            "[Step 25670] Loss: 2.4418\n",
            "[Step 25680] Loss: 2.5920\n",
            "[Step 25690] Loss: 2.0952\n",
            "[Step 25700] Loss: 2.1254\n",
            "[Step 25710] Loss: 2.4930\n",
            "[Step 25720] Loss: 2.1887\n",
            "[Step 25730] Loss: 2.1266\n",
            "[Step 25740] Loss: 2.2449\n",
            "[Step 25750] Loss: 2.2275\n",
            "[Step 25760] Loss: 2.1504\n",
            "[Step 25770] Loss: 2.2462\n",
            "[Step 25780] Loss: 1.7691\n",
            "[Step 25790] Loss: 2.1730\n",
            "[Step 25800] Loss: 2.3926\n",
            "[Step 25810] Loss: 2.1410\n",
            "[Step 25820] Loss: 2.9001\n",
            "[Step 25830] Loss: 2.1491\n",
            "[Step 25840] Loss: 2.5909\n",
            "[Step 25850] Loss: 2.9644\n",
            "[Step 25860] Loss: 2.0880\n",
            "[Step 25870] Loss: 2.2306\n",
            "[Step 25880] Loss: 2.4222\n",
            "[Step 25890] Loss: 2.0945\n",
            "[Step 25900] Loss: 2.6959\n",
            "[Step 25910] Loss: 1.9463\n",
            "[Step 25920] Loss: 2.1659\n",
            "[Step 25930] Loss: 2.4572\n",
            "[Step 25940] Loss: 2.0664\n",
            "[Step 25950] Loss: 2.1226\n",
            "[Step 25960] Loss: 2.5190\n",
            "[Step 25970] Loss: 1.9693\n",
            "[Step 25980] Loss: 2.2331\n",
            "[Step 25990] Loss: 2.3882\n",
            "[Step 26000] Loss: 2.1265\n",
            "[Step 26010] Loss: 2.7060\n",
            "[Step 26020] Loss: 2.2639\n",
            "[Step 26030] Loss: 2.6301\n",
            "[Step 26040] Loss: 2.0061\n",
            "[Step 26050] Loss: 2.0343\n",
            "[Step 26060] Loss: 2.5580\n",
            "[Step 26070] Loss: 2.3349\n",
            "[Step 26080] Loss: 2.6378\n",
            "[Step 26090] Loss: 2.9028\n",
            "[Step 26100] Loss: 2.1635\n",
            "[Step 26110] Loss: 1.9004\n",
            "[Step 26120] Loss: 2.8626\n",
            "[Step 26130] Loss: 2.8646\n",
            "[Step 26140] Loss: 2.3083\n",
            "[Step 26150] Loss: 2.0872\n",
            "[Step 26160] Loss: 2.5200\n",
            "[Step 26170] Loss: 2.9642\n",
            "[Step 26180] Loss: 2.6252\n",
            "[Step 26190] Loss: 2.3525\n",
            "[Step 26200] Loss: 2.5080\n",
            "[Step 26210] Loss: 2.1158\n",
            "[Step 26220] Loss: 2.1948\n",
            "[Step 26230] Loss: 2.9431\n",
            "[Step 26240] Loss: 2.6026\n",
            "[Step 26250] Loss: 2.7430\n",
            "[Step 26260] Loss: 2.9628\n",
            "[Step 26270] Loss: 2.2469\n",
            "[Step 26280] Loss: 2.1991\n",
            "[Step 26290] Loss: 2.6377\n",
            "[Step 26300] Loss: 2.6260\n",
            "[Step 26310] Loss: 2.1929\n",
            "[Step 26320] Loss: 2.6078\n",
            "[Step 26330] Loss: 2.4018\n",
            "[Step 26340] Loss: 2.6825\n",
            "[Step 26350] Loss: 2.8369\n",
            "[Step 26360] Loss: 2.1958\n",
            "[Step 26370] Loss: 2.4372\n",
            "[Step 26380] Loss: 2.0243\n",
            "[Step 26390] Loss: 2.1098\n",
            "[Step 26400] Loss: 2.2085\n",
            "[Step 26410] Loss: 2.6719\n",
            "[Step 26420] Loss: 3.1128\n",
            "[Step 26430] Loss: 2.4548\n",
            "[Step 26440] Loss: 2.4723\n",
            "[Step 26450] Loss: 2.8007\n",
            "[Step 26460] Loss: 2.3077\n",
            "[Step 26470] Loss: 2.2430\n",
            "[Step 26480] Loss: 2.2850\n",
            "[Step 26490] Loss: 2.3920\n",
            "[Step 26500] Loss: 2.2932\n",
            "[Step 26510] Loss: 2.1762\n",
            "ðŸ“˜ Epoch 79 - Avg Training Loss: 2.3683\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4808 | Accuracy: 0.3806 | Precision: 0.3639\n",
            "[Step 26520] Loss: 2.8236\n",
            "[Step 26530] Loss: 2.5661\n",
            "[Step 26540] Loss: 2.3615\n",
            "[Step 26550] Loss: 2.4041\n",
            "[Step 26560] Loss: 2.0161\n",
            "[Step 26570] Loss: 2.6010\n",
            "[Step 26580] Loss: 2.4724\n",
            "[Step 26590] Loss: 2.3040\n",
            "[Step 26600] Loss: 2.6543\n",
            "[Step 26610] Loss: 1.8109\n",
            "[Step 26620] Loss: 2.0944\n",
            "[Step 26630] Loss: 1.5785\n",
            "[Step 26640] Loss: 2.2887\n",
            "[Step 26650] Loss: 2.3545\n",
            "[Step 26660] Loss: 1.9829\n",
            "[Step 26670] Loss: 1.9974\n",
            "[Step 26680] Loss: 1.9835\n",
            "[Step 26690] Loss: 2.2093\n",
            "[Step 26700] Loss: 2.4733\n",
            "[Step 26710] Loss: 2.4546\n",
            "[Step 26720] Loss: 2.0977\n",
            "[Step 26730] Loss: 2.7867\n",
            "[Step 26740] Loss: 2.6159\n",
            "[Step 26750] Loss: 2.3676\n",
            "[Step 26760] Loss: 2.5782\n",
            "[Step 26770] Loss: 2.7939\n",
            "[Step 26780] Loss: 2.1277\n",
            "[Step 26790] Loss: 2.5397\n",
            "[Step 26800] Loss: 2.5119\n",
            "[Step 26810] Loss: 2.0835\n",
            "[Step 26820] Loss: 2.2181\n",
            "[Step 26830] Loss: 2.6646\n",
            "[Step 26840] Loss: 2.4279\n",
            "[Step 26850] Loss: 1.9839\n",
            "[Step 26860] Loss: 2.6679\n",
            "[Step 26870] Loss: 2.0331\n",
            "[Step 26880] Loss: 2.1593\n",
            "[Step 26890] Loss: 2.3214\n",
            "[Step 26900] Loss: 2.2086\n",
            "[Step 26910] Loss: 1.8915\n",
            "[Step 26920] Loss: 2.4720\n",
            "[Step 26930] Loss: 2.3500\n",
            "[Step 26940] Loss: 2.2212\n",
            "[Step 26950] Loss: 2.7591\n",
            "[Step 26960] Loss: 2.2813\n",
            "[Step 26970] Loss: 2.1523\n",
            "[Step 26980] Loss: 2.1197\n",
            "[Step 26990] Loss: 2.5531\n",
            "[Step 27000] Loss: 2.4554\n",
            "[Step 27010] Loss: 2.6859\n",
            "[Step 27020] Loss: 2.7672\n",
            "[Step 27030] Loss: 2.6193\n",
            "[Step 27040] Loss: 2.1421\n",
            "[Step 27050] Loss: 2.4468\n",
            "[Step 27060] Loss: 2.2060\n",
            "[Step 27070] Loss: 2.3668\n",
            "[Step 27080] Loss: 2.5085\n",
            "[Step 27090] Loss: 2.5075\n",
            "[Step 27100] Loss: 2.5048\n",
            "[Step 27110] Loss: 2.1597\n",
            "[Step 27120] Loss: 2.5664\n",
            "[Step 27130] Loss: 2.6135\n",
            "[Step 27140] Loss: 2.3489\n",
            "[Step 27150] Loss: 2.5718\n",
            "[Step 27160] Loss: 2.3783\n",
            "[Step 27170] Loss: 2.4604\n",
            "[Step 27180] Loss: 1.8605\n",
            "[Step 27190] Loss: 2.4284\n",
            "[Step 27200] Loss: 2.5491\n",
            "[Step 27210] Loss: 2.2448\n",
            "[Step 27220] Loss: 2.3793\n",
            "[Step 27230] Loss: 2.0680\n",
            "[Step 27240] Loss: 2.0072\n",
            "[Step 27250] Loss: 2.3352\n",
            "[Step 27260] Loss: 2.6622\n",
            "[Step 27270] Loss: 2.1044\n",
            "[Step 27280] Loss: 2.0952\n",
            "[Step 27290] Loss: 2.4408\n",
            "[Step 27300] Loss: 2.8536\n",
            "[Step 27310] Loss: 2.6765\n",
            "[Step 27320] Loss: 2.5381\n",
            "[Step 27330] Loss: 2.0764\n",
            "[Step 27340] Loss: 2.5197\n",
            "[Step 27350] Loss: 2.2811\n",
            "[Step 27360] Loss: 2.1749\n",
            "[Step 27370] Loss: 2.4714\n",
            "[Step 27380] Loss: 2.4794\n",
            "[Step 27390] Loss: 2.2940\n",
            "[Step 27400] Loss: 2.2953\n",
            "[Step 27410] Loss: 2.4498\n",
            "[Step 27420] Loss: 2.7132\n",
            "[Step 27430] Loss: 2.5064\n",
            "[Step 27440] Loss: 2.4735\n",
            "[Step 27450] Loss: 2.8736\n",
            "[Step 27460] Loss: 2.7571\n",
            "[Step 27470] Loss: 2.3657\n",
            "[Step 27480] Loss: 2.5016\n",
            "[Step 27490] Loss: 2.1807\n",
            "[Step 27500] Loss: 3.0644\n",
            "[Step 27510] Loss: 2.0322\n",
            "[Step 27520] Loss: 2.5041\n",
            "[Step 27530] Loss: 2.4612\n",
            "[Step 27540] Loss: 2.6354\n",
            "[Step 27550] Loss: 2.6298\n",
            "[Step 27560] Loss: 2.3266\n",
            "[Step 27570] Loss: 2.6427\n",
            "[Step 27580] Loss: 2.7647\n",
            "[Step 27590] Loss: 2.2380\n",
            "[Step 27600] Loss: 2.3433\n",
            "[Step 27610] Loss: 2.3141\n",
            "[Step 27620] Loss: 2.4767\n",
            "[Step 27630] Loss: 2.1965\n",
            "[Step 27640] Loss: 2.3536\n",
            "[Step 27650] Loss: 1.7795\n",
            "[Step 27660] Loss: 2.1604\n",
            "[Step 27670] Loss: 2.3326\n",
            "[Step 27680] Loss: 2.3710\n",
            "[Step 27690] Loss: 2.1732\n",
            "[Step 27700] Loss: 2.0662\n",
            "[Step 27710] Loss: 2.7610\n",
            "[Step 27720] Loss: 2.4186\n",
            "[Step 27730] Loss: 2.3341\n",
            "[Step 27740] Loss: 2.1960\n",
            "[Step 27750] Loss: 2.5427\n",
            "[Step 27760] Loss: 2.5468\n",
            "[Step 27770] Loss: 2.6150\n",
            "[Step 27780] Loss: 1.7055\n",
            "[Step 27790] Loss: 3.0156\n",
            "[Step 27800] Loss: 2.3824\n",
            "[Step 27810] Loss: 2.5655\n",
            "[Step 27820] Loss: 2.3642\n",
            "[Step 27830] Loss: 2.2001\n",
            "[Step 27840] Loss: 2.0548\n",
            "[Step 27850] Loss: 2.4967\n",
            "[Step 27860] Loss: 2.2535\n",
            "[Step 27870] Loss: 1.8700\n",
            "[Step 27880] Loss: 3.3011\n",
            "[Step 27890] Loss: 2.5461\n",
            "[Step 27900] Loss: 2.9360\n",
            "[Step 27910] Loss: 2.3973\n",
            "[Step 27920] Loss: 2.4355\n",
            "[Step 27930] Loss: 2.2787\n",
            "[Step 27940] Loss: 2.0022\n",
            "[Step 27950] Loss: 2.2778\n",
            "[Step 27960] Loss: 2.0765\n",
            "[Step 27970] Loss: 2.0600\n",
            "[Step 27980] Loss: 2.2896\n",
            "[Step 27990] Loss: 2.5904\n",
            "[Step 28000] Loss: 2.1459\n",
            "[Step 28010] Loss: 2.3198\n",
            "[Step 28020] Loss: 2.7586\n",
            "[Step 28030] Loss: 2.5297\n",
            "[Step 28040] Loss: 1.9366\n",
            "[Step 28050] Loss: 2.4651\n",
            "[Step 28060] Loss: 2.3615\n",
            "[Step 28070] Loss: 2.3925\n",
            "[Step 28080] Loss: 2.5775\n",
            "[Step 28090] Loss: 2.2494\n",
            "[Step 28100] Loss: 2.5607\n",
            "[Step 28110] Loss: 2.6765\n",
            "[Step 28120] Loss: 2.4540\n",
            "[Step 28130] Loss: 2.0985\n",
            "[Step 28140] Loss: 2.1325\n",
            "[Step 28150] Loss: 2.2799\n",
            "[Step 28160] Loss: 1.9692\n",
            "[Step 28170] Loss: 2.4881\n",
            "[Step 28180] Loss: 2.2738\n",
            "[Step 28190] Loss: 2.2207\n",
            "[Step 28200] Loss: 2.3568\n",
            "[Step 28210] Loss: 1.9725\n",
            "[Step 28220] Loss: 2.5879\n",
            "[Step 28230] Loss: 2.2990\n",
            "[Step 28240] Loss: 2.7295\n",
            "[Step 28250] Loss: 2.5665\n",
            "[Step 28260] Loss: 2.1426\n",
            "[Step 28270] Loss: 2.5297\n",
            "[Step 28280] Loss: 1.9597\n",
            "[Step 28290] Loss: 2.6541\n",
            "[Step 28300] Loss: 2.4213\n",
            "[Step 28310] Loss: 2.3125\n",
            "[Step 28320] Loss: 2.4836\n",
            "[Step 28330] Loss: 2.3569\n",
            "[Step 28340] Loss: 2.1761\n",
            "[Step 28350] Loss: 2.2964\n",
            "[Step 28360] Loss: 2.5570\n",
            "[Step 28370] Loss: 2.2482\n",
            "[Step 28380] Loss: 2.4931\n",
            "[Step 28390] Loss: 2.3942\n",
            "[Step 28400] Loss: 2.4783\n",
            "[Step 28410] Loss: 2.2969\n",
            "ðŸ“˜ Epoch 80 - Avg Training Loss: 2.3757\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4891 | Accuracy: 0.3750 | Precision: 0.3559\n",
            "[Step 28420] Loss: 2.0961\n",
            "[Step 28430] Loss: 2.3227\n",
            "[Step 28440] Loss: 2.1032\n",
            "[Step 28450] Loss: 2.4298\n",
            "[Step 28460] Loss: 2.1607\n",
            "[Step 28470] Loss: 1.8402\n",
            "[Step 28480] Loss: 2.4839\n",
            "[Step 28490] Loss: 1.8715\n",
            "[Step 28500] Loss: 2.0500\n",
            "[Step 28510] Loss: 2.7862\n",
            "[Step 28520] Loss: 1.7278\n",
            "[Step 28530] Loss: 2.2797\n",
            "[Step 28540] Loss: 2.5534\n",
            "[Step 28550] Loss: 2.1554\n",
            "[Step 28560] Loss: 1.9725\n",
            "[Step 28570] Loss: 2.3827\n",
            "[Step 28580] Loss: 2.1475\n",
            "[Step 28590] Loss: 2.2980\n",
            "[Step 28600] Loss: 2.2608\n",
            "[Step 28610] Loss: 2.1839\n",
            "[Step 28620] Loss: 2.4046\n",
            "[Step 28630] Loss: 2.4044\n",
            "[Step 28640] Loss: 2.0463\n",
            "[Step 28650] Loss: 2.4287\n",
            "[Step 28660] Loss: 2.8373\n",
            "[Step 28670] Loss: 2.5316\n",
            "[Step 28680] Loss: 2.3760\n",
            "[Step 28690] Loss: 3.1962\n",
            "[Step 28700] Loss: 1.9735\n",
            "[Step 28710] Loss: 2.8565\n",
            "[Step 28720] Loss: 2.0361\n",
            "[Step 28730] Loss: 2.5801\n",
            "[Step 28740] Loss: 1.9615\n",
            "[Step 28750] Loss: 2.8774\n",
            "[Step 28760] Loss: 2.8149\n",
            "[Step 28770] Loss: 2.2650\n",
            "[Step 28780] Loss: 2.3918\n",
            "[Step 28790] Loss: 2.4566\n",
            "[Step 28800] Loss: 2.1073\n",
            "[Step 28810] Loss: 2.8686\n",
            "[Step 28820] Loss: 2.0614\n",
            "[Step 28830] Loss: 2.4070\n",
            "[Step 28840] Loss: 2.9524\n",
            "[Step 28850] Loss: 2.5376\n",
            "[Step 28860] Loss: 2.5552\n",
            "[Step 28870] Loss: 2.2656\n",
            "[Step 28880] Loss: 2.6277\n",
            "[Step 28890] Loss: 1.8974\n",
            "[Step 28900] Loss: 1.8801\n",
            "[Step 28910] Loss: 2.5132\n",
            "[Step 28920] Loss: 2.3391\n",
            "[Step 28930] Loss: 2.8385\n",
            "[Step 28940] Loss: 2.5161\n",
            "[Step 28950] Loss: 2.2917\n",
            "[Step 28960] Loss: 2.1211\n",
            "[Step 28970] Loss: 2.2298\n",
            "[Step 28980] Loss: 2.1054\n",
            "[Step 28990] Loss: 2.0192\n",
            "[Step 29000] Loss: 2.7794\n",
            "[Step 29010] Loss: 2.6377\n",
            "[Step 29020] Loss: 2.1622\n",
            "[Step 29030] Loss: 2.3458\n",
            "[Step 29040] Loss: 2.3719\n",
            "[Step 29050] Loss: 2.7588\n",
            "[Step 29060] Loss: 2.3747\n",
            "[Step 29070] Loss: 2.1373\n",
            "[Step 29080] Loss: 2.6690\n",
            "[Step 29090] Loss: 1.8873\n",
            "[Step 29100] Loss: 2.5521\n",
            "[Step 29110] Loss: 2.5076\n",
            "[Step 29120] Loss: 2.7009\n",
            "[Step 29130] Loss: 2.8085\n",
            "[Step 29140] Loss: 2.0877\n",
            "[Step 29150] Loss: 2.6004\n",
            "[Step 29160] Loss: 2.5279\n",
            "[Step 29170] Loss: 2.9733\n",
            "[Step 29180] Loss: 2.3224\n",
            "[Step 29190] Loss: 3.1412\n",
            "[Step 29200] Loss: 2.3859\n",
            "[Step 29210] Loss: 2.3164\n",
            "[Step 29220] Loss: 2.3571\n",
            "[Step 29230] Loss: 2.2338\n",
            "[Step 29240] Loss: 2.5368\n",
            "[Step 29250] Loss: 2.1169\n",
            "[Step 29260] Loss: 1.7374\n",
            "[Step 29270] Loss: 2.5256\n",
            "[Step 29280] Loss: 2.7350\n",
            "[Step 29290] Loss: 2.4173\n",
            "[Step 29300] Loss: 2.3381\n",
            "[Step 29310] Loss: 2.3656\n",
            "[Step 29320] Loss: 2.0221\n",
            "[Step 29330] Loss: 3.1122\n",
            "[Step 29340] Loss: 2.2409\n",
            "[Step 29350] Loss: 2.0108\n",
            "[Step 29360] Loss: 2.5184\n",
            "[Step 29370] Loss: 2.6313\n",
            "[Step 29380] Loss: 2.0364\n",
            "[Step 29390] Loss: 2.5531\n",
            "[Step 29400] Loss: 2.5638\n",
            "[Step 29410] Loss: 2.1407\n",
            "[Step 29420] Loss: 2.6108\n",
            "[Step 29430] Loss: 2.5908\n",
            "[Step 29440] Loss: 2.1088\n",
            "[Step 29450] Loss: 2.3257\n",
            "[Step 29460] Loss: 2.8086\n",
            "[Step 29470] Loss: 2.2249\n",
            "[Step 29480] Loss: 2.9356\n",
            "[Step 29490] Loss: 2.5840\n",
            "[Step 29500] Loss: 2.5841\n",
            "[Step 29510] Loss: 2.5917\n",
            "[Step 29520] Loss: 2.3806\n",
            "[Step 29530] Loss: 2.6519\n",
            "[Step 29540] Loss: 2.1823\n",
            "[Step 29550] Loss: 2.1125\n",
            "[Step 29560] Loss: 2.3058\n",
            "[Step 29570] Loss: 2.0312\n",
            "[Step 29580] Loss: 2.3382\n",
            "[Step 29590] Loss: 2.1413\n",
            "[Step 29600] Loss: 2.0759\n",
            "[Step 29610] Loss: 2.2607\n",
            "[Step 29620] Loss: 2.3546\n",
            "[Step 29630] Loss: 1.9940\n",
            "[Step 29640] Loss: 2.6936\n",
            "[Step 29650] Loss: 2.8528\n",
            "[Step 29660] Loss: 2.1442\n",
            "[Step 29670] Loss: 2.3799\n",
            "[Step 29680] Loss: 2.5325\n",
            "[Step 29690] Loss: 2.1911\n",
            "[Step 29700] Loss: 2.0575\n",
            "[Step 29710] Loss: 2.1033\n",
            "[Step 29720] Loss: 2.1125\n",
            "[Step 29730] Loss: 2.3983\n",
            "[Step 29740] Loss: 2.7796\n",
            "[Step 29750] Loss: 2.3487\n",
            "[Step 29760] Loss: 1.8450\n",
            "[Step 29770] Loss: 2.6150\n",
            "[Step 29780] Loss: 2.5694\n",
            "[Step 29790] Loss: 2.4256\n",
            "[Step 29800] Loss: 2.6083\n",
            "[Step 29810] Loss: 2.5687\n",
            "[Step 29820] Loss: 2.6824\n",
            "[Step 29830] Loss: 2.4840\n",
            "[Step 29840] Loss: 2.8215\n",
            "[Step 29850] Loss: 2.8352\n",
            "[Step 29860] Loss: 2.2098\n",
            "[Step 29870] Loss: 2.4002\n",
            "[Step 29880] Loss: 2.4590\n",
            "[Step 29890] Loss: 1.7845\n",
            "[Step 29900] Loss: 2.4833\n",
            "[Step 29910] Loss: 2.9828\n",
            "[Step 29920] Loss: 2.1264\n",
            "[Step 29930] Loss: 1.9872\n",
            "[Step 29940] Loss: 2.9773\n",
            "[Step 29950] Loss: 2.3369\n",
            "[Step 29960] Loss: 2.6158\n",
            "[Step 29970] Loss: 2.5729\n",
            "[Step 29980] Loss: 2.1289\n",
            "[Step 29990] Loss: 2.5598\n",
            "[Step 30000] Loss: 2.3039\n",
            "[Step 30010] Loss: 2.3210\n",
            "[Step 30020] Loss: 3.0723\n",
            "[Step 30030] Loss: 2.2215\n",
            "[Step 30040] Loss: 2.0695\n",
            "[Step 30050] Loss: 2.1215\n",
            "[Step 30060] Loss: 2.5034\n",
            "[Step 30070] Loss: 2.3320\n",
            "[Step 30080] Loss: 2.1368\n",
            "[Step 30090] Loss: 2.6223\n",
            "[Step 30100] Loss: 2.1397\n",
            "[Step 30110] Loss: 2.6583\n",
            "[Step 30120] Loss: 1.8466\n",
            "[Step 30130] Loss: 1.8683\n",
            "[Step 30140] Loss: 2.4256\n",
            "[Step 30150] Loss: 2.3597\n",
            "[Step 30160] Loss: 2.4899\n",
            "[Step 30170] Loss: 2.2610\n",
            "[Step 30180] Loss: 2.2439\n",
            "[Step 30190] Loss: 2.5132\n",
            "[Step 30200] Loss: 2.6428\n",
            "[Step 30210] Loss: 2.6632\n",
            "[Step 30220] Loss: 2.8762\n",
            "[Step 30230] Loss: 2.3355\n",
            "[Step 30240] Loss: 1.8996\n",
            "[Step 30250] Loss: 2.5779\n",
            "[Step 30260] Loss: 2.1953\n",
            "[Step 30270] Loss: 2.6281\n",
            "[Step 30280] Loss: 2.6902\n",
            "[Step 30290] Loss: 2.1596\n",
            "[Step 30300] Loss: 2.3213\n",
            "ðŸ“˜ Epoch 81 - Avg Training Loss: 2.3811\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4928 | Accuracy: 0.3739 | Precision: 0.3522\n",
            "[Step 30310] Loss: 2.0545\n",
            "[Step 30320] Loss: 2.8090\n",
            "[Step 30330] Loss: 2.2872\n",
            "[Step 30340] Loss: 2.3762\n",
            "[Step 30350] Loss: 2.2675\n",
            "[Step 30360] Loss: 2.4071\n",
            "[Step 30370] Loss: 2.1855\n",
            "[Step 30380] Loss: 2.3785\n",
            "[Step 30390] Loss: 2.4681\n",
            "[Step 30400] Loss: 2.9321\n",
            "[Step 30410] Loss: 2.4420\n",
            "[Step 30420] Loss: 2.6068\n",
            "[Step 30430] Loss: 2.6505\n",
            "[Step 30440] Loss: 1.9973\n",
            "[Step 30450] Loss: 2.2345\n",
            "[Step 30460] Loss: 2.2916\n",
            "[Step 30470] Loss: 1.9253\n",
            "[Step 30480] Loss: 2.3706\n",
            "[Step 30490] Loss: 2.4298\n",
            "[Step 30500] Loss: 2.0422\n",
            "[Step 30510] Loss: 2.9074\n",
            "[Step 30520] Loss: 2.4335\n",
            "[Step 30530] Loss: 1.9241\n",
            "[Step 30540] Loss: 2.2875\n",
            "[Step 30550] Loss: 2.2515\n",
            "[Step 30560] Loss: 2.2664\n",
            "[Step 30570] Loss: 2.3887\n",
            "[Step 30580] Loss: 2.4679\n",
            "[Step 30590] Loss: 2.6375\n",
            "[Step 30600] Loss: 2.8176\n",
            "[Step 30610] Loss: 1.6204\n",
            "[Step 30620] Loss: 1.9248\n",
            "[Step 30630] Loss: 2.5266\n",
            "[Step 30640] Loss: 2.5342\n",
            "[Step 30650] Loss: 2.3219\n",
            "[Step 30660] Loss: 2.5351\n",
            "[Step 30670] Loss: 2.0897\n",
            "[Step 30680] Loss: 2.2881\n",
            "[Step 30690] Loss: 2.5554\n",
            "[Step 30700] Loss: 2.6096\n",
            "[Step 30710] Loss: 2.6294\n",
            "[Step 30720] Loss: 2.2344\n",
            "[Step 30730] Loss: 2.3005\n",
            "[Step 30740] Loss: 2.2703\n",
            "[Step 30750] Loss: 2.6374\n",
            "[Step 30760] Loss: 2.9180\n",
            "[Step 30770] Loss: 2.5578\n",
            "[Step 30780] Loss: 2.0085\n",
            "[Step 30790] Loss: 2.5905\n",
            "[Step 30800] Loss: 2.8589\n",
            "[Step 30810] Loss: 2.2951\n",
            "[Step 30820] Loss: 2.7626\n",
            "[Step 30830] Loss: 2.4490\n",
            "[Step 30840] Loss: 2.6365\n",
            "[Step 30850] Loss: 1.7839\n",
            "[Step 30860] Loss: 3.0719\n",
            "[Step 30870] Loss: 2.2461\n",
            "[Step 30880] Loss: 2.3070\n",
            "[Step 30890] Loss: 1.8423\n",
            "[Step 30900] Loss: 1.9199\n",
            "[Step 30910] Loss: 2.6123\n",
            "[Step 30920] Loss: 2.8774\n",
            "[Step 30930] Loss: 2.7429\n",
            "[Step 30940] Loss: 2.3886\n",
            "[Step 30950] Loss: 2.4549\n",
            "[Step 30960] Loss: 2.0104\n",
            "[Step 30970] Loss: 2.6633\n",
            "[Step 30980] Loss: 2.0720\n",
            "[Step 30990] Loss: 2.7340\n",
            "[Step 31000] Loss: 2.4542\n",
            "[Step 31010] Loss: 2.0335\n",
            "[Step 31020] Loss: 2.5883\n",
            "[Step 31030] Loss: 2.7467\n",
            "[Step 31040] Loss: 2.4783\n",
            "[Step 31050] Loss: 2.5447\n",
            "[Step 31060] Loss: 2.1889\n",
            "[Step 31070] Loss: 2.5693\n",
            "[Step 31080] Loss: 2.3796\n",
            "[Step 31090] Loss: 2.9182\n",
            "[Step 31100] Loss: 2.6285\n",
            "[Step 31110] Loss: 2.3973\n",
            "[Step 31120] Loss: 2.1804\n",
            "[Step 31130] Loss: 2.7086\n",
            "[Step 31140] Loss: 1.7665\n",
            "[Step 31150] Loss: 2.2395\n",
            "[Step 31160] Loss: 2.3375\n",
            "[Step 31170] Loss: 2.5658\n",
            "[Step 31180] Loss: 2.4014\n",
            "[Step 31190] Loss: 2.7128\n",
            "[Step 31200] Loss: 2.4303\n",
            "[Step 31210] Loss: 2.3521\n",
            "[Step 31220] Loss: 2.4154\n",
            "[Step 31230] Loss: 2.1030\n",
            "[Step 31240] Loss: 2.2095\n",
            "[Step 31250] Loss: 2.4945\n",
            "[Step 31260] Loss: 2.4969\n",
            "[Step 31270] Loss: 2.5553\n",
            "[Step 31280] Loss: 2.2100\n",
            "[Step 31290] Loss: 2.8481\n",
            "[Step 31300] Loss: 2.6421\n",
            "[Step 31310] Loss: 2.7524\n",
            "[Step 31320] Loss: 2.5580\n",
            "[Step 31330] Loss: 2.2132\n",
            "[Step 31340] Loss: 2.0904\n",
            "[Step 31350] Loss: 2.5644\n",
            "[Step 31360] Loss: 1.8708\n",
            "[Step 31370] Loss: 2.5239\n",
            "[Step 31380] Loss: 2.4608\n",
            "[Step 31390] Loss: 2.2431\n",
            "[Step 31400] Loss: 2.7563\n",
            "[Step 31410] Loss: 2.1924\n",
            "[Step 31420] Loss: 2.3277\n",
            "[Step 31430] Loss: 2.4648\n",
            "[Step 31440] Loss: 2.7142\n",
            "[Step 31450] Loss: 2.5305\n",
            "[Step 31460] Loss: 2.0675\n",
            "[Step 31470] Loss: 2.6255\n",
            "[Step 31480] Loss: 2.4793\n",
            "[Step 31490] Loss: 1.9174\n",
            "[Step 31500] Loss: 2.4497\n",
            "[Step 31510] Loss: 2.2293\n",
            "[Step 31520] Loss: 2.1638\n",
            "[Step 31530] Loss: 2.0652\n",
            "[Step 31540] Loss: 2.6255\n",
            "[Step 31550] Loss: 2.0923\n",
            "[Step 31560] Loss: 2.1984\n",
            "[Step 31570] Loss: 2.4780\n",
            "[Step 31580] Loss: 2.4328\n",
            "[Step 31590] Loss: 2.3653\n",
            "[Step 31600] Loss: 2.3873\n",
            "[Step 31610] Loss: 2.0845\n",
            "[Step 31620] Loss: 2.6171\n",
            "[Step 31630] Loss: 2.0169\n",
            "[Step 31640] Loss: 2.6990\n",
            "[Step 31650] Loss: 2.3426\n",
            "[Step 31660] Loss: 1.9003\n",
            "[Step 31670] Loss: 2.3167\n",
            "[Step 31680] Loss: 1.9238\n",
            "[Step 31690] Loss: 2.3492\n",
            "[Step 31700] Loss: 2.1110\n",
            "[Step 31710] Loss: 2.1600\n",
            "[Step 31720] Loss: 2.3810\n",
            "[Step 31730] Loss: 2.5501\n",
            "[Step 31740] Loss: 2.5457\n",
            "[Step 31750] Loss: 2.4182\n",
            "[Step 31760] Loss: 2.0721\n",
            "[Step 31770] Loss: 2.8884\n",
            "[Step 31780] Loss: 2.5647\n",
            "[Step 31790] Loss: 2.2534\n",
            "[Step 31800] Loss: 2.2663\n",
            "[Step 31810] Loss: 3.0380\n",
            "[Step 31820] Loss: 2.5863\n",
            "[Step 31830] Loss: 3.2898\n",
            "[Step 31840] Loss: 2.9298\n",
            "[Step 31850] Loss: 2.4154\n",
            "[Step 31860] Loss: 2.1018\n",
            "[Step 31870] Loss: 2.3905\n",
            "[Step 31880] Loss: 2.4652\n",
            "[Step 31890] Loss: 2.1915\n",
            "[Step 31900] Loss: 2.4121\n",
            "[Step 31910] Loss: 2.3618\n",
            "[Step 31920] Loss: 2.4128\n",
            "[Step 31930] Loss: 2.5292\n",
            "[Step 31940] Loss: 2.5587\n",
            "[Step 31950] Loss: 2.7617\n",
            "[Step 31960] Loss: 2.6097\n",
            "[Step 31970] Loss: 2.6244\n",
            "[Step 31980] Loss: 2.2765\n",
            "[Step 31990] Loss: 2.2692\n",
            "[Step 32000] Loss: 2.6176\n",
            "[Step 32010] Loss: 2.2115\n",
            "[Step 32020] Loss: 2.2833\n",
            "[Step 32030] Loss: 2.9408\n",
            "[Step 32040] Loss: 2.0719\n",
            "[Step 32050] Loss: 2.6198\n",
            "[Step 32060] Loss: 1.7427\n",
            "[Step 32070] Loss: 2.3998\n",
            "[Step 32080] Loss: 2.1931\n",
            "[Step 32090] Loss: 2.3810\n",
            "[Step 32100] Loss: 2.0940\n",
            "[Step 32110] Loss: 2.8397\n",
            "[Step 32120] Loss: 2.1546\n",
            "[Step 32130] Loss: 2.1840\n",
            "[Step 32140] Loss: 2.3013\n",
            "[Step 32150] Loss: 2.4590\n",
            "[Step 32160] Loss: 2.4963\n",
            "[Step 32170] Loss: 2.1408\n",
            "[Step 32180] Loss: 2.0628\n",
            "[Step 32190] Loss: 2.2700\n",
            "ðŸ“˜ Epoch 82 - Avg Training Loss: 2.3840\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4967 | Accuracy: 0.3758 | Precision: 0.3635\n",
            "[Step 32200] Loss: 2.3491\n",
            "[Step 32210] Loss: 2.4556\n",
            "[Step 32220] Loss: 2.4872\n",
            "[Step 32230] Loss: 1.7252\n",
            "[Step 32240] Loss: 3.0183\n",
            "[Step 32250] Loss: 2.0440\n",
            "[Step 32260] Loss: 2.4632\n",
            "[Step 32270] Loss: 2.5850\n",
            "[Step 32280] Loss: 2.0184\n",
            "[Step 32290] Loss: 2.2408\n",
            "[Step 32300] Loss: 2.2778\n",
            "[Step 32310] Loss: 2.0897\n",
            "[Step 32320] Loss: 2.5883\n",
            "[Step 32330] Loss: 2.0745\n",
            "[Step 32340] Loss: 2.0254\n",
            "[Step 32350] Loss: 2.3962\n",
            "[Step 32360] Loss: 2.7250\n",
            "[Step 32370] Loss: 2.5693\n",
            "[Step 32380] Loss: 2.2213\n",
            "[Step 32390] Loss: 2.3348\n",
            "[Step 32400] Loss: 2.5084\n",
            "[Step 32410] Loss: 1.9659\n",
            "[Step 32420] Loss: 2.5426\n",
            "[Step 32430] Loss: 2.0980\n",
            "[Step 32440] Loss: 2.2345\n",
            "[Step 32450] Loss: 2.2190\n",
            "[Step 32460] Loss: 2.2509\n",
            "[Step 32470] Loss: 2.5995\n",
            "[Step 32480] Loss: 2.4108\n",
            "[Step 32490] Loss: 2.6672\n",
            "[Step 32500] Loss: 2.5448\n",
            "[Step 32510] Loss: 2.9590\n",
            "[Step 32520] Loss: 2.4756\n",
            "[Step 32530] Loss: 2.4703\n",
            "[Step 32540] Loss: 2.8939\n",
            "[Step 32550] Loss: 2.2427\n",
            "[Step 32560] Loss: 2.1404\n",
            "[Step 32570] Loss: 2.2983\n",
            "[Step 32580] Loss: 2.3956\n",
            "[Step 32590] Loss: 2.5191\n",
            "[Step 32600] Loss: 2.3850\n",
            "[Step 32610] Loss: 2.7433\n",
            "[Step 32620] Loss: 2.0148\n",
            "[Step 32630] Loss: 2.4906\n",
            "[Step 32640] Loss: 2.6245\n",
            "[Step 32650] Loss: 2.5308\n",
            "[Step 32660] Loss: 2.3800\n",
            "[Step 32670] Loss: 1.6897\n",
            "[Step 32680] Loss: 2.3739\n",
            "[Step 32690] Loss: 2.2803\n",
            "[Step 32700] Loss: 2.8864\n",
            "[Step 32710] Loss: 2.3520\n",
            "[Step 32720] Loss: 2.6685\n",
            "[Step 32730] Loss: 2.5979\n",
            "[Step 32740] Loss: 2.4650\n",
            "[Step 32750] Loss: 2.4577\n",
            "[Step 32760] Loss: 2.1693\n",
            "[Step 32770] Loss: 3.0805\n",
            "[Step 32780] Loss: 2.0733\n",
            "[Step 32790] Loss: 2.3878\n",
            "[Step 32800] Loss: 2.6203\n",
            "[Step 32810] Loss: 2.8776\n",
            "[Step 32820] Loss: 1.7747\n",
            "[Step 32830] Loss: 2.5271\n",
            "[Step 32840] Loss: 2.2125\n",
            "[Step 32850] Loss: 2.4476\n",
            "[Step 32860] Loss: 2.5514\n",
            "[Step 32870] Loss: 2.4221\n",
            "[Step 32880] Loss: 2.8075\n",
            "[Step 32890] Loss: 2.7925\n",
            "[Step 32900] Loss: 2.4747\n",
            "[Step 32910] Loss: 2.5881\n",
            "[Step 32920] Loss: 2.0940\n",
            "[Step 32930] Loss: 2.5914\n",
            "[Step 32940] Loss: 2.1321\n",
            "[Step 32950] Loss: 3.2069\n",
            "[Step 32960] Loss: 1.9613\n",
            "[Step 32970] Loss: 2.1818\n",
            "[Step 32980] Loss: 2.5858\n",
            "[Step 32990] Loss: 2.8457\n",
            "[Step 33000] Loss: 2.4006\n",
            "[Step 33010] Loss: 1.9183\n",
            "[Step 33020] Loss: 2.5537\n",
            "[Step 33030] Loss: 1.9816\n",
            "[Step 33040] Loss: 1.6508\n",
            "[Step 33050] Loss: 2.3192\n",
            "[Step 33060] Loss: 2.3189\n",
            "[Step 33070] Loss: 2.2455\n",
            "[Step 33080] Loss: 2.5900\n",
            "[Step 33090] Loss: 2.2833\n",
            "[Step 33100] Loss: 2.8334\n",
            "[Step 33110] Loss: 2.5464\n",
            "[Step 33120] Loss: 2.3117\n",
            "[Step 33130] Loss: 2.4299\n",
            "[Step 33140] Loss: 2.7936\n",
            "[Step 33150] Loss: 2.5788\n",
            "[Step 33160] Loss: 2.6189\n",
            "[Step 33170] Loss: 2.4834\n",
            "[Step 33180] Loss: 2.5265\n",
            "[Step 33190] Loss: 2.3165\n",
            "[Step 33200] Loss: 2.4712\n",
            "[Step 33210] Loss: 1.9712\n",
            "[Step 33220] Loss: 2.2459\n",
            "[Step 33230] Loss: 2.4432\n",
            "[Step 33240] Loss: 2.4077\n",
            "[Step 33250] Loss: 2.0156\n",
            "[Step 33260] Loss: 1.8788\n",
            "[Step 33270] Loss: 2.9908\n",
            "[Step 33280] Loss: 2.6337\n",
            "[Step 33290] Loss: 2.5640\n",
            "[Step 33300] Loss: 2.2017\n",
            "[Step 33310] Loss: 2.9957\n",
            "[Step 33320] Loss: 2.4398\n",
            "[Step 33330] Loss: 2.7450\n",
            "[Step 33340] Loss: 2.5494\n",
            "[Step 33350] Loss: 2.4047\n",
            "[Step 33360] Loss: 3.0379\n",
            "[Step 33370] Loss: 2.7933\n",
            "[Step 33380] Loss: 1.6779\n",
            "[Step 33390] Loss: 2.5461\n",
            "[Step 33400] Loss: 2.0156\n",
            "[Step 33410] Loss: 2.6663\n",
            "[Step 33420] Loss: 2.8575\n",
            "[Step 33430] Loss: 2.3757\n",
            "[Step 33440] Loss: 2.7247\n",
            "[Step 33450] Loss: 2.3808\n",
            "[Step 33460] Loss: 2.2018\n",
            "[Step 33470] Loss: 2.4417\n",
            "[Step 33480] Loss: 2.3077\n",
            "[Step 33490] Loss: 2.4174\n",
            "[Step 33500] Loss: 2.4890\n",
            "[Step 33510] Loss: 2.4937\n",
            "[Step 33520] Loss: 2.5781\n",
            "[Step 33530] Loss: 2.1068\n",
            "[Step 33540] Loss: 2.1369\n",
            "[Step 33550] Loss: 2.5822\n",
            "[Step 33560] Loss: 2.7149\n",
            "[Step 33570] Loss: 1.9171\n",
            "[Step 33580] Loss: 2.5542\n",
            "[Step 33590] Loss: 2.5261\n",
            "[Step 33600] Loss: 2.2507\n",
            "[Step 33610] Loss: 2.3590\n",
            "[Step 33620] Loss: 2.5749\n",
            "[Step 33630] Loss: 2.3851\n",
            "[Step 33640] Loss: 2.7819\n",
            "[Step 33650] Loss: 2.2036\n",
            "[Step 33660] Loss: 2.6290\n",
            "[Step 33670] Loss: 2.4573\n",
            "[Step 33680] Loss: 2.0771\n",
            "[Step 33690] Loss: 2.4503\n",
            "[Step 33700] Loss: 1.8155\n",
            "[Step 33710] Loss: 2.4103\n",
            "[Step 33720] Loss: 2.6309\n",
            "[Step 33730] Loss: 2.5902\n",
            "[Step 33740] Loss: 2.8225\n",
            "[Step 33750] Loss: 2.4225\n",
            "[Step 33760] Loss: 2.3321\n",
            "[Step 33770] Loss: 3.0603\n",
            "[Step 33780] Loss: 2.0498\n",
            "[Step 33790] Loss: 2.5395\n",
            "[Step 33800] Loss: 2.2553\n",
            "[Step 33810] Loss: 2.0916\n",
            "[Step 33820] Loss: 2.3742\n",
            "[Step 33830] Loss: 2.1387\n",
            "[Step 33840] Loss: 2.4531\n",
            "[Step 33850] Loss: 2.3826\n",
            "[Step 33860] Loss: 2.5339\n",
            "[Step 33870] Loss: 2.7121\n",
            "[Step 33880] Loss: 2.3381\n",
            "[Step 33890] Loss: 2.4535\n",
            "[Step 33900] Loss: 2.6467\n",
            "[Step 33910] Loss: 2.6146\n",
            "[Step 33920] Loss: 2.0654\n",
            "[Step 33930] Loss: 2.6849\n",
            "[Step 33940] Loss: 2.3058\n",
            "[Step 33950] Loss: 2.3682\n",
            "[Step 33960] Loss: 2.1440\n",
            "[Step 33970] Loss: 2.4910\n",
            "[Step 33980] Loss: 2.8119\n",
            "[Step 33990] Loss: 2.3127\n",
            "[Step 34000] Loss: 2.4110\n",
            "[Step 34010] Loss: 1.9594\n",
            "[Step 34020] Loss: 2.0491\n",
            "[Step 34030] Loss: 2.3424\n",
            "[Step 34040] Loss: 1.9258\n",
            "[Step 34050] Loss: 2.6477\n",
            "[Step 34060] Loss: 2.5250\n",
            "[Step 34070] Loss: 2.4634\n",
            "[Step 34080] Loss: 2.5429\n",
            "[Step 34090] Loss: 1.9392\n",
            "ðŸ“˜ Epoch 83 - Avg Training Loss: 2.3865\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4994 | Accuracy: 0.3752 | Precision: 0.3600\n",
            "[Step 34100] Loss: 2.4561\n",
            "[Step 34110] Loss: 2.3726\n",
            "[Step 34120] Loss: 2.3487\n",
            "[Step 34130] Loss: 2.7375\n",
            "[Step 34140] Loss: 2.3542\n",
            "[Step 34150] Loss: 2.5472\n",
            "[Step 34160] Loss: 2.2596\n",
            "[Step 34170] Loss: 2.9028\n",
            "[Step 34180] Loss: 2.7017\n",
            "[Step 34190] Loss: 1.7580\n",
            "[Step 34200] Loss: 2.2739\n",
            "[Step 34210] Loss: 2.4833\n",
            "[Step 34220] Loss: 2.1945\n",
            "[Step 34230] Loss: 2.2944\n",
            "[Step 34240] Loss: 2.2673\n",
            "[Step 34250] Loss: 2.7420\n",
            "[Step 34260] Loss: 2.3631\n",
            "[Step 34270] Loss: 2.4921\n",
            "[Step 34280] Loss: 2.6501\n",
            "[Step 34290] Loss: 2.5830\n",
            "[Step 34300] Loss: 2.2372\n",
            "[Step 34310] Loss: 2.1231\n",
            "[Step 34320] Loss: 1.9652\n",
            "[Step 34330] Loss: 2.4696\n",
            "[Step 34340] Loss: 3.0950\n",
            "[Step 34350] Loss: 2.2400\n",
            "[Step 34360] Loss: 2.4748\n",
            "[Step 34370] Loss: 2.4396\n",
            "[Step 34380] Loss: 2.1946\n",
            "[Step 34390] Loss: 2.2423\n",
            "[Step 34400] Loss: 2.8858\n",
            "[Step 34410] Loss: 2.1411\n",
            "[Step 34420] Loss: 2.1775\n",
            "[Step 34430] Loss: 2.0819\n",
            "[Step 34440] Loss: 1.6211\n",
            "[Step 34450] Loss: 2.0642\n",
            "[Step 34460] Loss: 2.4479\n",
            "[Step 34470] Loss: 2.2325\n",
            "[Step 34480] Loss: 2.3816\n",
            "[Step 34490] Loss: 2.2351\n",
            "[Step 34500] Loss: 2.4834\n",
            "[Step 34510] Loss: 2.4515\n",
            "[Step 34520] Loss: 2.4104\n",
            "[Step 34530] Loss: 2.3425\n",
            "[Step 34540] Loss: 2.4011\n",
            "[Step 34550] Loss: 2.5872\n",
            "[Step 34560] Loss: 2.5404\n",
            "[Step 34570] Loss: 2.7714\n",
            "[Step 34580] Loss: 2.2283\n",
            "[Step 34590] Loss: 2.2420\n",
            "[Step 34600] Loss: 2.3377\n",
            "[Step 34610] Loss: 2.3722\n",
            "[Step 34620] Loss: 2.0677\n",
            "[Step 34630] Loss: 2.6214\n",
            "[Step 34640] Loss: 2.2494\n",
            "[Step 34650] Loss: 1.9358\n",
            "[Step 34660] Loss: 2.2593\n",
            "[Step 34670] Loss: 2.3410\n",
            "[Step 34680] Loss: 2.6228\n",
            "[Step 34690] Loss: 2.0995\n",
            "[Step 34700] Loss: 2.3407\n",
            "[Step 34710] Loss: 2.2997\n",
            "[Step 34720] Loss: 2.5107\n",
            "[Step 34730] Loss: 1.9832\n",
            "[Step 34740] Loss: 2.3743\n",
            "[Step 34750] Loss: 2.6262\n",
            "[Step 34760] Loss: 1.9710\n",
            "[Step 34770] Loss: 2.5937\n",
            "[Step 34780] Loss: 2.4683\n",
            "[Step 34790] Loss: 2.1578\n",
            "[Step 34800] Loss: 2.3547\n",
            "[Step 34810] Loss: 2.4902\n",
            "[Step 34820] Loss: 1.9928\n",
            "[Step 34830] Loss: 2.6202\n",
            "[Step 34840] Loss: 2.5606\n",
            "[Step 34850] Loss: 2.7074\n",
            "[Step 34860] Loss: 2.1669\n",
            "[Step 34870] Loss: 2.4482\n",
            "[Step 34880] Loss: 2.5254\n",
            "[Step 34890] Loss: 2.0633\n",
            "[Step 34900] Loss: 2.8164\n",
            "[Step 34910] Loss: 2.3993\n",
            "[Step 34920] Loss: 2.6376\n",
            "[Step 34930] Loss: 2.3012\n",
            "[Step 34940] Loss: 2.3461\n",
            "[Step 34950] Loss: 2.2053\n",
            "[Step 34960] Loss: 2.3367\n",
            "[Step 34970] Loss: 2.6739\n",
            "[Step 34980] Loss: 2.7417\n",
            "[Step 34990] Loss: 2.5458\n",
            "[Step 35000] Loss: 2.2135\n",
            "[Step 35010] Loss: 2.4360\n",
            "[Step 35020] Loss: 2.6935\n",
            "[Step 35030] Loss: 2.0865\n",
            "[Step 35040] Loss: 2.3636\n",
            "[Step 35050] Loss: 2.6407\n",
            "[Step 35060] Loss: 2.8007\n",
            "[Step 35070] Loss: 2.5513\n",
            "[Step 35080] Loss: 2.2599\n",
            "[Step 35090] Loss: 2.4645\n",
            "[Step 35100] Loss: 2.9573\n",
            "[Step 35110] Loss: 2.1055\n",
            "[Step 35120] Loss: 1.9931\n",
            "[Step 35130] Loss: 2.0561\n",
            "[Step 35140] Loss: 2.4639\n",
            "[Step 35150] Loss: 2.6605\n",
            "[Step 35160] Loss: 2.3963\n",
            "[Step 35170] Loss: 1.9211\n",
            "[Step 35180] Loss: 2.6426\n",
            "[Step 35190] Loss: 2.4023\n",
            "[Step 35200] Loss: 2.7269\n",
            "[Step 35210] Loss: 2.4375\n",
            "[Step 35220] Loss: 2.4258\n",
            "[Step 35230] Loss: 2.5162\n",
            "[Step 35240] Loss: 2.4066\n",
            "[Step 35250] Loss: 2.7788\n",
            "[Step 35260] Loss: 2.5599\n",
            "[Step 35270] Loss: 2.2145\n",
            "[Step 35280] Loss: 2.5105\n",
            "[Step 35290] Loss: 2.3978\n",
            "[Step 35300] Loss: 2.2867\n",
            "[Step 35310] Loss: 2.4620\n",
            "[Step 35320] Loss: 2.0589\n",
            "[Step 35330] Loss: 2.5103\n",
            "[Step 35340] Loss: 2.4706\n",
            "[Step 35350] Loss: 2.5432\n",
            "[Step 35360] Loss: 2.0820\n",
            "[Step 35370] Loss: 2.5294\n",
            "[Step 35380] Loss: 2.2285\n",
            "[Step 35390] Loss: 2.2817\n",
            "[Step 35400] Loss: 2.5702\n",
            "[Step 35410] Loss: 2.2149\n",
            "[Step 35420] Loss: 3.0260\n",
            "[Step 35430] Loss: 2.9747\n",
            "[Step 35440] Loss: 2.4633\n",
            "[Step 35450] Loss: 1.9726\n",
            "[Step 35460] Loss: 2.6786\n",
            "[Step 35470] Loss: 2.9992\n",
            "[Step 35480] Loss: 2.5456\n",
            "[Step 35490] Loss: 2.4755\n",
            "[Step 35500] Loss: 2.1469\n",
            "[Step 35510] Loss: 2.3924\n",
            "[Step 35520] Loss: 2.5744\n",
            "[Step 35530] Loss: 2.1093\n",
            "[Step 35540] Loss: 1.8994\n",
            "[Step 35550] Loss: 2.5969\n",
            "[Step 35560] Loss: 2.1786\n",
            "[Step 35570] Loss: 2.6977\n",
            "[Step 35580] Loss: 2.0826\n",
            "[Step 35590] Loss: 2.5958\n",
            "[Step 35600] Loss: 2.5128\n",
            "[Step 35610] Loss: 2.5023\n",
            "[Step 35620] Loss: 2.5403\n",
            "[Step 35630] Loss: 2.1915\n",
            "[Step 35640] Loss: 2.4060\n",
            "[Step 35650] Loss: 2.4079\n",
            "[Step 35660] Loss: 2.5610\n",
            "[Step 35670] Loss: 2.2195\n",
            "[Step 35680] Loss: 2.3503\n",
            "[Step 35690] Loss: 2.1379\n",
            "[Step 35700] Loss: 2.4472\n",
            "[Step 35710] Loss: 2.5605\n",
            "[Step 35720] Loss: 2.0257\n",
            "[Step 35730] Loss: 2.2178\n",
            "[Step 35740] Loss: 2.5052\n",
            "[Step 35750] Loss: 2.4729\n",
            "[Step 35760] Loss: 2.3017\n",
            "[Step 35770] Loss: 2.2911\n",
            "[Step 35780] Loss: 1.9229\n",
            "[Step 35790] Loss: 2.4243\n",
            "[Step 35800] Loss: 2.2705\n",
            "[Step 35810] Loss: 2.3486\n",
            "[Step 35820] Loss: 2.5706\n",
            "[Step 35830] Loss: 2.6372\n",
            "[Step 35840] Loss: 2.6127\n",
            "[Step 35850] Loss: 2.6768\n",
            "[Step 35860] Loss: 2.4799\n",
            "[Step 35870] Loss: 2.3207\n",
            "[Step 35880] Loss: 2.3479\n",
            "[Step 35890] Loss: 2.3841\n",
            "[Step 35900] Loss: 2.1597\n",
            "[Step 35910] Loss: 1.9624\n",
            "[Step 35920] Loss: 2.2080\n",
            "[Step 35930] Loss: 2.5479\n",
            "[Step 35940] Loss: 1.8105\n",
            "[Step 35950] Loss: 2.2727\n",
            "[Step 35960] Loss: 1.7585\n",
            "[Step 35970] Loss: 2.6329\n",
            "[Step 35980] Loss: 1.9401\n",
            "ðŸ“˜ Epoch 84 - Avg Training Loss: 2.3889\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5113 | Accuracy: 0.3708 | Precision: 0.3603\n",
            "[Step 35990] Loss: 2.1125\n",
            "[Step 36000] Loss: 2.3101\n",
            "[Step 36010] Loss: 2.2933\n",
            "[Step 36020] Loss: 2.6185\n",
            "[Step 36030] Loss: 2.1826\n",
            "[Step 36040] Loss: 2.2140\n",
            "[Step 36050] Loss: 2.5458\n",
            "[Step 36060] Loss: 2.5256\n",
            "[Step 36070] Loss: 2.3011\n",
            "[Step 36080] Loss: 2.6531\n",
            "[Step 36090] Loss: 2.4625\n",
            "[Step 36100] Loss: 2.3211\n",
            "[Step 36110] Loss: 2.5065\n",
            "[Step 36120] Loss: 2.3574\n",
            "[Step 36130] Loss: 1.9331\n",
            "[Step 36140] Loss: 2.7015\n",
            "[Step 36150] Loss: 2.3327\n",
            "[Step 36160] Loss: 1.8860\n",
            "[Step 36170] Loss: 2.5857\n",
            "[Step 36180] Loss: 2.2956\n",
            "[Step 36190] Loss: 2.3551\n",
            "[Step 36200] Loss: 2.8027\n",
            "[Step 36210] Loss: 2.4510\n",
            "[Step 36220] Loss: 2.4484\n",
            "[Step 36230] Loss: 2.4412\n",
            "[Step 36240] Loss: 2.1561\n",
            "[Step 36250] Loss: 1.8895\n",
            "[Step 36260] Loss: 1.9641\n",
            "[Step 36270] Loss: 2.2827\n",
            "[Step 36280] Loss: 2.1226\n",
            "[Step 36290] Loss: 2.2256\n",
            "[Step 36300] Loss: 2.3240\n",
            "[Step 36310] Loss: 2.9711\n",
            "[Step 36320] Loss: 1.8647\n",
            "[Step 36330] Loss: 2.8373\n",
            "[Step 36340] Loss: 2.3741\n",
            "[Step 36350] Loss: 2.3698\n",
            "[Step 36360] Loss: 2.3211\n",
            "[Step 36370] Loss: 2.5763\n",
            "[Step 36380] Loss: 2.2686\n",
            "[Step 36390] Loss: 2.6627\n",
            "[Step 36400] Loss: 2.3329\n",
            "[Step 36410] Loss: 2.4250\n",
            "[Step 36420] Loss: 2.4009\n",
            "[Step 36430] Loss: 2.4728\n",
            "[Step 36440] Loss: 2.7428\n",
            "[Step 36450] Loss: 2.2946\n",
            "[Step 36460] Loss: 2.4158\n",
            "[Step 36470] Loss: 1.8369\n",
            "[Step 36480] Loss: 2.4269\n",
            "[Step 36490] Loss: 2.6036\n",
            "[Step 36500] Loss: 2.6469\n",
            "[Step 36510] Loss: 1.8721\n",
            "[Step 36520] Loss: 2.4361\n",
            "[Step 36530] Loss: 2.8410\n",
            "[Step 36540] Loss: 2.2804\n",
            "[Step 36550] Loss: 2.7347\n",
            "[Step 36560] Loss: 2.0327\n",
            "[Step 36570] Loss: 2.1797\n",
            "[Step 36580] Loss: 1.8874\n",
            "[Step 36590] Loss: 2.5751\n",
            "[Step 36600] Loss: 2.5957\n",
            "[Step 36610] Loss: 2.1685\n",
            "[Step 36620] Loss: 2.7631\n",
            "[Step 36630] Loss: 2.8339\n",
            "[Step 36640] Loss: 2.1534\n",
            "[Step 36650] Loss: 2.4867\n",
            "[Step 36660] Loss: 2.7372\n",
            "[Step 36670] Loss: 2.4912\n",
            "[Step 36680] Loss: 2.5327\n",
            "[Step 36690] Loss: 2.8178\n",
            "[Step 36700] Loss: 2.7799\n",
            "[Step 36710] Loss: 2.2988\n",
            "[Step 36720] Loss: 2.3300\n",
            "[Step 36730] Loss: 2.5632\n",
            "[Step 36740] Loss: 2.8071\n",
            "[Step 36750] Loss: 2.4079\n",
            "[Step 36760] Loss: 2.6352\n",
            "[Step 36770] Loss: 2.6689\n",
            "[Step 36780] Loss: 2.5491\n",
            "[Step 36790] Loss: 2.3395\n",
            "[Step 36800] Loss: 2.6500\n",
            "[Step 36810] Loss: 2.3516\n",
            "[Step 36820] Loss: 2.7155\n",
            "[Step 36830] Loss: 2.4546\n",
            "[Step 36840] Loss: 2.4667\n",
            "[Step 36850] Loss: 2.8890\n",
            "[Step 36860] Loss: 2.5745\n",
            "[Step 36870] Loss: 2.7918\n",
            "[Step 36880] Loss: 3.1103\n",
            "[Step 36890] Loss: 2.4275\n",
            "[Step 36900] Loss: 2.1515\n",
            "[Step 36910] Loss: 2.1422\n",
            "[Step 36920] Loss: 2.5282\n",
            "[Step 36930] Loss: 2.3631\n",
            "[Step 36940] Loss: 2.4144\n",
            "[Step 36950] Loss: 2.2068\n",
            "[Step 36960] Loss: 2.6740\n",
            "[Step 36970] Loss: 2.1898\n",
            "[Step 36980] Loss: 2.3429\n",
            "[Step 36990] Loss: 2.2340\n",
            "[Step 37000] Loss: 2.1717\n",
            "[Step 37010] Loss: 1.9916\n",
            "[Step 37020] Loss: 3.0193\n",
            "[Step 37030] Loss: 2.7391\n",
            "[Step 37040] Loss: 2.5256\n",
            "[Step 37050] Loss: 2.2383\n",
            "[Step 37060] Loss: 2.2454\n",
            "[Step 37070] Loss: 2.6647\n",
            "[Step 37080] Loss: 2.4298\n",
            "[Step 37090] Loss: 2.4096\n",
            "[Step 37100] Loss: 2.6997\n",
            "[Step 37110] Loss: 2.6351\n",
            "[Step 37120] Loss: 2.0039\n",
            "[Step 37130] Loss: 2.3300\n",
            "[Step 37140] Loss: 2.1254\n",
            "[Step 37150] Loss: 2.7168\n",
            "[Step 37160] Loss: 2.4907\n",
            "[Step 37170] Loss: 2.2413\n",
            "[Step 37180] Loss: 2.4377\n",
            "[Step 37190] Loss: 2.3539\n",
            "[Step 37200] Loss: 2.5105\n",
            "[Step 37210] Loss: 2.3286\n",
            "[Step 37220] Loss: 2.3327\n",
            "[Step 37230] Loss: 2.6768\n",
            "[Step 37240] Loss: 2.5086\n",
            "[Step 37250] Loss: 2.7215\n",
            "[Step 37260] Loss: 2.4880\n",
            "[Step 37270] Loss: 2.1750\n",
            "[Step 37280] Loss: 2.3774\n",
            "[Step 37290] Loss: 2.4685\n",
            "[Step 37300] Loss: 2.7262\n",
            "[Step 37310] Loss: 2.5101\n",
            "[Step 37320] Loss: 2.7187\n",
            "[Step 37330] Loss: 2.3336\n",
            "[Step 37340] Loss: 2.4627\n",
            "[Step 37350] Loss: 2.3224\n",
            "[Step 37360] Loss: 2.8012\n",
            "[Step 37370] Loss: 2.0258\n",
            "[Step 37380] Loss: 2.4846\n",
            "[Step 37390] Loss: 2.6100\n",
            "[Step 37400] Loss: 2.5052\n",
            "[Step 37410] Loss: 2.4079\n",
            "[Step 37420] Loss: 1.8863\n",
            "[Step 37430] Loss: 1.8443\n",
            "[Step 37440] Loss: 2.1440\n",
            "[Step 37450] Loss: 2.8417\n",
            "[Step 37460] Loss: 2.4101\n",
            "[Step 37470] Loss: 2.3406\n",
            "[Step 37480] Loss: 2.5167\n",
            "[Step 37490] Loss: 2.2366\n",
            "[Step 37500] Loss: 2.7612\n",
            "[Step 37510] Loss: 2.6399\n",
            "[Step 37520] Loss: 2.0159\n",
            "[Step 37530] Loss: 2.7022\n",
            "[Step 37540] Loss: 2.8037\n",
            "[Step 37550] Loss: 2.2790\n",
            "[Step 37560] Loss: 2.4299\n",
            "[Step 37570] Loss: 2.7142\n",
            "[Step 37580] Loss: 2.6207\n",
            "[Step 37590] Loss: 2.0886\n",
            "[Step 37600] Loss: 1.9459\n",
            "[Step 37610] Loss: 2.5572\n",
            "[Step 37620] Loss: 2.2311\n",
            "[Step 37630] Loss: 2.4457\n",
            "[Step 37640] Loss: 2.3681\n",
            "[Step 37650] Loss: 2.2417\n",
            "[Step 37660] Loss: 2.4877\n",
            "[Step 37670] Loss: 2.0115\n",
            "[Step 37680] Loss: 2.3943\n",
            "[Step 37690] Loss: 2.4761\n",
            "[Step 37700] Loss: 2.7218\n",
            "[Step 37710] Loss: 2.3921\n",
            "[Step 37720] Loss: 2.3501\n",
            "[Step 37730] Loss: 2.1821\n",
            "[Step 37740] Loss: 2.6561\n",
            "[Step 37750] Loss: 2.2790\n",
            "[Step 37760] Loss: 2.8660\n",
            "[Step 37770] Loss: 2.3787\n",
            "[Step 37780] Loss: 2.4303\n",
            "[Step 37790] Loss: 2.5063\n",
            "[Step 37800] Loss: 1.7812\n",
            "[Step 37810] Loss: 2.1980\n",
            "[Step 37820] Loss: 2.8381\n",
            "[Step 37830] Loss: 2.5720\n",
            "[Step 37840] Loss: 2.0853\n",
            "[Step 37850] Loss: 3.0458\n",
            "[Step 37860] Loss: 2.4529\n",
            "[Step 37870] Loss: 2.5377\n",
            "[Step 37880] Loss: 2.1678\n",
            "ðŸ“˜ Epoch 85 - Avg Training Loss: 2.3898\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4980 | Accuracy: 0.3770 | Precision: 0.3627\n",
            "[Step 37890] Loss: 2.3434\n",
            "[Step 37900] Loss: 2.6832\n",
            "[Step 37910] Loss: 3.0540\n",
            "[Step 37920] Loss: 2.3017\n",
            "[Step 37930] Loss: 2.2691\n",
            "[Step 37940] Loss: 1.9010\n",
            "[Step 37950] Loss: 2.8949\n",
            "[Step 37960] Loss: 2.1527\n",
            "[Step 37970] Loss: 1.6511\n",
            "[Step 37980] Loss: 2.4729\n",
            "[Step 37990] Loss: 2.4795\n",
            "[Step 38000] Loss: 2.4810\n",
            "[Step 38010] Loss: 2.2747\n",
            "[Step 38020] Loss: 2.2736\n",
            "[Step 38030] Loss: 2.7462\n",
            "[Step 38040] Loss: 2.3119\n",
            "[Step 38050] Loss: 1.9945\n",
            "[Step 38060] Loss: 2.4076\n",
            "[Step 38070] Loss: 2.6822\n",
            "[Step 38080] Loss: 1.9607\n",
            "[Step 38090] Loss: 1.9512\n",
            "[Step 38100] Loss: 2.2450\n",
            "[Step 38110] Loss: 2.8797\n",
            "[Step 38120] Loss: 2.5936\n",
            "[Step 38130] Loss: 2.3489\n",
            "[Step 38140] Loss: 2.7361\n",
            "[Step 38150] Loss: 2.1369\n",
            "[Step 38160] Loss: 2.5774\n",
            "[Step 38170] Loss: 2.6718\n",
            "[Step 38180] Loss: 2.1858\n",
            "[Step 38190] Loss: 2.3251\n",
            "[Step 38200] Loss: 2.1851\n",
            "[Step 38210] Loss: 2.5287\n",
            "[Step 38220] Loss: 2.3128\n",
            "[Step 38230] Loss: 2.3775\n",
            "[Step 38240] Loss: 2.0892\n",
            "[Step 38250] Loss: 2.2574\n",
            "[Step 38260] Loss: 2.7199\n",
            "[Step 38270] Loss: 2.3413\n",
            "[Step 38280] Loss: 2.8418\n",
            "[Step 38290] Loss: 2.2842\n",
            "[Step 38300] Loss: 2.6848\n",
            "[Step 38310] Loss: 2.3428\n",
            "[Step 38320] Loss: 2.2816\n",
            "[Step 38330] Loss: 2.3765\n",
            "[Step 38340] Loss: 2.4587\n",
            "[Step 38350] Loss: 2.7806\n",
            "[Step 38360] Loss: 2.1901\n",
            "[Step 38370] Loss: 2.6823\n",
            "[Step 38380] Loss: 2.1460\n",
            "[Step 38390] Loss: 2.6165\n",
            "[Step 38400] Loss: 2.7725\n",
            "[Step 38410] Loss: 2.1677\n",
            "[Step 38420] Loss: 2.6660\n",
            "[Step 38430] Loss: 2.4979\n",
            "[Step 38440] Loss: 2.2894\n",
            "[Step 38450] Loss: 2.2476\n",
            "[Step 38460] Loss: 2.5780\n",
            "[Step 38470] Loss: 2.1980\n",
            "[Step 38480] Loss: 2.1470\n",
            "[Step 38490] Loss: 2.3319\n",
            "[Step 38500] Loss: 1.9373\n",
            "[Step 38510] Loss: 2.7018\n",
            "[Step 38520] Loss: 2.1884\n",
            "[Step 38530] Loss: 2.4100\n",
            "[Step 38540] Loss: 1.9795\n",
            "[Step 38550] Loss: 2.2262\n",
            "[Step 38560] Loss: 2.3849\n",
            "[Step 38570] Loss: 2.4276\n",
            "[Step 38580] Loss: 2.5879\n",
            "[Step 38590] Loss: 2.1147\n",
            "[Step 38600] Loss: 1.9089\n",
            "[Step 38610] Loss: 2.2497\n",
            "[Step 38620] Loss: 2.2883\n",
            "[Step 38630] Loss: 2.3638\n",
            "[Step 38640] Loss: 2.2439\n",
            "[Step 38650] Loss: 2.3972\n",
            "[Step 38660] Loss: 2.2074\n",
            "[Step 38670] Loss: 2.5983\n",
            "[Step 38680] Loss: 2.0985\n",
            "[Step 38690] Loss: 2.4233\n",
            "[Step 38700] Loss: 2.4552\n",
            "[Step 38710] Loss: 1.9385\n",
            "[Step 38720] Loss: 3.3627\n",
            "[Step 38730] Loss: 2.8279\n",
            "[Step 38740] Loss: 2.5673\n",
            "[Step 38750] Loss: 1.8388\n",
            "[Step 38760] Loss: 2.0560\n",
            "[Step 38770] Loss: 3.0529\n",
            "[Step 38780] Loss: 2.4995\n",
            "[Step 38790] Loss: 2.2656\n",
            "[Step 38800] Loss: 2.2461\n",
            "[Step 38810] Loss: 1.9190\n",
            "[Step 38820] Loss: 2.7609\n",
            "[Step 38830] Loss: 2.4251\n",
            "[Step 38840] Loss: 2.4569\n",
            "[Step 38850] Loss: 2.3351\n",
            "[Step 38860] Loss: 2.2919\n",
            "[Step 38870] Loss: 2.2889\n",
            "[Step 38880] Loss: 2.4343\n",
            "[Step 38890] Loss: 2.8650\n",
            "[Step 38900] Loss: 2.6361\n",
            "[Step 38910] Loss: 2.6454\n",
            "[Step 38920] Loss: 2.7484\n",
            "[Step 38930] Loss: 2.6828\n",
            "[Step 38940] Loss: 2.2048\n",
            "[Step 38950] Loss: 2.2054\n",
            "[Step 38960] Loss: 2.4197\n",
            "[Step 38970] Loss: 2.3736\n",
            "[Step 38980] Loss: 2.6757\n",
            "[Step 38990] Loss: 2.0893\n",
            "[Step 39000] Loss: 2.1009\n",
            "[Step 39010] Loss: 2.4989\n",
            "[Step 39020] Loss: 3.1783\n",
            "[Step 39030] Loss: 2.3844\n",
            "[Step 39040] Loss: 2.2819\n",
            "[Step 39050] Loss: 1.9873\n",
            "[Step 39060] Loss: 2.1131\n",
            "[Step 39070] Loss: 2.2580\n",
            "[Step 39080] Loss: 2.1309\n",
            "[Step 39090] Loss: 2.5650\n",
            "[Step 39100] Loss: 2.6688\n",
            "[Step 39110] Loss: 2.1865\n",
            "[Step 39120] Loss: 2.4772\n",
            "[Step 39130] Loss: 2.0867\n",
            "[Step 39140] Loss: 2.1596\n",
            "[Step 39150] Loss: 2.3692\n",
            "[Step 39160] Loss: 2.5552\n",
            "[Step 39170] Loss: 2.2083\n",
            "[Step 39180] Loss: 2.5226\n",
            "[Step 39190] Loss: 2.5898\n",
            "[Step 39200] Loss: 2.3358\n",
            "[Step 39210] Loss: 2.7518\n",
            "[Step 39220] Loss: 2.7824\n",
            "[Step 39230] Loss: 2.0854\n",
            "[Step 39240] Loss: 2.1142\n",
            "[Step 39250] Loss: 2.6188\n",
            "[Step 39260] Loss: 2.7758\n",
            "[Step 39270] Loss: 2.6940\n",
            "[Step 39280] Loss: 2.2149\n",
            "[Step 39290] Loss: 2.4760\n",
            "[Step 39300] Loss: 2.7797\n",
            "[Step 39310] Loss: 2.3477\n",
            "[Step 39320] Loss: 2.3100\n",
            "[Step 39330] Loss: 2.1547\n",
            "[Step 39340] Loss: 2.0835\n",
            "[Step 39350] Loss: 1.9121\n",
            "[Step 39360] Loss: 2.6058\n",
            "[Step 39370] Loss: 2.1243\n",
            "[Step 39380] Loss: 2.4547\n",
            "[Step 39390] Loss: 2.3943\n",
            "[Step 39400] Loss: 2.7802\n",
            "[Step 39410] Loss: 2.4024\n",
            "[Step 39420] Loss: 1.7696\n",
            "[Step 39430] Loss: 2.4435\n",
            "[Step 39440] Loss: 2.3219\n",
            "[Step 39450] Loss: 2.0849\n",
            "[Step 39460] Loss: 2.6782\n",
            "[Step 39470] Loss: 2.5206\n",
            "[Step 39480] Loss: 2.4719\n",
            "[Step 39490] Loss: 2.9690\n",
            "[Step 39500] Loss: 3.2200\n",
            "[Step 39510] Loss: 2.3541\n",
            "[Step 39520] Loss: 2.1592\n",
            "[Step 39530] Loss: 2.6255\n",
            "[Step 39540] Loss: 1.9867\n",
            "[Step 39550] Loss: 2.2499\n",
            "[Step 39560] Loss: 2.9474\n",
            "[Step 39570] Loss: 2.1654\n",
            "[Step 39580] Loss: 2.4772\n",
            "[Step 39590] Loss: 2.1891\n",
            "[Step 39600] Loss: 2.2699\n",
            "[Step 39610] Loss: 2.4336\n",
            "[Step 39620] Loss: 2.1490\n",
            "[Step 39630] Loss: 2.4430\n",
            "[Step 39640] Loss: 2.5524\n",
            "[Step 39650] Loss: 2.2101\n",
            "[Step 39660] Loss: 2.5197\n",
            "[Step 39670] Loss: 2.2912\n",
            "[Step 39680] Loss: 2.3142\n",
            "[Step 39690] Loss: 2.3296\n",
            "[Step 39700] Loss: 2.5013\n",
            "[Step 39710] Loss: 2.3317\n",
            "[Step 39720] Loss: 2.4246\n",
            "[Step 39730] Loss: 2.4943\n",
            "[Step 39740] Loss: 1.9674\n",
            "[Step 39750] Loss: 2.3648\n",
            "[Step 39760] Loss: 2.2359\n",
            "[Step 39770] Loss: 1.8153\n",
            "ðŸ“˜ Epoch 86 - Avg Training Loss: 2.3860\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5329 | Accuracy: 0.3653 | Precision: 0.3578\n",
            "[Step 39780] Loss: 2.3059\n",
            "[Step 39790] Loss: 2.3918\n",
            "[Step 39800] Loss: 2.0574\n",
            "[Step 39810] Loss: 2.6204\n",
            "[Step 39820] Loss: 2.5023\n",
            "[Step 39830] Loss: 2.5626\n",
            "[Step 39840] Loss: 1.8307\n",
            "[Step 39850] Loss: 2.5372\n",
            "[Step 39860] Loss: 2.0898\n",
            "[Step 39870] Loss: 2.3729\n",
            "[Step 39880] Loss: 2.4246\n",
            "[Step 39890] Loss: 2.2399\n",
            "[Step 39900] Loss: 2.3531\n",
            "[Step 39910] Loss: 2.0639\n",
            "[Step 39920] Loss: 2.5453\n",
            "[Step 39930] Loss: 2.3168\n",
            "[Step 39940] Loss: 2.6209\n",
            "[Step 39950] Loss: 2.0683\n",
            "[Step 39960] Loss: 2.1430\n",
            "[Step 39970] Loss: 2.2226\n",
            "[Step 39980] Loss: 2.4519\n",
            "[Step 39990] Loss: 2.4355\n",
            "[Step 40000] Loss: 2.8556\n",
            "[Step 40010] Loss: 2.2576\n",
            "[Step 40020] Loss: 2.5250\n",
            "[Step 40030] Loss: 2.7226\n",
            "[Step 40040] Loss: 1.9351\n",
            "[Step 40050] Loss: 2.7307\n",
            "[Step 40060] Loss: 2.6571\n",
            "[Step 40070] Loss: 1.5402\n",
            "[Step 40080] Loss: 2.7967\n",
            "[Step 40090] Loss: 2.1883\n",
            "[Step 40100] Loss: 2.4942\n",
            "[Step 40110] Loss: 2.2231\n",
            "[Step 40120] Loss: 2.0815\n",
            "[Step 40130] Loss: 2.3993\n",
            "[Step 40140] Loss: 2.2127\n",
            "[Step 40150] Loss: 2.2862\n",
            "[Step 40160] Loss: 2.2149\n",
            "[Step 40170] Loss: 2.4334\n",
            "[Step 40180] Loss: 2.4508\n",
            "[Step 40190] Loss: 2.4124\n",
            "[Step 40200] Loss: 2.6737\n",
            "[Step 40210] Loss: 2.2303\n",
            "[Step 40220] Loss: 2.2050\n",
            "[Step 40230] Loss: 2.3445\n",
            "[Step 40240] Loss: 2.2929\n",
            "[Step 40250] Loss: 2.6828\n",
            "[Step 40260] Loss: 2.4062\n",
            "[Step 40270] Loss: 2.5740\n",
            "[Step 40280] Loss: 2.2983\n",
            "[Step 40290] Loss: 2.3713\n",
            "[Step 40300] Loss: 2.0852\n",
            "[Step 40310] Loss: 2.1421\n",
            "[Step 40320] Loss: 2.6826\n",
            "[Step 40330] Loss: 2.4876\n",
            "[Step 40340] Loss: 2.3353\n",
            "[Step 40350] Loss: 2.4248\n",
            "[Step 40360] Loss: 2.1827\n",
            "[Step 40370] Loss: 2.7182\n",
            "[Step 40380] Loss: 2.2156\n",
            "[Step 40390] Loss: 2.2733\n",
            "[Step 40400] Loss: 2.4265\n",
            "[Step 40410] Loss: 2.1545\n",
            "[Step 40420] Loss: 2.3474\n",
            "[Step 40430] Loss: 2.1089\n",
            "[Step 40440] Loss: 2.5839\n",
            "[Step 40450] Loss: 2.2380\n",
            "[Step 40460] Loss: 2.3885\n",
            "[Step 40470] Loss: 2.4722\n",
            "[Step 40480] Loss: 2.4100\n",
            "[Step 40490] Loss: 2.6421\n",
            "[Step 40500] Loss: 2.1253\n",
            "[Step 40510] Loss: 2.2992\n",
            "[Step 40520] Loss: 2.0481\n",
            "[Step 40530] Loss: 2.6555\n",
            "[Step 40540] Loss: 2.2939\n",
            "[Step 40550] Loss: 1.9096\n",
            "[Step 40560] Loss: 2.5479\n",
            "[Step 40570] Loss: 2.3921\n",
            "[Step 40580] Loss: 1.8517\n",
            "[Step 40590] Loss: 2.1042\n",
            "[Step 40600] Loss: 2.5074\n",
            "[Step 40610] Loss: 2.5894\n",
            "[Step 40620] Loss: 3.1039\n",
            "[Step 40630] Loss: 2.3116\n",
            "[Step 40640] Loss: 2.3601\n",
            "[Step 40650] Loss: 2.6944\n",
            "[Step 40660] Loss: 1.7372\n",
            "[Step 40670] Loss: 2.0669\n",
            "[Step 40680] Loss: 2.5955\n",
            "[Step 40690] Loss: 2.5801\n",
            "[Step 40700] Loss: 2.1117\n",
            "[Step 40710] Loss: 2.3764\n",
            "[Step 40720] Loss: 2.7938\n",
            "[Step 40730] Loss: 2.6152\n",
            "[Step 40740] Loss: 2.1865\n",
            "[Step 40750] Loss: 2.3596\n",
            "[Step 40760] Loss: 2.5634\n",
            "[Step 40770] Loss: 1.6674\n",
            "[Step 40780] Loss: 2.2816\n",
            "[Step 40790] Loss: 2.9416\n",
            "[Step 40800] Loss: 2.2479\n",
            "[Step 40810] Loss: 2.1589\n",
            "[Step 40820] Loss: 2.2022\n",
            "[Step 40830] Loss: 2.4439\n",
            "[Step 40840] Loss: 2.6806\n",
            "[Step 40850] Loss: 2.6884\n",
            "[Step 40860] Loss: 2.3723\n",
            "[Step 40870] Loss: 2.6392\n",
            "[Step 40880] Loss: 2.4731\n",
            "[Step 40890] Loss: 2.2557\n",
            "[Step 40900] Loss: 2.4821\n",
            "[Step 40910] Loss: 2.2498\n",
            "[Step 40920] Loss: 2.2547\n",
            "[Step 40930] Loss: 2.0020\n",
            "[Step 40940] Loss: 1.9729\n",
            "[Step 40950] Loss: 2.0055\n",
            "[Step 40960] Loss: 2.6305\n",
            "[Step 40970] Loss: 2.0747\n",
            "[Step 40980] Loss: 2.1819\n",
            "[Step 40990] Loss: 2.7470\n",
            "[Step 41000] Loss: 2.4705\n",
            "[Step 41010] Loss: 2.1995\n",
            "[Step 41020] Loss: 2.3557\n",
            "[Step 41030] Loss: 2.4049\n",
            "[Step 41040] Loss: 2.3866\n",
            "[Step 41050] Loss: 2.3772\n",
            "[Step 41060] Loss: 2.0365\n",
            "[Step 41070] Loss: 2.8163\n",
            "[Step 41080] Loss: 2.3918\n",
            "[Step 41090] Loss: 2.6575\n",
            "[Step 41100] Loss: 2.3091\n",
            "[Step 41110] Loss: 2.4711\n",
            "[Step 41120] Loss: 2.2278\n",
            "[Step 41130] Loss: 2.7131\n",
            "[Step 41140] Loss: 2.3313\n",
            "[Step 41150] Loss: 3.0613\n",
            "[Step 41160] Loss: 2.6995\n",
            "[Step 41170] Loss: 2.6390\n",
            "[Step 41180] Loss: 2.0992\n",
            "[Step 41190] Loss: 2.5540\n",
            "[Step 41200] Loss: 2.6405\n",
            "[Step 41210] Loss: 2.1576\n",
            "[Step 41220] Loss: 2.6274\n",
            "[Step 41230] Loss: 2.7690\n",
            "[Step 41240] Loss: 2.2362\n",
            "[Step 41250] Loss: 2.4117\n",
            "[Step 41260] Loss: 2.0981\n",
            "[Step 41270] Loss: 2.6303\n",
            "[Step 41280] Loss: 2.3377\n",
            "[Step 41290] Loss: 2.4797\n",
            "[Step 41300] Loss: 2.8632\n",
            "[Step 41310] Loss: 2.5273\n",
            "[Step 41320] Loss: 2.6760\n",
            "[Step 41330] Loss: 3.1965\n",
            "[Step 41340] Loss: 2.1320\n",
            "[Step 41350] Loss: 2.1505\n",
            "[Step 41360] Loss: 2.6894\n",
            "[Step 41370] Loss: 2.4294\n",
            "[Step 41380] Loss: 2.1411\n",
            "[Step 41390] Loss: 2.4581\n",
            "[Step 41400] Loss: 2.2849\n",
            "[Step 41410] Loss: 2.1632\n",
            "[Step 41420] Loss: 2.8524\n",
            "[Step 41430] Loss: 2.3167\n",
            "[Step 41440] Loss: 2.1056\n",
            "[Step 41450] Loss: 2.8317\n",
            "[Step 41460] Loss: 2.7270\n",
            "[Step 41470] Loss: 2.4288\n",
            "[Step 41480] Loss: 2.1970\n",
            "[Step 41490] Loss: 2.8833\n",
            "[Step 41500] Loss: 2.5354\n",
            "[Step 41510] Loss: 2.5166\n",
            "[Step 41520] Loss: 2.6362\n",
            "[Step 41530] Loss: 2.4026\n",
            "[Step 41540] Loss: 2.2379\n",
            "[Step 41550] Loss: 2.2383\n",
            "[Step 41560] Loss: 2.4691\n",
            "[Step 41570] Loss: 2.4896\n",
            "[Step 41580] Loss: 2.7571\n",
            "[Step 41590] Loss: 2.1660\n",
            "[Step 41600] Loss: 2.0577\n",
            "[Step 41610] Loss: 2.2086\n",
            "[Step 41620] Loss: 2.2885\n",
            "[Step 41630] Loss: 2.1742\n",
            "[Step 41640] Loss: 1.9544\n",
            "[Step 41650] Loss: 2.0601\n",
            "[Step 41660] Loss: 2.2781\n",
            "ðŸ“˜ Epoch 87 - Avg Training Loss: 2.3787\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5095 | Accuracy: 0.3707 | Precision: 0.3524\n",
            "[Step 41670] Loss: 2.2548\n",
            "[Step 41680] Loss: 2.5541\n",
            "[Step 41690] Loss: 2.2966\n",
            "[Step 41700] Loss: 2.4297\n",
            "[Step 41710] Loss: 2.3818\n",
            "[Step 41720] Loss: 2.2212\n",
            "[Step 41730] Loss: 2.6690\n",
            "[Step 41740] Loss: 2.0484\n",
            "[Step 41750] Loss: 2.3417\n",
            "[Step 41760] Loss: 2.0194\n",
            "[Step 41770] Loss: 2.3768\n",
            "[Step 41780] Loss: 2.0837\n",
            "[Step 41790] Loss: 2.1702\n",
            "[Step 41800] Loss: 2.4925\n",
            "[Step 41810] Loss: 2.3607\n",
            "[Step 41820] Loss: 2.2442\n",
            "[Step 41830] Loss: 2.2588\n",
            "[Step 41840] Loss: 2.2409\n",
            "[Step 41850] Loss: 2.5395\n",
            "[Step 41860] Loss: 2.6691\n",
            "[Step 41870] Loss: 2.9145\n",
            "[Step 41880] Loss: 2.2055\n",
            "[Step 41890] Loss: 2.7803\n",
            "[Step 41900] Loss: 2.1244\n",
            "[Step 41910] Loss: 2.0147\n",
            "[Step 41920] Loss: 1.9433\n",
            "[Step 41930] Loss: 2.1108\n",
            "[Step 41940] Loss: 2.2091\n",
            "[Step 41950] Loss: 2.4355\n",
            "[Step 41960] Loss: 2.1081\n",
            "[Step 41970] Loss: 2.7095\n",
            "[Step 41980] Loss: 2.7225\n",
            "[Step 41990] Loss: 2.3221\n",
            "[Step 42000] Loss: 2.0314\n",
            "[Step 42010] Loss: 2.2814\n",
            "[Step 42020] Loss: 2.7289\n",
            "[Step 42030] Loss: 2.4633\n",
            "[Step 42040] Loss: 2.5676\n",
            "[Step 42050] Loss: 2.7182\n",
            "[Step 42060] Loss: 2.2343\n",
            "[Step 42070] Loss: 2.5749\n",
            "[Step 42080] Loss: 2.1135\n",
            "[Step 42090] Loss: 2.2449\n",
            "[Step 42100] Loss: 2.2812\n",
            "[Step 42110] Loss: 2.4800\n",
            "[Step 42120] Loss: 2.0199\n",
            "[Step 42130] Loss: 1.9976\n",
            "[Step 42140] Loss: 2.3885\n",
            "[Step 42150] Loss: 2.9413\n",
            "[Step 42160] Loss: 2.5449\n",
            "[Step 42170] Loss: 2.3145\n",
            "[Step 42180] Loss: 2.3400\n",
            "[Step 42190] Loss: 1.9393\n",
            "[Step 42200] Loss: 2.3656\n",
            "[Step 42210] Loss: 2.3267\n",
            "[Step 42220] Loss: 2.9257\n",
            "[Step 42230] Loss: 2.6126\n",
            "[Step 42240] Loss: 3.3288\n",
            "[Step 42250] Loss: 2.2602\n",
            "[Step 42260] Loss: 2.4924\n",
            "[Step 42270] Loss: 2.3907\n",
            "[Step 42280] Loss: 2.2327\n",
            "[Step 42290] Loss: 2.1676\n",
            "[Step 42300] Loss: 2.0139\n",
            "[Step 42310] Loss: 2.5631\n",
            "[Step 42320] Loss: 2.7363\n",
            "[Step 42330] Loss: 2.4096\n",
            "[Step 42340] Loss: 3.0595\n",
            "[Step 42350] Loss: 2.2441\n",
            "[Step 42360] Loss: 2.2913\n",
            "[Step 42370] Loss: 2.1535\n",
            "[Step 42380] Loss: 2.6502\n",
            "[Step 42390] Loss: 1.9736\n",
            "[Step 42400] Loss: 2.0618\n",
            "[Step 42410] Loss: 2.1068\n",
            "[Step 42420] Loss: 2.6583\n",
            "[Step 42430] Loss: 1.9395\n",
            "[Step 42440] Loss: 2.1190\n",
            "[Step 42450] Loss: 2.2487\n",
            "[Step 42460] Loss: 2.6736\n",
            "[Step 42470] Loss: 2.0970\n",
            "[Step 42480] Loss: 2.4645\n",
            "[Step 42490] Loss: 2.4827\n",
            "[Step 42500] Loss: 2.5784\n",
            "[Step 42510] Loss: 2.6909\n",
            "[Step 42520] Loss: 2.5114\n",
            "[Step 42530] Loss: 2.3854\n",
            "[Step 42540] Loss: 1.9380\n",
            "[Step 42550] Loss: 2.7384\n",
            "[Step 42560] Loss: 2.0466\n",
            "[Step 42570] Loss: 2.2732\n",
            "[Step 42580] Loss: 2.1952\n",
            "[Step 42590] Loss: 2.4246\n",
            "[Step 42600] Loss: 2.2444\n",
            "[Step 42610] Loss: 2.9696\n",
            "[Step 42620] Loss: 2.2452\n",
            "[Step 42630] Loss: 2.8403\n",
            "[Step 42640] Loss: 1.8908\n",
            "[Step 42650] Loss: 2.4245\n",
            "[Step 42660] Loss: 2.5925\n",
            "[Step 42670] Loss: 2.3727\n",
            "[Step 42680] Loss: 2.4163\n",
            "[Step 42690] Loss: 2.6118\n",
            "[Step 42700] Loss: 2.3121\n",
            "[Step 42710] Loss: 2.2951\n",
            "[Step 42720] Loss: 1.5567\n",
            "[Step 42730] Loss: 2.3807\n",
            "[Step 42740] Loss: 2.5023\n",
            "[Step 42750] Loss: 2.3967\n",
            "[Step 42760] Loss: 2.3447\n",
            "[Step 42770] Loss: 2.8784\n",
            "[Step 42780] Loss: 2.3418\n",
            "[Step 42790] Loss: 2.2457\n",
            "[Step 42800] Loss: 1.7578\n",
            "[Step 42810] Loss: 2.3582\n",
            "[Step 42820] Loss: 2.1738\n",
            "[Step 42830] Loss: 2.1569\n",
            "[Step 42840] Loss: 2.9226\n",
            "[Step 42850] Loss: 1.9033\n",
            "[Step 42860] Loss: 2.3410\n",
            "[Step 42870] Loss: 2.4128\n",
            "[Step 42880] Loss: 2.3040\n",
            "[Step 42890] Loss: 2.8899\n",
            "[Step 42900] Loss: 2.3855\n",
            "[Step 42910] Loss: 2.6906\n",
            "[Step 42920] Loss: 2.0346\n",
            "[Step 42930] Loss: 1.9742\n",
            "[Step 42940] Loss: 2.3817\n",
            "[Step 42950] Loss: 2.0164\n",
            "[Step 42960] Loss: 2.3533\n",
            "[Step 42970] Loss: 2.3281\n",
            "[Step 42980] Loss: 2.2074\n",
            "[Step 42990] Loss: 2.2096\n",
            "[Step 43000] Loss: 2.7835\n",
            "[Step 43010] Loss: 2.5828\n",
            "[Step 43020] Loss: 2.3927\n",
            "[Step 43030] Loss: 2.5646\n",
            "[Step 43040] Loss: 2.4154\n",
            "[Step 43050] Loss: 2.4388\n",
            "[Step 43060] Loss: 2.5915\n",
            "[Step 43070] Loss: 2.2348\n",
            "[Step 43080] Loss: 2.3180\n",
            "[Step 43090] Loss: 2.4354\n",
            "[Step 43100] Loss: 2.2956\n",
            "[Step 43110] Loss: 2.4193\n",
            "[Step 43120] Loss: 2.0189\n",
            "[Step 43130] Loss: 1.9845\n",
            "[Step 43140] Loss: 1.9257\n",
            "[Step 43150] Loss: 2.5065\n",
            "[Step 43160] Loss: 2.1192\n",
            "[Step 43170] Loss: 2.1523\n",
            "[Step 43180] Loss: 2.2105\n",
            "[Step 43190] Loss: 2.6557\n",
            "[Step 43200] Loss: 2.7559\n",
            "[Step 43210] Loss: 2.2845\n",
            "[Step 43220] Loss: 2.1382\n",
            "[Step 43230] Loss: 2.3888\n",
            "[Step 43240] Loss: 2.5044\n",
            "[Step 43250] Loss: 2.3554\n",
            "[Step 43260] Loss: 2.3508\n",
            "[Step 43270] Loss: 2.4094\n",
            "[Step 43280] Loss: 2.3433\n",
            "[Step 43290] Loss: 2.5939\n",
            "[Step 43300] Loss: 2.3685\n",
            "[Step 43310] Loss: 2.0528\n",
            "[Step 43320] Loss: 2.4542\n",
            "[Step 43330] Loss: 2.0560\n",
            "[Step 43340] Loss: 2.5314\n",
            "[Step 43350] Loss: 1.8953\n",
            "[Step 43360] Loss: 2.4815\n",
            "[Step 43370] Loss: 2.2811\n",
            "[Step 43380] Loss: 2.4444\n",
            "[Step 43390] Loss: 2.6853\n",
            "[Step 43400] Loss: 2.6079\n",
            "[Step 43410] Loss: 2.2524\n",
            "[Step 43420] Loss: 2.1546\n",
            "[Step 43430] Loss: 2.5448\n",
            "[Step 43440] Loss: 2.4460\n",
            "[Step 43450] Loss: 2.8117\n",
            "[Step 43460] Loss: 2.1583\n",
            "[Step 43470] Loss: 2.1928\n",
            "[Step 43480] Loss: 2.4163\n",
            "[Step 43490] Loss: 2.4178\n",
            "[Step 43500] Loss: 2.0938\n",
            "[Step 43510] Loss: 2.0102\n",
            "[Step 43520] Loss: 2.4692\n",
            "[Step 43530] Loss: 2.1484\n",
            "[Step 43540] Loss: 2.6282\n",
            "[Step 43550] Loss: 2.9359\n",
            "[Step 43560] Loss: 2.0201\n",
            "ðŸ“˜ Epoch 88 - Avg Training Loss: 2.3673\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4849 | Accuracy: 0.3803 | Precision: 0.3687\n",
            "[Step 43570] Loss: 2.7571\n",
            "[Step 43580] Loss: 2.3282\n",
            "[Step 43590] Loss: 2.5263\n",
            "[Step 43600] Loss: 2.2706\n",
            "[Step 43610] Loss: 2.3684\n",
            "[Step 43620] Loss: 2.1667\n",
            "[Step 43630] Loss: 2.3135\n",
            "[Step 43640] Loss: 2.4216\n",
            "[Step 43650] Loss: 2.3920\n",
            "[Step 43660] Loss: 2.1937\n",
            "[Step 43670] Loss: 2.5030\n",
            "[Step 43680] Loss: 2.7412\n",
            "[Step 43690] Loss: 2.4579\n",
            "[Step 43700] Loss: 2.5717\n",
            "[Step 43710] Loss: 2.2737\n",
            "[Step 43720] Loss: 1.6151\n",
            "[Step 43730] Loss: 2.1806\n",
            "[Step 43740] Loss: 1.4959\n",
            "[Step 43750] Loss: 3.1212\n",
            "[Step 43760] Loss: 2.0372\n",
            "[Step 43770] Loss: 2.1423\n",
            "[Step 43780] Loss: 2.5857\n",
            "[Step 43790] Loss: 2.8145\n",
            "[Step 43800] Loss: 2.7010\n",
            "[Step 43810] Loss: 2.6051\n",
            "[Step 43820] Loss: 2.1456\n",
            "[Step 43830] Loss: 2.3900\n",
            "[Step 43840] Loss: 2.6525\n",
            "[Step 43850] Loss: 2.3080\n",
            "[Step 43860] Loss: 2.5113\n",
            "[Step 43870] Loss: 2.3495\n",
            "[Step 43880] Loss: 2.2829\n",
            "[Step 43890] Loss: 2.4246\n",
            "[Step 43900] Loss: 2.0347\n",
            "[Step 43910] Loss: 2.6382\n",
            "[Step 43920] Loss: 1.6923\n",
            "[Step 43930] Loss: 2.1879\n",
            "[Step 43940] Loss: 2.3720\n",
            "[Step 43950] Loss: 2.5234\n",
            "[Step 43960] Loss: 2.1454\n",
            "[Step 43970] Loss: 2.9344\n",
            "[Step 43980] Loss: 2.7406\n",
            "[Step 43990] Loss: 2.1076\n",
            "[Step 44000] Loss: 2.5266\n",
            "[Step 44010] Loss: 2.7091\n",
            "[Step 44020] Loss: 2.1949\n",
            "[Step 44030] Loss: 2.2618\n",
            "[Step 44040] Loss: 2.8012\n",
            "[Step 44050] Loss: 2.3179\n",
            "[Step 44060] Loss: 2.8062\n",
            "[Step 44070] Loss: 2.5408\n",
            "[Step 44080] Loss: 2.3410\n",
            "[Step 44090] Loss: 2.5347\n",
            "[Step 44100] Loss: 2.4064\n",
            "[Step 44110] Loss: 2.5429\n",
            "[Step 44120] Loss: 2.3359\n",
            "[Step 44130] Loss: 2.3703\n",
            "[Step 44140] Loss: 2.6068\n",
            "[Step 44150] Loss: 2.9165\n",
            "[Step 44160] Loss: 2.3268\n",
            "[Step 44170] Loss: 2.1737\n",
            "[Step 44180] Loss: 2.2129\n",
            "[Step 44190] Loss: 2.1536\n",
            "[Step 44200] Loss: 2.5594\n",
            "[Step 44210] Loss: 2.4076\n",
            "[Step 44220] Loss: 2.4113\n",
            "[Step 44230] Loss: 2.7369\n",
            "[Step 44240] Loss: 2.5390\n",
            "[Step 44250] Loss: 2.1176\n",
            "[Step 44260] Loss: 2.2303\n",
            "[Step 44270] Loss: 2.8522\n",
            "[Step 44280] Loss: 2.2019\n",
            "[Step 44290] Loss: 2.4573\n",
            "[Step 44300] Loss: 2.2363\n",
            "[Step 44310] Loss: 2.6126\n",
            "[Step 44320] Loss: 2.5562\n",
            "[Step 44330] Loss: 2.5274\n",
            "[Step 44340] Loss: 2.1634\n",
            "[Step 44350] Loss: 2.7739\n",
            "[Step 44360] Loss: 2.2734\n",
            "[Step 44370] Loss: 1.9529\n",
            "[Step 44380] Loss: 2.3962\n",
            "[Step 44390] Loss: 2.1493\n",
            "[Step 44400] Loss: 2.4383\n",
            "[Step 44410] Loss: 2.4316\n",
            "[Step 44420] Loss: 2.7758\n",
            "[Step 44430] Loss: 2.4755\n",
            "[Step 44440] Loss: 2.4949\n",
            "[Step 44450] Loss: 1.8599\n",
            "[Step 44460] Loss: 2.4527\n",
            "[Step 44470] Loss: 2.1332\n",
            "[Step 44480] Loss: 2.3685\n",
            "[Step 44490] Loss: 2.2070\n",
            "[Step 44500] Loss: 2.6307\n",
            "[Step 44510] Loss: 2.5453\n",
            "[Step 44520] Loss: 2.3805\n",
            "[Step 44530] Loss: 2.1892\n",
            "[Step 44540] Loss: 2.2558\n",
            "[Step 44550] Loss: 2.1792\n",
            "[Step 44560] Loss: 1.9627\n",
            "[Step 44570] Loss: 2.6446\n",
            "[Step 44580] Loss: 1.6279\n",
            "[Step 44590] Loss: 2.6130\n",
            "[Step 44600] Loss: 2.7206\n",
            "[Step 44610] Loss: 3.0057\n",
            "[Step 44620] Loss: 2.4301\n",
            "[Step 44630] Loss: 2.3437\n",
            "[Step 44640] Loss: 2.4752\n",
            "[Step 44650] Loss: 2.4090\n",
            "[Step 44660] Loss: 2.4424\n",
            "[Step 44670] Loss: 2.7111\n",
            "[Step 44680] Loss: 2.5155\n",
            "[Step 44690] Loss: 2.1323\n",
            "[Step 44700] Loss: 2.3389\n",
            "[Step 44710] Loss: 2.3284\n",
            "[Step 44720] Loss: 2.1078\n",
            "[Step 44730] Loss: 1.8386\n",
            "[Step 44740] Loss: 2.3825\n",
            "[Step 44750] Loss: 2.2689\n",
            "[Step 44760] Loss: 1.8727\n",
            "[Step 44770] Loss: 2.0529\n",
            "[Step 44780] Loss: 2.4012\n",
            "[Step 44790] Loss: 2.4989\n",
            "[Step 44800] Loss: 2.3251\n",
            "[Step 44810] Loss: 2.0649\n",
            "[Step 44820] Loss: 2.2993\n",
            "[Step 44830] Loss: 2.7051\n",
            "[Step 44840] Loss: 2.0322\n",
            "[Step 44850] Loss: 1.8174\n",
            "[Step 44860] Loss: 2.2734\n",
            "[Step 44870] Loss: 2.1402\n",
            "[Step 44880] Loss: 2.8065\n",
            "[Step 44890] Loss: 1.9558\n",
            "[Step 44900] Loss: 2.2817\n",
            "[Step 44910] Loss: 2.2881\n",
            "[Step 44920] Loss: 2.6533\n",
            "[Step 44930] Loss: 2.3539\n",
            "[Step 44940] Loss: 2.4968\n",
            "[Step 44950] Loss: 2.9807\n",
            "[Step 44960] Loss: 2.5528\n",
            "[Step 44970] Loss: 2.3398\n",
            "[Step 44980] Loss: 2.2545\n",
            "[Step 44990] Loss: 2.9671\n",
            "[Step 45000] Loss: 2.2867\n",
            "[Step 45010] Loss: 2.1040\n",
            "[Step 45020] Loss: 2.0297\n",
            "[Step 45030] Loss: 2.8124\n",
            "[Step 45040] Loss: 2.3424\n",
            "[Step 45050] Loss: 2.4682\n",
            "[Step 45060] Loss: 2.1730\n",
            "[Step 45070] Loss: 2.6145\n",
            "[Step 45080] Loss: 2.2151\n",
            "[Step 45090] Loss: 2.6919\n",
            "[Step 45100] Loss: 1.9761\n",
            "[Step 45110] Loss: 2.5586\n",
            "[Step 45120] Loss: 2.3912\n",
            "[Step 45130] Loss: 2.2723\n",
            "[Step 45140] Loss: 2.5441\n",
            "[Step 45150] Loss: 2.0102\n",
            "[Step 45160] Loss: 2.3258\n",
            "[Step 45170] Loss: 2.2638\n",
            "[Step 45180] Loss: 1.9373\n",
            "[Step 45190] Loss: 2.7055\n",
            "[Step 45200] Loss: 2.4485\n",
            "[Step 45210] Loss: 2.3359\n",
            "[Step 45220] Loss: 2.4970\n",
            "[Step 45230] Loss: 2.3619\n",
            "[Step 45240] Loss: 2.4992\n",
            "[Step 45250] Loss: 2.7138\n",
            "[Step 45260] Loss: 2.2963\n",
            "[Step 45270] Loss: 2.1754\n",
            "[Step 45280] Loss: 2.1549\n",
            "[Step 45290] Loss: 2.5658\n",
            "[Step 45300] Loss: 2.6911\n",
            "[Step 45310] Loss: 2.0415\n",
            "[Step 45320] Loss: 2.2879\n",
            "[Step 45330] Loss: 2.7316\n",
            "[Step 45340] Loss: 1.9774\n",
            "[Step 45350] Loss: 2.4478\n",
            "[Step 45360] Loss: 2.3120\n",
            "[Step 45370] Loss: 3.1067\n",
            "[Step 45380] Loss: 2.1270\n",
            "[Step 45390] Loss: 2.6100\n",
            "[Step 45400] Loss: 2.1355\n",
            "[Step 45410] Loss: 2.0333\n",
            "[Step 45420] Loss: 2.0107\n",
            "[Step 45430] Loss: 2.1439\n",
            "[Step 45440] Loss: 2.4726\n",
            "[Step 45450] Loss: 2.7637\n",
            "ðŸ“˜ Epoch 89 - Avg Training Loss: 2.3495\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4987 | Accuracy: 0.3732 | Precision: 0.3595\n",
            "[Step 45460] Loss: 2.2076\n",
            "[Step 45470] Loss: 2.7006\n",
            "[Step 45480] Loss: 2.1772\n",
            "[Step 45490] Loss: 2.4995\n",
            "[Step 45500] Loss: 2.4398\n",
            "[Step 45510] Loss: 2.3517\n",
            "[Step 45520] Loss: 2.3753\n",
            "[Step 45530] Loss: 2.6917\n",
            "[Step 45540] Loss: 2.1405\n",
            "[Step 45550] Loss: 2.8424\n",
            "[Step 45560] Loss: 2.7108\n",
            "[Step 45570] Loss: 1.9960\n",
            "[Step 45580] Loss: 2.1410\n",
            "[Step 45590] Loss: 2.1406\n",
            "[Step 45600] Loss: 2.5193\n",
            "[Step 45610] Loss: 2.6357\n",
            "[Step 45620] Loss: 2.6256\n",
            "[Step 45630] Loss: 2.5426\n",
            "[Step 45640] Loss: 2.5078\n",
            "[Step 45650] Loss: 2.0723\n",
            "[Step 45660] Loss: 2.9495\n",
            "[Step 45670] Loss: 1.9688\n",
            "[Step 45680] Loss: 2.2777\n",
            "[Step 45690] Loss: 2.8481\n",
            "[Step 45700] Loss: 2.2596\n",
            "[Step 45710] Loss: 2.1029\n",
            "[Step 45720] Loss: 2.4276\n",
            "[Step 45730] Loss: 2.4470\n",
            "[Step 45740] Loss: 2.2221\n",
            "[Step 45750] Loss: 2.9387\n",
            "[Step 45760] Loss: 2.4510\n",
            "[Step 45770] Loss: 1.8938\n",
            "[Step 45780] Loss: 2.2216\n",
            "[Step 45790] Loss: 2.6882\n",
            "[Step 45800] Loss: 2.4080\n",
            "[Step 45810] Loss: 2.1422\n",
            "[Step 45820] Loss: 2.0743\n",
            "[Step 45830] Loss: 2.1909\n",
            "[Step 45840] Loss: 2.1577\n",
            "[Step 45850] Loss: 2.2787\n",
            "[Step 45860] Loss: 2.5034\n",
            "[Step 45870] Loss: 2.7591\n",
            "[Step 45880] Loss: 1.8818\n",
            "[Step 45890] Loss: 2.3193\n",
            "[Step 45900] Loss: 2.4271\n",
            "[Step 45910] Loss: 2.3632\n",
            "[Step 45920] Loss: 1.9864\n",
            "[Step 45930] Loss: 1.9022\n",
            "[Step 45940] Loss: 2.1072\n",
            "[Step 45950] Loss: 2.3609\n",
            "[Step 45960] Loss: 2.1678\n",
            "[Step 45970] Loss: 1.9939\n",
            "[Step 45980] Loss: 2.0402\n",
            "[Step 45990] Loss: 2.3514\n",
            "[Step 46000] Loss: 2.6989\n",
            "[Step 46010] Loss: 2.0962\n",
            "[Step 46020] Loss: 2.3059\n",
            "[Step 46030] Loss: 2.1037\n",
            "[Step 46040] Loss: 2.3523\n",
            "[Step 46050] Loss: 2.6624\n",
            "[Step 46060] Loss: 2.0376\n",
            "[Step 46070] Loss: 2.3727\n",
            "[Step 46080] Loss: 2.8042\n",
            "[Step 46090] Loss: 2.3901\n",
            "[Step 46100] Loss: 2.4684\n",
            "[Step 46110] Loss: 2.4403\n",
            "[Step 46120] Loss: 1.9366\n",
            "[Step 46130] Loss: 2.1976\n",
            "[Step 46140] Loss: 2.4485\n",
            "[Step 46150] Loss: 2.3753\n",
            "[Step 46160] Loss: 2.0890\n",
            "[Step 46170] Loss: 2.2089\n",
            "[Step 46180] Loss: 2.2776\n",
            "[Step 46190] Loss: 1.8433\n",
            "[Step 46200] Loss: 2.4081\n",
            "[Step 46210] Loss: 2.4143\n",
            "[Step 46220] Loss: 1.9145\n",
            "[Step 46230] Loss: 2.8819\n",
            "[Step 46240] Loss: 2.0878\n",
            "[Step 46250] Loss: 2.4064\n",
            "[Step 46260] Loss: 2.2460\n",
            "[Step 46270] Loss: 2.4136\n",
            "[Step 46280] Loss: 2.4357\n",
            "[Step 46290] Loss: 2.6765\n",
            "[Step 46300] Loss: 2.6086\n",
            "[Step 46310] Loss: 2.4735\n",
            "[Step 46320] Loss: 2.5859\n",
            "[Step 46330] Loss: 2.2848\n",
            "[Step 46340] Loss: 2.1498\n",
            "[Step 46350] Loss: 2.3803\n",
            "[Step 46360] Loss: 1.8377\n",
            "[Step 46370] Loss: 2.5116\n",
            "[Step 46380] Loss: 2.3092\n",
            "[Step 46390] Loss: 2.3568\n",
            "[Step 46400] Loss: 2.3786\n",
            "[Step 46410] Loss: 2.3164\n",
            "[Step 46420] Loss: 2.0096\n",
            "[Step 46430] Loss: 2.2730\n",
            "[Step 46440] Loss: 2.3782\n",
            "[Step 46450] Loss: 2.0671\n",
            "[Step 46460] Loss: 2.2183\n",
            "[Step 46470] Loss: 2.7368\n",
            "[Step 46480] Loss: 2.3965\n",
            "[Step 46490] Loss: 2.0548\n",
            "[Step 46500] Loss: 2.9959\n",
            "[Step 46510] Loss: 2.8775\n",
            "[Step 46520] Loss: 2.4265\n",
            "[Step 46530] Loss: 2.2551\n",
            "[Step 46540] Loss: 2.5867\n",
            "[Step 46550] Loss: 2.2113\n",
            "[Step 46560] Loss: 2.0141\n",
            "[Step 46570] Loss: 1.9892\n",
            "[Step 46580] Loss: 2.8883\n",
            "[Step 46590] Loss: 2.2837\n",
            "[Step 46600] Loss: 2.0618\n",
            "[Step 46610] Loss: 1.8035\n",
            "[Step 46620] Loss: 2.3219\n",
            "[Step 46630] Loss: 2.5153\n",
            "[Step 46640] Loss: 1.8023\n",
            "[Step 46650] Loss: 2.7038\n",
            "[Step 46660] Loss: 2.4461\n",
            "[Step 46670] Loss: 2.6636\n",
            "[Step 46680] Loss: 2.4312\n",
            "[Step 46690] Loss: 2.5459\n",
            "[Step 46700] Loss: 2.1956\n",
            "[Step 46710] Loss: 2.8173\n",
            "[Step 46720] Loss: 1.9202\n",
            "[Step 46730] Loss: 2.3761\n",
            "[Step 46740] Loss: 2.6848\n",
            "[Step 46750] Loss: 2.2515\n",
            "[Step 46760] Loss: 2.3349\n",
            "[Step 46770] Loss: 2.3346\n",
            "[Step 46780] Loss: 2.2243\n",
            "[Step 46790] Loss: 2.3019\n",
            "[Step 46800] Loss: 2.3837\n",
            "[Step 46810] Loss: 2.1459\n",
            "[Step 46820] Loss: 2.0160\n",
            "[Step 46830] Loss: 2.1873\n",
            "[Step 46840] Loss: 2.6908\n",
            "[Step 46850] Loss: 2.1875\n",
            "[Step 46860] Loss: 2.4559\n",
            "[Step 46870] Loss: 1.9563\n",
            "[Step 46880] Loss: 2.2343\n",
            "[Step 46890] Loss: 2.5439\n",
            "[Step 46900] Loss: 2.3336\n",
            "[Step 46910] Loss: 2.1155\n",
            "[Step 46920] Loss: 2.1760\n",
            "[Step 46930] Loss: 2.3931\n",
            "[Step 46940] Loss: 2.2638\n",
            "[Step 46950] Loss: 2.0695\n",
            "[Step 46960] Loss: 2.3418\n",
            "[Step 46970] Loss: 2.2468\n",
            "[Step 46980] Loss: 2.1594\n",
            "[Step 46990] Loss: 2.1220\n",
            "[Step 47000] Loss: 1.9429\n",
            "[Step 47010] Loss: 2.5983\n",
            "[Step 47020] Loss: 2.7209\n",
            "[Step 47030] Loss: 2.0437\n",
            "[Step 47040] Loss: 2.1964\n",
            "[Step 47050] Loss: 2.4066\n",
            "[Step 47060] Loss: 2.2760\n",
            "[Step 47070] Loss: 1.9326\n",
            "[Step 47080] Loss: 2.4182\n",
            "[Step 47090] Loss: 2.3187\n",
            "[Step 47100] Loss: 2.4034\n",
            "[Step 47110] Loss: 2.3439\n",
            "[Step 47120] Loss: 2.4667\n",
            "[Step 47130] Loss: 2.1156\n",
            "[Step 47140] Loss: 2.0018\n",
            "[Step 47150] Loss: 2.4788\n",
            "[Step 47160] Loss: 2.4590\n",
            "[Step 47170] Loss: 2.2868\n",
            "[Step 47180] Loss: 2.1710\n",
            "[Step 47190] Loss: 3.0414\n",
            "[Step 47200] Loss: 2.2744\n",
            "[Step 47210] Loss: 2.2203\n",
            "[Step 47220] Loss: 2.4042\n",
            "[Step 47230] Loss: 1.9080\n",
            "[Step 47240] Loss: 2.7703\n",
            "[Step 47250] Loss: 2.1313\n",
            "[Step 47260] Loss: 2.2957\n",
            "[Step 47270] Loss: 2.5797\n",
            "[Step 47280] Loss: 2.1602\n",
            "[Step 47290] Loss: 2.7742\n",
            "[Step 47300] Loss: 2.0891\n",
            "[Step 47310] Loss: 2.6009\n",
            "[Step 47320] Loss: 2.2534\n",
            "[Step 47330] Loss: 1.9091\n",
            "[Step 47340] Loss: 1.9704\n",
            "[Step 47350] Loss: 2.5066\n",
            "ðŸ“˜ Epoch 90 - Avg Training Loss: 2.3384\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4621 | Accuracy: 0.3807 | Precision: 0.3618\n",
            "âœ… Continued training complete\n"
          ]
        }
      ],
      "source": [
        "new_training_args = {\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"num_additional_epochs\": 5,\n",
        "    \"logging_steps\": 10,\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=new_training_args[\"learning_rate\"],\n",
        "    weight_decay=new_training_args[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "starting_epoch = 65\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(starting_epoch, starting_epoch + new_training_args[\"num_additional_epochs\"]):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % new_training_args[\"logging_steps\"] == 0:\n",
        "            print(f\"[Step {global_step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"ðŸ“˜ Epoch {epoch+1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    evaluate_val(model, val_loader, criterion, device)\n",
        "\n",
        "print(\"âœ… Continued training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "collapsed": true,
        "id": "AF9MAIh1oF-c",
        "outputId": "cc90cf84-1b7b-43dd-c4d7-63174af0fe71"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2969228479>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Save weights only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_weights_4.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/NexHack/model_full_4.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    779\u001b[0m             )\n\u001b[1;32m    780\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_compute_crc32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "save_path = '/content/drive/My Drive/NexHack/model_weights_4.pth'\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=True)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model_weights_1 = MyResNet50()\n",
        "\n",
        "torch.save(model_weights_1.state_dict(), 'model_weights_4.pth')\n",
        "torch.save(model_weights_1.state_dict(), save_path)\n",
        "\n",
        "save_path = '/content/drive/My Drive/NexHack/model_full_4.pth'\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=True)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model_full_1 = MyResNet50()\n",
        "\n",
        "torch.save(model_full_1, 'model_full_4.pth')\n",
        "torch.save(model_full_1.state_dict(), save_path)\n",
        "\n",
        "save_path = '/content/drive/My Drive/NexHack/checkpoint_4.pth'\n",
        "\n",
        "checkpoint = {\n",
        "    'epoch': 40,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'training_args': training_args\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, save_path)\n",
        "\n",
        "print(\"âœ… Saved training checkpoint to Google Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI4nbjrhgoHC"
      },
      "source": [
        "### **New Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ewz8jBB2fSHS",
        "outputId": "45123158-4855-41fc-d954-bc030f1a22b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 10] Loss: 2.0447\n",
            "[Step 20] Loss: 2.4238\n",
            "[Step 30] Loss: 2.3454\n",
            "[Step 40] Loss: 2.5764\n",
            "[Step 50] Loss: 2.5672\n",
            "[Step 60] Loss: 2.6720\n",
            "[Step 70] Loss: 1.9865\n",
            "[Step 80] Loss: 2.2443\n",
            "[Step 90] Loss: 2.4006\n",
            "[Step 100] Loss: 2.1740\n",
            "[Step 110] Loss: 2.0742\n",
            "[Step 120] Loss: 2.4228\n",
            "[Step 130] Loss: 2.6764\n",
            "[Step 140] Loss: 2.8126\n",
            "[Step 150] Loss: 2.0586\n",
            "[Step 160] Loss: 2.1948\n",
            "[Step 170] Loss: 2.3934\n",
            "[Step 180] Loss: 2.4214\n",
            "[Step 190] Loss: 2.9980\n",
            "[Step 200] Loss: 2.1040\n",
            "[Step 210] Loss: 2.1892\n",
            "[Step 220] Loss: 2.6544\n",
            "[Step 230] Loss: 2.4610\n",
            "[Step 240] Loss: 2.6737\n",
            "[Step 250] Loss: 2.6814\n",
            "[Step 260] Loss: 2.4827\n",
            "[Step 270] Loss: 2.4284\n",
            "[Step 280] Loss: 2.5538\n",
            "[Step 290] Loss: 2.2473\n",
            "[Step 300] Loss: 2.8773\n",
            "[Step 310] Loss: 3.0561\n",
            "[Step 320] Loss: 2.3896\n",
            "[Step 330] Loss: 2.4687\n",
            "[Step 340] Loss: 2.4388\n",
            "[Step 350] Loss: 2.3006\n",
            "[Step 360] Loss: 2.4746\n",
            "[Step 370] Loss: 2.6394\n",
            "[Step 380] Loss: 2.3361\n",
            "[Step 390] Loss: 2.9191\n",
            "[Step 400] Loss: 2.2343\n",
            "[Step 410] Loss: 2.4828\n",
            "[Step 420] Loss: 2.5228\n",
            "[Step 430] Loss: 2.1587\n",
            "[Step 440] Loss: 2.2464\n",
            "[Step 450] Loss: 2.2946\n",
            "[Step 460] Loss: 2.5133\n",
            "[Step 470] Loss: 3.0191\n",
            "[Step 480] Loss: 2.5381\n",
            "[Step 490] Loss: 2.4039\n",
            "[Step 500] Loss: 2.2014\n",
            "[Step 510] Loss: 2.9587\n",
            "[Step 520] Loss: 2.7002\n",
            "[Step 530] Loss: 2.5744\n",
            "[Step 540] Loss: 2.3091\n",
            "[Step 550] Loss: 2.3892\n",
            "[Step 560] Loss: 2.6405\n",
            "[Step 570] Loss: 2.0624\n",
            "[Step 580] Loss: 2.5470\n",
            "[Step 590] Loss: 2.0941\n",
            "[Step 600] Loss: 2.3653\n",
            "[Step 610] Loss: 2.8015\n",
            "[Step 620] Loss: 2.0990\n",
            "[Step 630] Loss: 2.2210\n",
            "[Step 640] Loss: 2.6999\n",
            "[Step 650] Loss: 2.4877\n",
            "[Step 660] Loss: 2.5582\n",
            "[Step 670] Loss: 2.7461\n",
            "[Step 680] Loss: 2.2170\n",
            "[Step 690] Loss: 2.4892\n",
            "[Step 700] Loss: 2.3909\n",
            "[Step 710] Loss: 2.2636\n",
            "[Step 720] Loss: 2.5711\n",
            "[Step 730] Loss: 2.6476\n",
            "[Step 740] Loss: 3.4963\n",
            "[Step 750] Loss: 2.6427\n",
            "[Step 760] Loss: 2.2005\n",
            "[Step 770] Loss: 2.6043\n",
            "[Step 780] Loss: 2.7335\n",
            "[Step 790] Loss: 2.8027\n",
            "[Step 800] Loss: 2.0765\n",
            "[Step 810] Loss: 2.8328\n",
            "[Step 820] Loss: 2.4686\n",
            "[Step 830] Loss: 2.3268\n",
            "[Step 840] Loss: 2.5441\n",
            "[Step 850] Loss: 2.4563\n",
            "[Step 860] Loss: 2.7281\n",
            "[Step 870] Loss: 2.3366\n",
            "[Step 880] Loss: 2.3723\n",
            "[Step 890] Loss: 2.6939\n",
            "[Step 900] Loss: 2.4259\n",
            "[Step 910] Loss: 2.8365\n",
            "[Step 920] Loss: 2.5838\n",
            "[Step 930] Loss: 2.6871\n",
            "[Step 940] Loss: 2.2095\n",
            "[Step 950] Loss: 2.2712\n",
            "[Step 960] Loss: 2.7229\n",
            "[Step 970] Loss: 2.2057\n",
            "[Step 980] Loss: 2.8889\n",
            "[Step 990] Loss: 2.6509\n",
            "[Step 1000] Loss: 2.4621\n",
            "[Step 1010] Loss: 1.8890\n",
            "[Step 1020] Loss: 2.0523\n",
            "[Step 1030] Loss: 2.5148\n",
            "[Step 1040] Loss: 2.5010\n",
            "[Step 1050] Loss: 2.4726\n",
            "[Step 1060] Loss: 2.8383\n",
            "[Step 1070] Loss: 2.2362\n",
            "[Step 1080] Loss: 2.3169\n",
            "[Step 1090] Loss: 2.5724\n",
            "[Step 1100] Loss: 2.1745\n",
            "[Step 1110] Loss: 2.4317\n",
            "[Step 1120] Loss: 2.6122\n",
            "[Step 1130] Loss: 2.1725\n",
            "[Step 1140] Loss: 2.2286\n",
            "[Step 1150] Loss: 2.5765\n",
            "[Step 1160] Loss: 2.5176\n",
            "[Step 1170] Loss: 3.4736\n",
            "[Step 1180] Loss: 2.8233\n",
            "[Step 1190] Loss: 2.5500\n",
            "[Step 1200] Loss: 2.4643\n",
            "[Step 1210] Loss: 2.3609\n",
            "[Step 1220] Loss: 2.4912\n",
            "[Step 1230] Loss: 2.4543\n",
            "[Step 1240] Loss: 2.4069\n",
            "[Step 1250] Loss: 2.4915\n",
            "[Step 1260] Loss: 2.5557\n",
            "[Step 1270] Loss: 2.5892\n",
            "[Step 1280] Loss: 2.8123\n",
            "[Step 1290] Loss: 3.2289\n",
            "[Step 1300] Loss: 2.5089\n",
            "[Step 1310] Loss: 2.8344\n",
            "[Step 1320] Loss: 2.3157\n",
            "[Step 1330] Loss: 2.6049\n",
            "[Step 1340] Loss: 2.9380\n",
            "[Step 1350] Loss: 2.5171\n",
            "[Step 1360] Loss: 2.6013\n",
            "[Step 1370] Loss: 2.0743\n",
            "[Step 1380] Loss: 2.6239\n",
            "[Step 1390] Loss: 2.5896\n",
            "[Step 1400] Loss: 2.0474\n",
            "[Step 1410] Loss: 2.4531\n",
            "[Step 1420] Loss: 2.4922\n",
            "[Step 1430] Loss: 2.7028\n",
            "[Step 1440] Loss: 2.3979\n",
            "[Step 1450] Loss: 2.2160\n",
            "[Step 1460] Loss: 2.8099\n",
            "[Step 1470] Loss: 2.6536\n",
            "[Step 1480] Loss: 2.9101\n",
            "[Step 1490] Loss: 2.4977\n",
            "[Step 1500] Loss: 2.0175\n",
            "[Step 1510] Loss: 2.7834\n",
            "[Step 1520] Loss: 2.4104\n",
            "[Step 1530] Loss: 2.4049\n",
            "[Step 1540] Loss: 2.9946\n",
            "[Step 1550] Loss: 2.7713\n",
            "[Step 1560] Loss: 2.7056\n",
            "[Step 1570] Loss: 3.0117\n",
            "[Step 1580] Loss: 2.8048\n",
            "[Step 1590] Loss: 2.6145\n",
            "[Step 1600] Loss: 2.1819\n",
            "[Step 1610] Loss: 2.5583\n",
            "[Step 1620] Loss: 2.8163\n",
            "[Step 1630] Loss: 2.7715\n",
            "[Step 1640] Loss: 2.5953\n",
            "[Step 1650] Loss: 2.8628\n",
            "[Step 1660] Loss: 2.7409\n",
            "[Step 1670] Loss: 2.7879\n",
            "[Step 1680] Loss: 2.1943\n",
            "[Step 1690] Loss: 2.4795\n",
            "[Step 1700] Loss: 3.0135\n",
            "[Step 1710] Loss: 2.2682\n",
            "[Step 1720] Loss: 2.7846\n",
            "[Step 1730] Loss: 2.9632\n",
            "[Step 1740] Loss: 2.6807\n",
            "[Step 1750] Loss: 2.4272\n",
            "[Step 1760] Loss: 2.9484\n",
            "[Step 1770] Loss: 2.5211\n",
            "[Step 1780] Loss: 2.3282\n",
            "[Step 1790] Loss: 2.7439\n",
            "[Step 1800] Loss: 2.1308\n",
            "[Step 1810] Loss: 2.6036\n",
            "[Step 1820] Loss: 2.7166\n",
            "[Step 1830] Loss: 2.6502\n",
            "[Step 1840] Loss: 2.4555\n",
            "[Step 1850] Loss: 2.3206\n",
            "[Step 1860] Loss: 2.7370\n",
            "[Step 1870] Loss: 2.6742\n",
            "[Step 1880] Loss: 2.6858\n",
            "[Step 1890] Loss: 1.9561\n",
            "ðŸ“˜ Epoch 41 - Avg Training Loss: 2.5304\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5706 | Accuracy: 0.3596 | Precision: 0.3830\n",
            "[Step 1900] Loss: 2.4306\n",
            "[Step 1910] Loss: 2.8350\n",
            "[Step 1920] Loss: 2.5288\n",
            "[Step 1930] Loss: 2.2956\n",
            "[Step 1940] Loss: 2.7482\n",
            "[Step 1950] Loss: 2.2864\n",
            "[Step 1960] Loss: 2.2635\n",
            "[Step 1970] Loss: 1.9580\n",
            "[Step 1980] Loss: 2.1031\n",
            "[Step 1990] Loss: 2.2296\n",
            "[Step 2000] Loss: 2.7655\n",
            "[Step 2010] Loss: 2.1628\n",
            "[Step 2020] Loss: 2.4441\n",
            "[Step 2030] Loss: 2.5120\n",
            "[Step 2040] Loss: 1.8801\n",
            "[Step 2050] Loss: 2.5468\n",
            "[Step 2060] Loss: 2.5822\n",
            "[Step 2070] Loss: 2.0832\n",
            "[Step 2080] Loss: 1.5842\n",
            "[Step 2090] Loss: 2.2123\n",
            "[Step 2100] Loss: 2.2888\n",
            "[Step 2110] Loss: 2.2943\n",
            "[Step 2120] Loss: 2.7208\n",
            "[Step 2130] Loss: 2.4345\n",
            "[Step 2140] Loss: 2.4345\n",
            "[Step 2150] Loss: 2.3232\n",
            "[Step 2160] Loss: 2.5310\n",
            "[Step 2170] Loss: 1.9639\n",
            "[Step 2180] Loss: 3.1518\n",
            "[Step 2190] Loss: 2.6829\n",
            "[Step 2200] Loss: 2.8638\n",
            "[Step 2210] Loss: 2.9441\n",
            "[Step 2220] Loss: 2.4465\n",
            "[Step 2230] Loss: 2.6597\n",
            "[Step 2240] Loss: 2.2174\n",
            "[Step 2250] Loss: 1.9962\n",
            "[Step 2260] Loss: 2.7569\n",
            "[Step 2270] Loss: 2.4981\n",
            "[Step 2280] Loss: 2.2049\n",
            "[Step 2290] Loss: 2.7360\n",
            "[Step 2300] Loss: 1.9675\n",
            "[Step 2310] Loss: 2.5107\n",
            "[Step 2320] Loss: 2.9674\n",
            "[Step 2330] Loss: 2.5021\n",
            "[Step 2340] Loss: 2.5269\n",
            "[Step 2350] Loss: 2.2111\n",
            "[Step 2360] Loss: 2.6268\n",
            "[Step 2370] Loss: 2.2777\n",
            "[Step 2380] Loss: 2.2960\n",
            "[Step 2390] Loss: 2.3019\n",
            "[Step 2400] Loss: 2.3604\n",
            "[Step 2410] Loss: 2.2633\n",
            "[Step 2420] Loss: 3.0311\n",
            "[Step 2430] Loss: 2.1024\n",
            "[Step 2440] Loss: 1.7436\n",
            "[Step 2450] Loss: 2.4262\n",
            "[Step 2460] Loss: 2.3730\n",
            "[Step 2470] Loss: 2.5280\n",
            "[Step 2480] Loss: 2.5143\n",
            "[Step 2490] Loss: 2.8162\n",
            "[Step 2500] Loss: 2.7184\n",
            "[Step 2510] Loss: 2.1924\n",
            "[Step 2520] Loss: 3.0820\n",
            "[Step 2530] Loss: 2.5937\n",
            "[Step 2540] Loss: 2.2845\n",
            "[Step 2550] Loss: 2.2953\n",
            "[Step 2560] Loss: 2.5430\n",
            "[Step 2570] Loss: 1.8296\n",
            "[Step 2580] Loss: 2.2328\n",
            "[Step 2590] Loss: 2.6140\n",
            "[Step 2600] Loss: 2.8633\n",
            "[Step 2610] Loss: 2.6066\n",
            "[Step 2620] Loss: 2.6541\n",
            "[Step 2630] Loss: 2.1185\n",
            "[Step 2640] Loss: 2.2321\n",
            "[Step 2650] Loss: 2.5593\n",
            "[Step 2660] Loss: 2.7724\n",
            "[Step 2670] Loss: 2.2249\n",
            "[Step 2680] Loss: 2.2880\n",
            "[Step 2690] Loss: 1.9881\n",
            "[Step 2700] Loss: 2.5833\n",
            "[Step 2710] Loss: 2.1925\n",
            "[Step 2720] Loss: 2.8638\n",
            "[Step 2730] Loss: 2.8136\n",
            "[Step 2740] Loss: 2.0613\n",
            "[Step 2750] Loss: 2.4418\n",
            "[Step 2760] Loss: 2.5041\n",
            "[Step 2770] Loss: 2.2919\n",
            "[Step 2780] Loss: 2.1510\n",
            "[Step 2790] Loss: 2.2659\n",
            "[Step 2800] Loss: 1.8273\n",
            "[Step 2810] Loss: 2.5874\n",
            "[Step 2820] Loss: 2.2423\n",
            "[Step 2830] Loss: 2.6075\n",
            "[Step 2840] Loss: 2.9044\n",
            "[Step 2850] Loss: 2.8369\n",
            "[Step 2860] Loss: 2.8828\n",
            "[Step 2870] Loss: 3.0958\n",
            "[Step 2880] Loss: 2.4545\n",
            "[Step 2890] Loss: 2.5720\n",
            "[Step 2900] Loss: 2.4807\n",
            "[Step 2910] Loss: 2.5412\n",
            "[Step 2920] Loss: 2.4615\n",
            "[Step 2930] Loss: 3.2070\n",
            "[Step 2940] Loss: 2.4969\n",
            "[Step 2950] Loss: 2.4198\n",
            "[Step 2960] Loss: 2.7090\n",
            "[Step 2970] Loss: 2.6906\n",
            "[Step 2980] Loss: 3.0451\n",
            "[Step 2990] Loss: 2.3210\n",
            "[Step 3000] Loss: 2.5383\n",
            "[Step 3010] Loss: 2.5986\n",
            "[Step 3020] Loss: 2.3240\n",
            "[Step 3030] Loss: 2.5812\n",
            "[Step 3040] Loss: 3.0503\n",
            "[Step 3050] Loss: 2.2499\n",
            "[Step 3060] Loss: 2.5846\n",
            "[Step 3070] Loss: 2.2993\n",
            "[Step 3080] Loss: 2.7413\n",
            "[Step 3090] Loss: 2.7795\n",
            "[Step 3100] Loss: 2.6761\n",
            "[Step 3110] Loss: 2.1166\n",
            "[Step 3120] Loss: 3.1267\n",
            "[Step 3130] Loss: 2.2850\n",
            "[Step 3140] Loss: 2.4867\n",
            "[Step 3150] Loss: 2.5928\n",
            "[Step 3160] Loss: 2.3339\n",
            "[Step 3170] Loss: 3.0004\n",
            "[Step 3180] Loss: 2.4821\n",
            "[Step 3190] Loss: 2.7250\n",
            "[Step 3200] Loss: 2.3793\n",
            "[Step 3210] Loss: 2.5792\n",
            "[Step 3220] Loss: 2.3617\n",
            "[Step 3230] Loss: 2.2514\n",
            "[Step 3240] Loss: 2.6577\n",
            "[Step 3250] Loss: 1.9457\n",
            "[Step 3260] Loss: 1.9650\n",
            "[Step 3270] Loss: 2.2788\n",
            "[Step 3280] Loss: 2.7509\n",
            "[Step 3290] Loss: 2.2061\n",
            "[Step 3300] Loss: 2.7091\n",
            "[Step 3310] Loss: 2.8749\n",
            "[Step 3320] Loss: 2.2190\n",
            "[Step 3330] Loss: 2.5534\n",
            "[Step 3340] Loss: 2.3004\n",
            "[Step 3350] Loss: 2.5303\n",
            "[Step 3360] Loss: 2.4360\n",
            "[Step 3370] Loss: 2.0824\n",
            "[Step 3380] Loss: 2.4957\n",
            "[Step 3390] Loss: 2.4010\n",
            "[Step 3400] Loss: 1.9027\n",
            "[Step 3410] Loss: 2.6730\n",
            "[Step 3420] Loss: 2.2399\n",
            "[Step 3430] Loss: 2.7721\n",
            "[Step 3440] Loss: 3.1195\n",
            "[Step 3450] Loss: 2.2246\n",
            "[Step 3460] Loss: 2.7129\n",
            "[Step 3470] Loss: 2.7739\n",
            "[Step 3480] Loss: 2.3133\n",
            "[Step 3490] Loss: 2.1802\n",
            "[Step 3500] Loss: 2.4327\n",
            "[Step 3510] Loss: 2.3641\n",
            "[Step 3520] Loss: 2.1890\n",
            "[Step 3530] Loss: 2.0383\n",
            "[Step 3540] Loss: 2.6656\n",
            "[Step 3550] Loss: 2.4584\n",
            "[Step 3560] Loss: 2.1104\n",
            "[Step 3570] Loss: 2.2730\n",
            "[Step 3580] Loss: 1.9795\n",
            "[Step 3590] Loss: 2.1971\n",
            "[Step 3600] Loss: 3.1339\n",
            "[Step 3610] Loss: 2.3662\n",
            "[Step 3620] Loss: 2.3422\n",
            "[Step 3630] Loss: 2.5369\n",
            "[Step 3640] Loss: 2.0292\n",
            "[Step 3650] Loss: 2.2412\n",
            "[Step 3660] Loss: 2.6779\n",
            "[Step 3670] Loss: 2.5599\n",
            "[Step 3680] Loss: 2.4236\n",
            "[Step 3690] Loss: 2.7083\n",
            "[Step 3700] Loss: 2.5225\n",
            "[Step 3710] Loss: 2.8899\n",
            "[Step 3720] Loss: 2.4495\n",
            "[Step 3730] Loss: 2.7001\n",
            "[Step 3740] Loss: 2.2057\n",
            "[Step 3750] Loss: 2.5538\n",
            "[Step 3760] Loss: 2.5639\n",
            "[Step 3770] Loss: 2.7946\n",
            "[Step 3780] Loss: 2.3589\n",
            "ðŸ“˜ Epoch 42 - Avg Training Loss: 2.5051\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5460 | Accuracy: 0.3651 | Precision: 0.3746\n",
            "[Step 3790] Loss: 2.4648\n",
            "[Step 3800] Loss: 2.2911\n",
            "[Step 3810] Loss: 3.0169\n",
            "[Step 3820] Loss: 2.6859\n",
            "[Step 3830] Loss: 3.0959\n",
            "[Step 3840] Loss: 2.4355\n",
            "[Step 3850] Loss: 2.5788\n",
            "[Step 3860] Loss: 2.3293\n",
            "[Step 3870] Loss: 2.4922\n",
            "[Step 3880] Loss: 2.5783\n",
            "[Step 3890] Loss: 2.5974\n",
            "[Step 3900] Loss: 2.6911\n",
            "[Step 3910] Loss: 2.3814\n",
            "[Step 3920] Loss: 2.6261\n",
            "[Step 3930] Loss: 2.3819\n",
            "[Step 3940] Loss: 2.2013\n",
            "[Step 3950] Loss: 2.6703\n",
            "[Step 3960] Loss: 2.2598\n",
            "[Step 3970] Loss: 2.4070\n",
            "[Step 3980] Loss: 2.4765\n",
            "[Step 3990] Loss: 2.0365\n",
            "[Step 4000] Loss: 2.3954\n",
            "[Step 4010] Loss: 2.8753\n",
            "[Step 4020] Loss: 2.4866\n",
            "[Step 4030] Loss: 2.5547\n",
            "[Step 4040] Loss: 2.6240\n",
            "[Step 4050] Loss: 2.8971\n",
            "[Step 4060] Loss: 2.5508\n",
            "[Step 4070] Loss: 2.4291\n",
            "[Step 4080] Loss: 2.4557\n",
            "[Step 4090] Loss: 2.5572\n",
            "[Step 4100] Loss: 2.8933\n",
            "[Step 4110] Loss: 2.4804\n",
            "[Step 4120] Loss: 2.4371\n",
            "[Step 4130] Loss: 2.3490\n",
            "[Step 4140] Loss: 2.5928\n",
            "[Step 4150] Loss: 2.3398\n",
            "[Step 4160] Loss: 2.2741\n",
            "[Step 4170] Loss: 2.3813\n",
            "[Step 4180] Loss: 2.8239\n",
            "[Step 4190] Loss: 2.3948\n",
            "[Step 4200] Loss: 2.7258\n",
            "[Step 4210] Loss: 2.3696\n",
            "[Step 4220] Loss: 2.2891\n",
            "[Step 4230] Loss: 2.1215\n",
            "[Step 4240] Loss: 2.7528\n",
            "[Step 4250] Loss: 2.6263\n",
            "[Step 4260] Loss: 2.4366\n",
            "[Step 4270] Loss: 2.0504\n",
            "[Step 4280] Loss: 1.9774\n",
            "[Step 4290] Loss: 2.8979\n",
            "[Step 4300] Loss: 1.8968\n",
            "[Step 4310] Loss: 2.4581\n",
            "[Step 4320] Loss: 2.6455\n",
            "[Step 4330] Loss: 1.9583\n",
            "[Step 4340] Loss: 2.6535\n",
            "[Step 4350] Loss: 2.6267\n",
            "[Step 4360] Loss: 2.5856\n",
            "[Step 4370] Loss: 2.4814\n",
            "[Step 4380] Loss: 2.8834\n",
            "[Step 4390] Loss: 2.1152\n",
            "[Step 4400] Loss: 2.4334\n",
            "[Step 4410] Loss: 2.5552\n",
            "[Step 4420] Loss: 2.3773\n",
            "[Step 4430] Loss: 2.5602\n",
            "[Step 4440] Loss: 2.0865\n",
            "[Step 4450] Loss: 2.6074\n",
            "[Step 4460] Loss: 2.7317\n",
            "[Step 4470] Loss: 2.1575\n",
            "[Step 4480] Loss: 2.4917\n",
            "[Step 4490] Loss: 2.9031\n",
            "[Step 4500] Loss: 2.1638\n",
            "[Step 4510] Loss: 2.3989\n",
            "[Step 4520] Loss: 2.3143\n",
            "[Step 4530] Loss: 2.7598\n",
            "[Step 4540] Loss: 2.7182\n",
            "[Step 4550] Loss: 2.2285\n",
            "[Step 4560] Loss: 2.5718\n",
            "[Step 4570] Loss: 3.1825\n",
            "[Step 4580] Loss: 2.1647\n",
            "[Step 4590] Loss: 2.5069\n",
            "[Step 4600] Loss: 2.8186\n",
            "[Step 4610] Loss: 2.3639\n",
            "[Step 4620] Loss: 2.9005\n",
            "[Step 4630] Loss: 2.6370\n",
            "[Step 4640] Loss: 2.7140\n",
            "[Step 4650] Loss: 1.6952\n",
            "[Step 4660] Loss: 2.5821\n",
            "[Step 4670] Loss: 2.2555\n",
            "[Step 4680] Loss: 3.0716\n",
            "[Step 4690] Loss: 2.3335\n",
            "[Step 4700] Loss: 2.5619\n",
            "[Step 4710] Loss: 2.5754\n",
            "[Step 4720] Loss: 2.5355\n",
            "[Step 4730] Loss: 2.5694\n",
            "[Step 4740] Loss: 2.6880\n",
            "[Step 4750] Loss: 2.1875\n",
            "[Step 4760] Loss: 2.2288\n",
            "[Step 4770] Loss: 2.5548\n",
            "[Step 4780] Loss: 3.0602\n",
            "[Step 4790] Loss: 2.1285\n",
            "[Step 4800] Loss: 2.4363\n",
            "[Step 4810] Loss: 2.7299\n",
            "[Step 4820] Loss: 2.6436\n",
            "[Step 4830] Loss: 2.1037\n",
            "[Step 4840] Loss: 2.6242\n",
            "[Step 4850] Loss: 2.4670\n",
            "[Step 4860] Loss: 2.2813\n",
            "[Step 4870] Loss: 2.1313\n",
            "[Step 4880] Loss: 2.4289\n",
            "[Step 4890] Loss: 2.6463\n",
            "[Step 4900] Loss: 2.3977\n",
            "[Step 4910] Loss: 2.6539\n",
            "[Step 4920] Loss: 2.5686\n",
            "[Step 4930] Loss: 2.6155\n",
            "[Step 4940] Loss: 2.5797\n",
            "[Step 4950] Loss: 2.4803\n",
            "[Step 4960] Loss: 2.4984\n",
            "[Step 4970] Loss: 2.5073\n",
            "[Step 4980] Loss: 2.5084\n",
            "[Step 4990] Loss: 2.3619\n",
            "[Step 5000] Loss: 2.9819\n",
            "[Step 5010] Loss: 2.5375\n",
            "[Step 5020] Loss: 2.4740\n",
            "[Step 5030] Loss: 2.2106\n",
            "[Step 5040] Loss: 2.7394\n",
            "[Step 5050] Loss: 2.5479\n",
            "[Step 5060] Loss: 2.2827\n",
            "[Step 5070] Loss: 2.6146\n",
            "[Step 5080] Loss: 2.6084\n",
            "[Step 5090] Loss: 2.5185\n",
            "[Step 5100] Loss: 2.2412\n",
            "[Step 5110] Loss: 2.4204\n",
            "[Step 5120] Loss: 2.9607\n",
            "[Step 5130] Loss: 2.1251\n",
            "[Step 5140] Loss: 2.1508\n",
            "[Step 5150] Loss: 2.5160\n",
            "[Step 5160] Loss: 2.5513\n",
            "[Step 5170] Loss: 2.4525\n",
            "[Step 5180] Loss: 2.6126\n",
            "[Step 5190] Loss: 2.7095\n",
            "[Step 5200] Loss: 2.5409\n",
            "[Step 5210] Loss: 2.8326\n",
            "[Step 5220] Loss: 1.7920\n",
            "[Step 5230] Loss: 2.5751\n",
            "[Step 5240] Loss: 2.3513\n",
            "[Step 5250] Loss: 2.7955\n",
            "[Step 5260] Loss: 2.6535\n",
            "[Step 5270] Loss: 2.4474\n",
            "[Step 5280] Loss: 2.6805\n",
            "[Step 5290] Loss: 2.8904\n",
            "[Step 5300] Loss: 2.8643\n",
            "[Step 5310] Loss: 2.7943\n",
            "[Step 5320] Loss: 2.5004\n",
            "[Step 5330] Loss: 2.1419\n",
            "[Step 5340] Loss: 2.2160\n",
            "[Step 5350] Loss: 2.2473\n",
            "[Step 5360] Loss: 2.6764\n",
            "[Step 5370] Loss: 2.6788\n",
            "[Step 5380] Loss: 2.7698\n",
            "[Step 5390] Loss: 2.2000\n",
            "[Step 5400] Loss: 2.3505\n",
            "[Step 5410] Loss: 2.3560\n",
            "[Step 5420] Loss: 2.1310\n",
            "[Step 5430] Loss: 2.4865\n",
            "[Step 5440] Loss: 2.7815\n",
            "[Step 5450] Loss: 2.4910\n",
            "[Step 5460] Loss: 2.5408\n",
            "[Step 5470] Loss: 2.6515\n",
            "[Step 5480] Loss: 2.7724\n",
            "[Step 5490] Loss: 2.4331\n",
            "[Step 5500] Loss: 2.4321\n",
            "[Step 5510] Loss: 2.8784\n",
            "[Step 5520] Loss: 2.3486\n",
            "[Step 5530] Loss: 2.2893\n",
            "[Step 5540] Loss: 2.2530\n",
            "[Step 5550] Loss: 1.9126\n",
            "[Step 5560] Loss: 2.4602\n",
            "[Step 5570] Loss: 2.8525\n",
            "[Step 5580] Loss: 2.9814\n",
            "[Step 5590] Loss: 2.4150\n",
            "[Step 5600] Loss: 2.3982\n",
            "[Step 5610] Loss: 2.5303\n",
            "[Step 5620] Loss: 2.6891\n",
            "[Step 5630] Loss: 2.7325\n",
            "[Step 5640] Loss: 2.4130\n",
            "[Step 5650] Loss: 2.1174\n",
            "[Step 5660] Loss: 2.3236\n",
            "[Step 5670] Loss: 2.1565\n",
            "[Step 5680] Loss: 1.7338\n",
            "ðŸ“˜ Epoch 43 - Avg Training Loss: 2.4798\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4643 | Accuracy: 0.3765 | Precision: 0.3793\n",
            "[Step 5690] Loss: 2.6039\n",
            "[Step 5700] Loss: 2.5932\n",
            "[Step 5710] Loss: 2.2289\n",
            "[Step 5720] Loss: 2.5378\n",
            "[Step 5730] Loss: 2.3725\n",
            "[Step 5740] Loss: 2.4279\n",
            "[Step 5750] Loss: 2.7753\n",
            "[Step 5760] Loss: 2.5602\n",
            "[Step 5770] Loss: 2.7185\n",
            "[Step 5780] Loss: 2.3522\n",
            "[Step 5790] Loss: 1.8162\n",
            "[Step 5800] Loss: 2.7370\n",
            "[Step 5810] Loss: 1.8882\n",
            "[Step 5820] Loss: 2.5491\n",
            "[Step 5830] Loss: 2.3776\n",
            "[Step 5840] Loss: 2.5719\n",
            "[Step 5850] Loss: 2.5967\n",
            "[Step 5860] Loss: 2.0384\n",
            "[Step 5870] Loss: 2.6350\n",
            "[Step 5880] Loss: 2.6420\n",
            "[Step 5890] Loss: 2.5033\n",
            "[Step 5900] Loss: 2.1951\n",
            "[Step 5910] Loss: 2.3260\n",
            "[Step 5920] Loss: 1.9341\n",
            "[Step 5930] Loss: 2.5134\n",
            "[Step 5940] Loss: 2.5252\n",
            "[Step 5950] Loss: 1.7379\n",
            "[Step 5960] Loss: 2.7299\n",
            "[Step 5970] Loss: 2.4492\n",
            "[Step 5980] Loss: 2.9054\n",
            "[Step 5990] Loss: 2.1714\n",
            "[Step 6000] Loss: 1.4246\n",
            "[Step 6010] Loss: 2.6354\n",
            "[Step 6020] Loss: 2.6897\n",
            "[Step 6030] Loss: 2.9780\n",
            "[Step 6040] Loss: 2.2095\n",
            "[Step 6050] Loss: 2.5827\n",
            "[Step 6060] Loss: 2.6513\n",
            "[Step 6070] Loss: 2.5561\n",
            "[Step 6080] Loss: 2.2163\n",
            "[Step 6090] Loss: 2.3676\n",
            "[Step 6100] Loss: 2.4207\n",
            "[Step 6110] Loss: 2.3275\n",
            "[Step 6120] Loss: 2.4706\n",
            "[Step 6130] Loss: 2.2019\n",
            "[Step 6140] Loss: 2.6117\n",
            "[Step 6150] Loss: 2.5254\n",
            "[Step 6160] Loss: 3.0154\n",
            "[Step 6170] Loss: 2.5346\n",
            "[Step 6180] Loss: 2.7828\n",
            "[Step 6190] Loss: 2.4799\n",
            "[Step 6200] Loss: 2.5079\n",
            "[Step 6210] Loss: 2.7429\n",
            "[Step 6220] Loss: 2.6880\n",
            "[Step 6230] Loss: 2.9150\n",
            "[Step 6240] Loss: 2.5974\n",
            "[Step 6250] Loss: 2.2403\n",
            "[Step 6260] Loss: 2.6083\n",
            "[Step 6270] Loss: 2.0114\n",
            "[Step 6280] Loss: 2.6331\n",
            "[Step 6290] Loss: 2.2672\n",
            "[Step 6300] Loss: 2.5463\n",
            "[Step 6310] Loss: 2.4779\n",
            "[Step 6320] Loss: 3.2269\n",
            "[Step 6330] Loss: 2.6778\n",
            "[Step 6340] Loss: 2.3044\n",
            "[Step 6350] Loss: 2.5497\n",
            "[Step 6360] Loss: 2.2572\n",
            "[Step 6370] Loss: 2.7034\n",
            "[Step 6380] Loss: 2.4451\n",
            "[Step 6390] Loss: 2.0039\n",
            "[Step 6400] Loss: 2.7342\n",
            "[Step 6410] Loss: 2.7704\n",
            "[Step 6420] Loss: 1.8493\n",
            "[Step 6430] Loss: 2.1945\n",
            "[Step 6440] Loss: 2.4272\n",
            "[Step 6450] Loss: 2.1199\n",
            "[Step 6460] Loss: 2.6338\n",
            "[Step 6470] Loss: 2.2075\n",
            "[Step 6480] Loss: 2.2906\n",
            "[Step 6490] Loss: 2.8263\n",
            "[Step 6500] Loss: 2.0581\n",
            "[Step 6510] Loss: 2.2246\n",
            "[Step 6520] Loss: 2.3683\n",
            "[Step 6530] Loss: 2.1264\n",
            "[Step 6540] Loss: 2.7017\n",
            "[Step 6550] Loss: 2.4415\n",
            "[Step 6560] Loss: 2.1483\n",
            "[Step 6570] Loss: 2.0167\n",
            "[Step 6580] Loss: 2.3142\n",
            "[Step 6590] Loss: 2.8085\n",
            "[Step 6600] Loss: 2.9704\n",
            "[Step 6610] Loss: 2.5562\n",
            "[Step 6620] Loss: 1.8888\n",
            "[Step 6630] Loss: 2.4319\n",
            "[Step 6640] Loss: 1.9173\n",
            "[Step 6650] Loss: 1.7342\n",
            "[Step 6660] Loss: 2.6411\n",
            "[Step 6670] Loss: 2.5021\n",
            "[Step 6680] Loss: 2.1053\n",
            "[Step 6690] Loss: 2.3699\n",
            "[Step 6700] Loss: 2.9455\n",
            "[Step 6710] Loss: 2.3871\n",
            "[Step 6720] Loss: 2.7813\n",
            "[Step 6730] Loss: 2.7757\n",
            "[Step 6740] Loss: 2.6166\n",
            "[Step 6750] Loss: 2.0365\n",
            "[Step 6760] Loss: 2.5016\n",
            "[Step 6770] Loss: 2.1699\n",
            "[Step 6780] Loss: 2.4744\n",
            "[Step 6790] Loss: 2.4305\n",
            "[Step 6800] Loss: 2.3256\n",
            "[Step 6810] Loss: 2.2309\n",
            "[Step 6820] Loss: 2.5998\n",
            "[Step 6830] Loss: 2.7689\n",
            "[Step 6840] Loss: 2.2571\n",
            "[Step 6850] Loss: 2.9302\n",
            "[Step 6860] Loss: 2.4917\n",
            "[Step 6870] Loss: 2.4707\n",
            "[Step 6880] Loss: 2.4490\n",
            "[Step 6890] Loss: 2.1367\n",
            "[Step 6900] Loss: 2.9001\n",
            "[Step 6910] Loss: 2.3554\n",
            "[Step 6920] Loss: 2.3109\n",
            "[Step 6930] Loss: 2.9485\n",
            "[Step 6940] Loss: 1.8575\n",
            "[Step 6950] Loss: 2.3549\n",
            "[Step 6960] Loss: 2.4570\n",
            "[Step 6970] Loss: 2.2433\n",
            "[Step 6980] Loss: 2.3639\n",
            "[Step 6990] Loss: 2.1159\n",
            "[Step 7000] Loss: 2.8310\n",
            "[Step 7010] Loss: 2.5481\n",
            "[Step 7020] Loss: 3.1996\n",
            "[Step 7030] Loss: 2.5722\n",
            "[Step 7040] Loss: 2.3535\n",
            "[Step 7050] Loss: 2.2048\n",
            "[Step 7060] Loss: 2.3800\n",
            "[Step 7070] Loss: 2.6460\n",
            "[Step 7080] Loss: 2.4543\n",
            "[Step 7090] Loss: 2.3166\n",
            "[Step 7100] Loss: 2.0235\n",
            "[Step 7110] Loss: 2.9085\n",
            "[Step 7120] Loss: 2.3643\n",
            "[Step 7130] Loss: 2.6337\n",
            "[Step 7140] Loss: 2.3561\n",
            "[Step 7150] Loss: 2.5324\n",
            "[Step 7160] Loss: 2.1091\n",
            "[Step 7170] Loss: 2.7132\n",
            "[Step 7180] Loss: 2.1432\n",
            "[Step 7190] Loss: 2.8333\n",
            "[Step 7200] Loss: 2.4889\n",
            "[Step 7210] Loss: 2.2114\n",
            "[Step 7220] Loss: 2.3035\n",
            "[Step 7230] Loss: 2.6754\n",
            "[Step 7240] Loss: 1.8691\n",
            "[Step 7250] Loss: 2.3728\n",
            "[Step 7260] Loss: 2.4037\n",
            "[Step 7270] Loss: 2.5904\n",
            "[Step 7280] Loss: 2.2175\n",
            "[Step 7290] Loss: 2.1656\n",
            "[Step 7300] Loss: 2.7841\n",
            "[Step 7310] Loss: 1.9940\n",
            "[Step 7320] Loss: 3.0051\n",
            "[Step 7330] Loss: 2.3204\n",
            "[Step 7340] Loss: 2.0371\n",
            "[Step 7350] Loss: 2.6439\n",
            "[Step 7360] Loss: 3.1248\n",
            "[Step 7370] Loss: 2.6770\n",
            "[Step 7380] Loss: 2.4080\n",
            "[Step 7390] Loss: 2.3802\n",
            "[Step 7400] Loss: 2.8753\n",
            "[Step 7410] Loss: 2.4907\n",
            "[Step 7420] Loss: 2.4241\n",
            "[Step 7430] Loss: 2.6494\n",
            "[Step 7440] Loss: 2.5003\n",
            "[Step 7450] Loss: 2.4592\n",
            "[Step 7460] Loss: 2.5538\n",
            "[Step 7470] Loss: 2.4022\n",
            "[Step 7480] Loss: 2.3267\n",
            "[Step 7490] Loss: 2.3410\n",
            "[Step 7500] Loss: 2.1345\n",
            "[Step 7510] Loss: 2.0604\n",
            "[Step 7520] Loss: 2.8875\n",
            "[Step 7530] Loss: 2.0208\n",
            "[Step 7540] Loss: 2.7640\n",
            "[Step 7550] Loss: 2.2773\n",
            "[Step 7560] Loss: 2.6096\n",
            "[Step 7570] Loss: 2.2967\n",
            "ðŸ“˜ Epoch 44 - Avg Training Loss: 2.4346\n",
            "ðŸ“Š Final Validation â€” Loss: 2.5011 | Accuracy: 0.3733 | Precision: 0.3795\n",
            "[Step 7580] Loss: 2.5285\n",
            "[Step 7590] Loss: 2.4275\n",
            "[Step 7600] Loss: 2.4882\n",
            "[Step 7610] Loss: 1.8976\n",
            "[Step 7620] Loss: 2.3290\n",
            "[Step 7630] Loss: 2.2548\n",
            "[Step 7640] Loss: 2.4377\n",
            "[Step 7650] Loss: 1.9755\n",
            "[Step 7660] Loss: 2.7463\n",
            "[Step 7670] Loss: 2.2102\n",
            "[Step 7680] Loss: 1.9515\n",
            "[Step 7690] Loss: 2.2035\n",
            "[Step 7700] Loss: 2.5828\n",
            "[Step 7710] Loss: 2.1373\n",
            "[Step 7720] Loss: 2.3758\n",
            "[Step 7730] Loss: 2.1305\n",
            "[Step 7740] Loss: 2.3584\n",
            "[Step 7750] Loss: 2.3266\n",
            "[Step 7760] Loss: 2.1593\n",
            "[Step 7770] Loss: 2.3291\n",
            "[Step 7780] Loss: 2.3187\n",
            "[Step 7790] Loss: 2.7756\n",
            "[Step 7800] Loss: 2.0821\n",
            "[Step 7810] Loss: 2.1081\n",
            "[Step 7820] Loss: 3.2807\n",
            "[Step 7830] Loss: 2.7235\n",
            "[Step 7840] Loss: 3.0933\n",
            "[Step 7850] Loss: 2.6129\n",
            "[Step 7860] Loss: 2.8421\n",
            "[Step 7870] Loss: 2.3442\n",
            "[Step 7880] Loss: 2.2468\n",
            "[Step 7890] Loss: 2.1079\n",
            "[Step 7900] Loss: 2.7602\n",
            "[Step 7910] Loss: 2.1723\n",
            "[Step 7920] Loss: 2.5745\n",
            "[Step 7930] Loss: 2.7793\n",
            "[Step 7940] Loss: 2.4537\n",
            "[Step 7950] Loss: 2.7022\n",
            "[Step 7960] Loss: 2.4019\n",
            "[Step 7970] Loss: 2.2546\n",
            "[Step 7980] Loss: 2.1834\n",
            "[Step 7990] Loss: 2.1340\n",
            "[Step 8000] Loss: 2.6711\n",
            "[Step 8010] Loss: 2.6628\n",
            "[Step 8020] Loss: 2.6145\n",
            "[Step 8030] Loss: 2.2934\n",
            "[Step 8040] Loss: 2.0476\n",
            "[Step 8050] Loss: 2.2873\n",
            "[Step 8060] Loss: 2.6835\n",
            "[Step 8070] Loss: 2.2712\n",
            "[Step 8080] Loss: 2.8817\n",
            "[Step 8090] Loss: 2.5106\n",
            "[Step 8100] Loss: 2.0746\n",
            "[Step 8110] Loss: 1.9697\n",
            "[Step 8120] Loss: 2.2877\n",
            "[Step 8130] Loss: 2.8512\n",
            "[Step 8140] Loss: 2.6244\n",
            "[Step 8150] Loss: 3.0223\n",
            "[Step 8160] Loss: 2.6961\n",
            "[Step 8170] Loss: 2.1663\n",
            "[Step 8180] Loss: 2.3798\n",
            "[Step 8190] Loss: 2.6166\n",
            "[Step 8200] Loss: 2.8261\n",
            "[Step 8210] Loss: 2.4916\n",
            "[Step 8220] Loss: 2.9712\n",
            "[Step 8230] Loss: 2.0291\n",
            "[Step 8240] Loss: 2.1932\n",
            "[Step 8250] Loss: 2.1273\n",
            "[Step 8260] Loss: 2.1015\n",
            "[Step 8270] Loss: 2.2705\n",
            "[Step 8280] Loss: 2.7835\n",
            "[Step 8290] Loss: 2.3453\n",
            "[Step 8300] Loss: 2.3125\n",
            "[Step 8310] Loss: 2.3523\n",
            "[Step 8320] Loss: 2.2156\n",
            "[Step 8330] Loss: 2.1720\n",
            "[Step 8340] Loss: 2.4483\n",
            "[Step 8350] Loss: 2.2402\n",
            "[Step 8360] Loss: 2.7005\n",
            "[Step 8370] Loss: 2.3101\n",
            "[Step 8380] Loss: 2.2007\n",
            "[Step 8390] Loss: 2.6379\n",
            "[Step 8400] Loss: 2.4769\n",
            "[Step 8410] Loss: 2.6220\n",
            "[Step 8420] Loss: 2.0761\n",
            "[Step 8430] Loss: 2.5095\n",
            "[Step 8440] Loss: 2.7717\n",
            "[Step 8450] Loss: 2.4713\n",
            "[Step 8460] Loss: 2.2827\n",
            "[Step 8470] Loss: 2.1675\n",
            "[Step 8480] Loss: 2.4142\n",
            "[Step 8490] Loss: 2.1327\n",
            "[Step 8500] Loss: 2.1070\n",
            "[Step 8510] Loss: 2.3251\n",
            "[Step 8520] Loss: 2.0610\n",
            "[Step 8530] Loss: 2.0398\n",
            "[Step 8540] Loss: 2.5174\n",
            "[Step 8550] Loss: 2.3751\n",
            "[Step 8560] Loss: 2.1577\n",
            "[Step 8570] Loss: 2.6918\n",
            "[Step 8580] Loss: 1.8906\n",
            "[Step 8590] Loss: 2.4243\n",
            "[Step 8600] Loss: 2.8796\n",
            "[Step 8610] Loss: 2.2525\n",
            "[Step 8620] Loss: 2.5347\n",
            "[Step 8630] Loss: 2.3224\n",
            "[Step 8640] Loss: 2.1606\n",
            "[Step 8650] Loss: 2.3988\n",
            "[Step 8660] Loss: 2.1474\n",
            "[Step 8670] Loss: 2.3313\n",
            "[Step 8680] Loss: 2.7792\n",
            "[Step 8690] Loss: 1.8641\n",
            "[Step 8700] Loss: 2.4119\n",
            "[Step 8710] Loss: 2.1808\n",
            "[Step 8720] Loss: 2.2437\n",
            "[Step 8730] Loss: 2.8748\n",
            "[Step 8740] Loss: 2.6719\n",
            "[Step 8750] Loss: 2.4101\n",
            "[Step 8760] Loss: 2.4684\n",
            "[Step 8770] Loss: 2.2999\n",
            "[Step 8780] Loss: 2.7742\n",
            "[Step 8790] Loss: 2.0120\n",
            "[Step 8800] Loss: 2.7781\n",
            "[Step 8810] Loss: 2.5536\n",
            "[Step 8820] Loss: 2.3843\n",
            "[Step 8830] Loss: 2.2457\n",
            "[Step 8840] Loss: 2.3435\n",
            "[Step 8850] Loss: 2.8677\n",
            "[Step 8860] Loss: 2.0382\n",
            "[Step 8870] Loss: 2.4955\n",
            "[Step 8880] Loss: 2.3933\n",
            "[Step 8890] Loss: 1.9937\n",
            "[Step 8900] Loss: 2.2890\n",
            "[Step 8910] Loss: 2.4130\n",
            "[Step 8920] Loss: 2.3019\n",
            "[Step 8930] Loss: 1.9824\n",
            "[Step 8940] Loss: 2.0056\n",
            "[Step 8950] Loss: 2.1648\n",
            "[Step 8960] Loss: 2.2608\n",
            "[Step 8970] Loss: 2.3320\n",
            "[Step 8980] Loss: 2.5911\n",
            "[Step 8990] Loss: 3.0452\n",
            "[Step 9000] Loss: 2.6018\n",
            "[Step 9010] Loss: 2.6051\n",
            "[Step 9020] Loss: 1.8507\n",
            "[Step 9030] Loss: 2.3800\n",
            "[Step 9040] Loss: 2.7392\n",
            "[Step 9050] Loss: 2.0755\n",
            "[Step 9060] Loss: 2.7888\n",
            "[Step 9070] Loss: 2.8617\n",
            "[Step 9080] Loss: 2.3980\n",
            "[Step 9090] Loss: 2.2873\n",
            "[Step 9100] Loss: 2.2729\n",
            "[Step 9110] Loss: 2.1619\n",
            "[Step 9120] Loss: 2.4838\n",
            "[Step 9130] Loss: 2.2564\n",
            "[Step 9140] Loss: 1.9120\n",
            "[Step 9150] Loss: 2.5090\n",
            "[Step 9160] Loss: 2.5985\n",
            "[Step 9170] Loss: 2.3795\n",
            "[Step 9180] Loss: 2.7969\n",
            "[Step 9190] Loss: 2.4253\n",
            "[Step 9200] Loss: 2.3598\n",
            "[Step 9210] Loss: 2.3309\n",
            "[Step 9220] Loss: 2.6383\n",
            "[Step 9230] Loss: 2.4232\n",
            "[Step 9240] Loss: 2.3310\n",
            "[Step 9250] Loss: 2.5022\n",
            "[Step 9260] Loss: 2.6632\n",
            "[Step 9270] Loss: 2.5195\n",
            "[Step 9280] Loss: 2.9350\n",
            "[Step 9290] Loss: 2.5483\n",
            "[Step 9300] Loss: 2.7296\n",
            "[Step 9310] Loss: 2.3789\n",
            "[Step 9320] Loss: 2.7713\n",
            "[Step 9330] Loss: 3.2224\n",
            "[Step 9340] Loss: 2.4608\n",
            "[Step 9350] Loss: 2.1131\n",
            "[Step 9360] Loss: 2.7236\n",
            "[Step 9370] Loss: 2.4811\n",
            "[Step 9380] Loss: 2.2315\n",
            "[Step 9390] Loss: 2.9011\n",
            "[Step 9400] Loss: 2.7325\n",
            "[Step 9410] Loss: 2.3493\n",
            "[Step 9420] Loss: 2.5049\n",
            "[Step 9430] Loss: 2.6805\n",
            "[Step 9440] Loss: 1.7459\n",
            "[Step 9450] Loss: 3.4146\n",
            "[Step 9460] Loss: 2.0391\n",
            "[Step 9470] Loss: 2.7505\n",
            "ðŸ“˜ Epoch 45 - Avg Training Loss: 2.3977\n",
            "ðŸ“Š Final Validation â€” Loss: 2.4532 | Accuracy: 0.3839 | Precision: 0.3870\n",
            "[Step 9480] Loss: 2.2022\n",
            "[Step 9490] Loss: 2.1765\n",
            "[Step 9500] Loss: 1.7412\n",
            "[Step 9510] Loss: 2.3102\n",
            "[Step 9520] Loss: 1.8203\n",
            "[Step 9530] Loss: 2.3804\n",
            "[Step 9540] Loss: 1.9918\n",
            "[Step 9550] Loss: 2.4617\n",
            "[Step 9560] Loss: 2.4012\n",
            "[Step 9570] Loss: 2.4455\n",
            "[Step 9580] Loss: 2.4588\n",
            "[Step 9590] Loss: 2.5460\n",
            "[Step 9600] Loss: 2.4208\n",
            "[Step 9610] Loss: 2.3248\n",
            "[Step 9620] Loss: 2.4101\n",
            "[Step 9630] Loss: 2.4490\n",
            "[Step 9640] Loss: 3.0278\n",
            "[Step 9650] Loss: 2.6195\n",
            "[Step 9660] Loss: 1.9712\n",
            "[Step 9670] Loss: 2.7808\n",
            "[Step 9680] Loss: 2.3869\n",
            "[Step 9690] Loss: 2.1551\n",
            "[Step 9700] Loss: 2.5494\n",
            "[Step 9710] Loss: 2.5596\n",
            "[Step 9720] Loss: 1.9387\n",
            "[Step 9730] Loss: 2.4398\n",
            "[Step 9740] Loss: 2.4988\n",
            "[Step 9750] Loss: 2.1154\n",
            "[Step 9760] Loss: 2.3537\n",
            "[Step 9770] Loss: 2.4654\n",
            "[Step 9780] Loss: 2.2510\n",
            "[Step 9790] Loss: 2.5056\n",
            "[Step 9800] Loss: 2.2517\n",
            "[Step 9810] Loss: 2.6254\n",
            "[Step 9820] Loss: 2.6457\n",
            "[Step 9830] Loss: 2.6431\n",
            "[Step 9840] Loss: 2.5601\n",
            "[Step 9850] Loss: 1.9870\n",
            "[Step 9860] Loss: 2.5626\n",
            "[Step 9870] Loss: 2.0609\n",
            "[Step 9880] Loss: 2.1336\n",
            "[Step 9890] Loss: 3.2606\n",
            "[Step 9900] Loss: 2.2446\n",
            "[Step 9910] Loss: 2.5317\n",
            "[Step 9920] Loss: 2.2958\n",
            "[Step 9930] Loss: 3.1583\n",
            "[Step 9940] Loss: 2.3675\n",
            "[Step 9950] Loss: 2.1698\n",
            "[Step 9960] Loss: 2.7758\n",
            "[Step 9970] Loss: 2.7418\n",
            "[Step 9980] Loss: 2.8153\n",
            "[Step 9990] Loss: 1.9952\n",
            "[Step 10000] Loss: 1.8800\n",
            "[Step 10010] Loss: 2.5871\n",
            "[Step 10020] Loss: 2.0948\n",
            "[Step 10030] Loss: 2.2124\n",
            "[Step 10040] Loss: 1.8492\n",
            "[Step 10050] Loss: 2.8241\n",
            "[Step 10060] Loss: 2.5085\n",
            "[Step 10070] Loss: 2.7455\n",
            "[Step 10080] Loss: 2.1869\n",
            "[Step 10090] Loss: 2.3415\n",
            "[Step 10100] Loss: 2.9177\n",
            "[Step 10110] Loss: 2.5004\n",
            "[Step 10120] Loss: 2.0671\n",
            "[Step 10130] Loss: 2.1761\n",
            "[Step 10140] Loss: 2.1704\n",
            "[Step 10150] Loss: 2.0623\n",
            "[Step 10160] Loss: 2.2757\n",
            "[Step 10170] Loss: 2.5944\n",
            "[Step 10180] Loss: 2.0419\n",
            "[Step 10190] Loss: 2.1110\n",
            "[Step 10200] Loss: 2.0681\n",
            "[Step 10210] Loss: 2.0126\n",
            "[Step 10220] Loss: 2.1908\n",
            "[Step 10230] Loss: 2.4974\n",
            "[Step 10240] Loss: 2.3484\n",
            "[Step 10250] Loss: 2.2230\n",
            "[Step 10260] Loss: 2.6339\n",
            "[Step 10270] Loss: 2.7120\n",
            "[Step 10280] Loss: 2.4170\n",
            "[Step 10290] Loss: 2.3734\n",
            "[Step 10300] Loss: 1.5803\n",
            "[Step 10310] Loss: 2.2330\n",
            "[Step 10320] Loss: 3.0583\n",
            "[Step 10330] Loss: 2.2181\n",
            "[Step 10340] Loss: 2.2236\n",
            "[Step 10350] Loss: 2.7534\n",
            "[Step 10360] Loss: 2.2429\n",
            "[Step 10370] Loss: 2.7314\n",
            "[Step 10380] Loss: 2.5149\n",
            "[Step 10390] Loss: 2.5252\n",
            "[Step 10400] Loss: 2.0161\n",
            "[Step 10410] Loss: 2.7327\n",
            "[Step 10420] Loss: 1.9641\n",
            "[Step 10430] Loss: 2.5646\n",
            "[Step 10440] Loss: 2.5212\n",
            "[Step 10450] Loss: 2.3658\n",
            "[Step 10460] Loss: 2.4457\n",
            "[Step 10470] Loss: 2.5184\n",
            "[Step 10480] Loss: 2.8644\n",
            "[Step 10490] Loss: 2.0756\n",
            "[Step 10500] Loss: 1.9376\n",
            "[Step 10510] Loss: 2.3088\n",
            "[Step 10520] Loss: 2.0701\n",
            "[Step 10530] Loss: 2.1952\n",
            "[Step 10540] Loss: 3.2836\n",
            "[Step 10550] Loss: 2.0533\n",
            "[Step 10560] Loss: 2.4435\n",
            "[Step 10570] Loss: 2.6324\n",
            "[Step 10580] Loss: 2.0253\n",
            "[Step 10590] Loss: 2.1930\n",
            "[Step 10600] Loss: 1.9874\n",
            "[Step 10610] Loss: 2.4266\n",
            "[Step 10620] Loss: 2.1541\n",
            "[Step 10630] Loss: 2.3226\n",
            "[Step 10640] Loss: 2.0545\n",
            "[Step 10650] Loss: 2.8544\n",
            "[Step 10660] Loss: 2.4197\n",
            "[Step 10670] Loss: 2.1747\n",
            "[Step 10680] Loss: 2.2906\n",
            "[Step 10690] Loss: 2.8670\n",
            "[Step 10700] Loss: 1.9480\n",
            "[Step 10710] Loss: 2.8316\n",
            "[Step 10720] Loss: 2.5818\n",
            "[Step 10730] Loss: 2.3014\n",
            "[Step 10740] Loss: 2.5171\n",
            "[Step 10750] Loss: 2.1419\n",
            "[Step 10760] Loss: 1.9728\n",
            "[Step 10770] Loss: 1.8703\n",
            "[Step 10780] Loss: 2.2089\n",
            "[Step 10790] Loss: 1.6086\n",
            "[Step 10800] Loss: 1.9819\n",
            "[Step 10810] Loss: 2.4803\n",
            "[Step 10820] Loss: 2.3604\n",
            "[Step 10830] Loss: 2.6761\n",
            "[Step 10840] Loss: 2.6711\n",
            "[Step 10850] Loss: 2.3386\n",
            "[Step 10860] Loss: 2.0801\n",
            "[Step 10870] Loss: 2.0457\n",
            "[Step 10880] Loss: 2.1723\n",
            "[Step 10890] Loss: 2.9801\n",
            "[Step 10900] Loss: 2.2282\n",
            "[Step 10910] Loss: 2.1252\n",
            "[Step 10920] Loss: 2.8747\n",
            "[Step 10930] Loss: 2.8783\n",
            "[Step 10940] Loss: 2.4914\n",
            "[Step 10950] Loss: 2.5805\n",
            "[Step 10960] Loss: 2.1738\n",
            "[Step 10970] Loss: 2.5826\n",
            "[Step 10980] Loss: 2.4810\n",
            "[Step 10990] Loss: 2.2404\n",
            "[Step 11000] Loss: 1.9828\n",
            "[Step 11010] Loss: 2.7834\n",
            "[Step 11020] Loss: 2.2637\n",
            "[Step 11030] Loss: 2.2116\n",
            "[Step 11040] Loss: 2.1730\n",
            "[Step 11050] Loss: 2.5562\n",
            "[Step 11060] Loss: 2.1632\n",
            "[Step 11070] Loss: 2.3391\n",
            "[Step 11080] Loss: 2.3899\n",
            "[Step 11090] Loss: 2.2169\n",
            "[Step 11100] Loss: 2.4399\n",
            "[Step 11110] Loss: 2.3442\n",
            "[Step 11120] Loss: 2.7456\n",
            "[Step 11130] Loss: 2.4222\n",
            "[Step 11140] Loss: 2.5054\n",
            "[Step 11150] Loss: 2.1847\n",
            "[Step 11160] Loss: 2.1339\n",
            "[Step 11170] Loss: 2.5604\n",
            "[Step 11180] Loss: 2.0486\n",
            "[Step 11190] Loss: 2.6878\n",
            "[Step 11200] Loss: 2.3714\n",
            "[Step 11210] Loss: 2.3432\n",
            "[Step 11220] Loss: 2.0740\n",
            "[Step 11230] Loss: 2.1517\n",
            "[Step 11240] Loss: 1.9549\n",
            "[Step 11250] Loss: 3.1628\n",
            "[Step 11260] Loss: 2.4108\n",
            "[Step 11270] Loss: 2.2644\n",
            "[Step 11280] Loss: 2.4317\n",
            "[Step 11290] Loss: 2.6427\n",
            "[Step 11300] Loss: 2.3332\n",
            "[Step 11310] Loss: 2.4966\n",
            "[Step 11320] Loss: 1.8774\n",
            "[Step 11330] Loss: 2.3173\n",
            "[Step 11340] Loss: 1.9224\n",
            "[Step 11350] Loss: 2.4562\n",
            "[Step 11360] Loss: 2.3779\n",
            "ðŸ“˜ Epoch 46 - Avg Training Loss: 2.3487\n",
            "ðŸ“Š Final Validation â€” Loss: 2.3833 | Accuracy: 0.3962 | Precision: 0.3904\n",
            "[Step 11370] Loss: 2.2856\n",
            "[Step 11380] Loss: 2.1585\n",
            "[Step 11390] Loss: 2.9040\n",
            "[Step 11400] Loss: 2.2099\n",
            "[Step 11410] Loss: 2.4640\n",
            "[Step 11420] Loss: 1.7992\n",
            "[Step 11430] Loss: 2.2094\n",
            "[Step 11440] Loss: 1.7661\n",
            "[Step 11450] Loss: 2.2029\n",
            "[Step 11460] Loss: 2.3917\n",
            "[Step 11470] Loss: 2.5702\n",
            "[Step 11480] Loss: 2.5442\n",
            "[Step 11490] Loss: 2.7558\n",
            "[Step 11500] Loss: 2.2614\n",
            "[Step 11510] Loss: 2.0921\n",
            "[Step 11520] Loss: 2.2706\n",
            "[Step 11530] Loss: 2.6404\n",
            "[Step 11540] Loss: 1.9142\n",
            "[Step 11550] Loss: 1.6214\n",
            "[Step 11560] Loss: 2.2306\n",
            "[Step 11570] Loss: 2.0884\n",
            "[Step 11580] Loss: 2.7896\n",
            "[Step 11590] Loss: 2.8211\n",
            "[Step 11600] Loss: 2.6188\n",
            "[Step 11610] Loss: 2.0707\n",
            "[Step 11620] Loss: 2.6304\n",
            "[Step 11630] Loss: 1.8981\n",
            "[Step 11640] Loss: 2.3111\n",
            "[Step 11650] Loss: 2.1582\n",
            "[Step 11660] Loss: 2.1001\n",
            "[Step 11670] Loss: 2.4395\n",
            "[Step 11680] Loss: 2.2362\n",
            "[Step 11690] Loss: 2.6298\n",
            "[Step 11700] Loss: 2.6397\n",
            "[Step 11710] Loss: 2.5437\n",
            "[Step 11720] Loss: 2.4067\n",
            "[Step 11730] Loss: 2.0976\n",
            "[Step 11740] Loss: 1.9720\n",
            "[Step 11750] Loss: 2.6115\n",
            "[Step 11760] Loss: 2.0424\n",
            "[Step 11770] Loss: 2.0010\n",
            "[Step 11780] Loss: 2.1355\n",
            "[Step 11790] Loss: 2.0450\n",
            "[Step 11800] Loss: 2.1443\n",
            "[Step 11810] Loss: 2.5536\n",
            "[Step 11820] Loss: 2.4283\n",
            "[Step 11830] Loss: 1.8999\n",
            "[Step 11840] Loss: 2.0927\n",
            "[Step 11850] Loss: 2.4431\n",
            "[Step 11860] Loss: 2.1045\n",
            "[Step 11870] Loss: 2.4354\n",
            "[Step 11880] Loss: 2.2545\n",
            "[Step 11890] Loss: 1.9217\n",
            "[Step 11900] Loss: 2.4344\n",
            "[Step 11910] Loss: 2.2065\n",
            "[Step 11920] Loss: 2.4164\n",
            "[Step 11930] Loss: 2.4033\n",
            "[Step 11940] Loss: 2.1601\n",
            "[Step 11950] Loss: 2.2442\n",
            "[Step 11960] Loss: 2.9320\n",
            "[Step 11970] Loss: 2.5131\n",
            "[Step 11980] Loss: 2.4345\n",
            "[Step 11990] Loss: 2.4044\n",
            "[Step 12000] Loss: 2.2480\n",
            "[Step 12010] Loss: 2.3602\n",
            "[Step 12020] Loss: 2.1794\n",
            "[Step 12030] Loss: 2.4119\n",
            "[Step 12040] Loss: 2.1949\n",
            "[Step 12050] Loss: 2.2576\n",
            "[Step 12060] Loss: 2.1412\n",
            "[Step 12070] Loss: 2.2507\n",
            "[Step 12080] Loss: 1.9943\n",
            "[Step 12090] Loss: 2.1943\n",
            "[Step 12100] Loss: 2.5252\n",
            "[Step 12110] Loss: 2.9011\n",
            "[Step 12120] Loss: 2.2439\n",
            "[Step 12130] Loss: 2.2325\n",
            "[Step 12140] Loss: 2.1659\n",
            "[Step 12150] Loss: 2.4435\n",
            "[Step 12160] Loss: 2.6799\n",
            "[Step 12170] Loss: 2.1034\n",
            "[Step 12180] Loss: 2.2798\n",
            "[Step 12190] Loss: 1.9323\n",
            "[Step 12200] Loss: 2.0000\n",
            "[Step 12210] Loss: 1.8595\n",
            "[Step 12220] Loss: 1.8864\n",
            "[Step 12230] Loss: 2.5157\n",
            "[Step 12240] Loss: 2.1890\n",
            "[Step 12250] Loss: 2.5332\n",
            "[Step 12260] Loss: 2.0462\n",
            "[Step 12270] Loss: 1.8286\n",
            "[Step 12280] Loss: 2.1932\n",
            "[Step 12290] Loss: 2.4661\n",
            "[Step 12300] Loss: 2.2897\n",
            "[Step 12310] Loss: 2.1175\n",
            "[Step 12320] Loss: 2.6951\n",
            "[Step 12330] Loss: 2.1203\n",
            "[Step 12340] Loss: 2.2106\n",
            "[Step 12350] Loss: 2.1761\n",
            "[Step 12360] Loss: 2.3940\n",
            "[Step 12370] Loss: 2.3083\n",
            "[Step 12380] Loss: 2.3598\n",
            "[Step 12390] Loss: 2.1878\n",
            "[Step 12400] Loss: 2.2592\n",
            "[Step 12410] Loss: 2.6078\n",
            "[Step 12420] Loss: 2.3706\n",
            "[Step 12430] Loss: 1.8738\n",
            "[Step 12440] Loss: 2.3480\n",
            "[Step 12450] Loss: 1.9012\n",
            "[Step 12460] Loss: 2.6731\n",
            "[Step 12470] Loss: 1.5438\n",
            "[Step 12480] Loss: 2.5066\n",
            "[Step 12490] Loss: 2.4060\n",
            "[Step 12500] Loss: 2.6509\n",
            "[Step 12510] Loss: 1.7789\n",
            "[Step 12520] Loss: 2.0333\n",
            "[Step 12530] Loss: 2.1212\n",
            "[Step 12540] Loss: 1.9270\n",
            "[Step 12550] Loss: 2.6876\n",
            "[Step 12560] Loss: 2.6448\n",
            "[Step 12570] Loss: 2.3276\n",
            "[Step 12580] Loss: 2.2779\n",
            "[Step 12590] Loss: 2.6478\n",
            "[Step 12600] Loss: 2.3426\n",
            "[Step 12610] Loss: 2.6782\n",
            "[Step 12620] Loss: 2.1465\n",
            "[Step 12630] Loss: 2.0814\n",
            "[Step 12640] Loss: 2.6720\n",
            "[Step 12650] Loss: 1.9079\n",
            "[Step 12660] Loss: 2.9409\n",
            "[Step 12670] Loss: 2.6229\n",
            "[Step 12680] Loss: 2.6380\n",
            "[Step 12690] Loss: 2.6024\n",
            "[Step 12700] Loss: 2.3919\n",
            "[Step 12710] Loss: 2.2534\n",
            "[Step 12720] Loss: 2.1950\n",
            "[Step 12730] Loss: 2.6469\n",
            "[Step 12740] Loss: 1.8233\n",
            "[Step 12750] Loss: 2.1719\n",
            "[Step 12760] Loss: 2.2558\n",
            "[Step 12770] Loss: 2.8033\n",
            "[Step 12780] Loss: 2.0932\n",
            "[Step 12790] Loss: 2.5598\n",
            "[Step 12800] Loss: 2.4057\n",
            "[Step 12810] Loss: 2.4376\n",
            "[Step 12820] Loss: 1.8960\n",
            "[Step 12830] Loss: 2.2053\n",
            "[Step 12840] Loss: 2.3713\n",
            "[Step 12850] Loss: 2.0804\n",
            "[Step 12860] Loss: 1.7871\n",
            "[Step 12870] Loss: 2.2982\n",
            "[Step 12880] Loss: 2.4553\n",
            "[Step 12890] Loss: 2.3432\n",
            "[Step 12900] Loss: 2.4764\n",
            "[Step 12910] Loss: 2.5541\n",
            "[Step 12920] Loss: 2.3198\n",
            "[Step 12930] Loss: 2.5286\n",
            "[Step 12940] Loss: 2.0735\n",
            "[Step 12950] Loss: 2.6589\n",
            "[Step 12960] Loss: 2.2402\n",
            "[Step 12970] Loss: 1.9812\n",
            "[Step 12980] Loss: 2.1938\n",
            "[Step 12990] Loss: 2.2040\n",
            "[Step 13000] Loss: 2.2902\n",
            "[Step 13010] Loss: 2.0025\n",
            "[Step 13020] Loss: 2.1551\n",
            "[Step 13030] Loss: 2.5249\n",
            "[Step 13040] Loss: 2.2808\n",
            "[Step 13050] Loss: 1.9974\n",
            "[Step 13060] Loss: 1.9751\n",
            "[Step 13070] Loss: 2.0294\n",
            "[Step 13080] Loss: 2.9279\n",
            "[Step 13090] Loss: 2.2510\n",
            "[Step 13100] Loss: 2.2368\n",
            "[Step 13110] Loss: 2.5311\n",
            "[Step 13120] Loss: 2.4975\n",
            "[Step 13130] Loss: 2.6895\n",
            "[Step 13140] Loss: 1.9539\n",
            "[Step 13150] Loss: 2.1634\n",
            "[Step 13160] Loss: 2.3859\n",
            "[Step 13170] Loss: 2.5161\n",
            "[Step 13180] Loss: 2.6446\n",
            "[Step 13190] Loss: 2.5382\n",
            "[Step 13200] Loss: 2.3437\n",
            "[Step 13210] Loss: 2.7139\n",
            "[Step 13220] Loss: 2.3125\n",
            "[Step 13230] Loss: 1.8746\n",
            "[Step 13240] Loss: 2.1945\n",
            "[Step 13250] Loss: 1.8806\n",
            "ðŸ“˜ Epoch 47 - Avg Training Loss: 2.2996\n",
            "ðŸ“Š Final Validation â€” Loss: 2.3441 | Accuracy: 0.4075 | Precision: 0.3982\n",
            "[Step 13260] Loss: 1.8885\n",
            "[Step 13270] Loss: 1.8958\n",
            "[Step 13280] Loss: 2.5755\n",
            "[Step 13290] Loss: 2.5326\n",
            "[Step 13300] Loss: 2.9648\n",
            "[Step 13310] Loss: 2.3384\n",
            "[Step 13320] Loss: 2.2392\n",
            "[Step 13330] Loss: 1.9975\n",
            "[Step 13340] Loss: 2.7437\n",
            "[Step 13350] Loss: 2.3689\n",
            "[Step 13360] Loss: 2.6372\n",
            "[Step 13370] Loss: 1.9969\n",
            "[Step 13380] Loss: 2.8230\n",
            "[Step 13390] Loss: 2.8681\n",
            "[Step 13400] Loss: 2.1295\n",
            "[Step 13410] Loss: 2.2747\n",
            "[Step 13420] Loss: 1.6964\n",
            "[Step 13430] Loss: 2.1752\n",
            "[Step 13440] Loss: 1.8070\n",
            "[Step 13450] Loss: 2.0978\n",
            "[Step 13460] Loss: 2.0780\n",
            "[Step 13470] Loss: 2.7309\n",
            "[Step 13480] Loss: 2.9817\n",
            "[Step 13490] Loss: 2.2487\n",
            "[Step 13500] Loss: 2.7458\n",
            "[Step 13510] Loss: 2.8647\n",
            "[Step 13520] Loss: 1.9443\n",
            "[Step 13530] Loss: 2.6498\n",
            "[Step 13540] Loss: 1.9912\n",
            "[Step 13550] Loss: 2.1937\n",
            "[Step 13560] Loss: 2.2570\n",
            "[Step 13570] Loss: 2.8235\n",
            "[Step 13580] Loss: 1.9117\n",
            "[Step 13590] Loss: 2.2817\n",
            "[Step 13600] Loss: 1.9878\n",
            "[Step 13610] Loss: 1.9674\n",
            "[Step 13620] Loss: 2.4858\n",
            "[Step 13630] Loss: 2.3273\n",
            "[Step 13640] Loss: 2.1883\n",
            "[Step 13650] Loss: 2.4636\n",
            "[Step 13660] Loss: 1.8879\n",
            "[Step 13670] Loss: 2.2788\n",
            "[Step 13680] Loss: 2.4232\n",
            "[Step 13690] Loss: 2.4236\n",
            "[Step 13700] Loss: 1.7974\n",
            "[Step 13710] Loss: 2.4853\n",
            "[Step 13720] Loss: 2.0234\n",
            "[Step 13730] Loss: 2.1071\n",
            "[Step 13740] Loss: 2.0360\n",
            "[Step 13750] Loss: 2.1671\n",
            "[Step 13760] Loss: 2.0273\n",
            "[Step 13770] Loss: 2.4633\n",
            "[Step 13780] Loss: 2.3249\n",
            "[Step 13790] Loss: 2.0409\n",
            "[Step 13800] Loss: 2.4996\n",
            "[Step 13810] Loss: 1.8527\n",
            "[Step 13820] Loss: 1.8858\n",
            "[Step 13830] Loss: 2.1330\n",
            "[Step 13840] Loss: 2.3743\n",
            "[Step 13850] Loss: 2.0834\n",
            "[Step 13860] Loss: 2.5852\n",
            "[Step 13870] Loss: 2.4436\n",
            "[Step 13880] Loss: 2.4993\n",
            "[Step 13890] Loss: 1.8842\n",
            "[Step 13900] Loss: 2.7809\n",
            "[Step 13910] Loss: 2.1178\n",
            "[Step 13920] Loss: 1.9501\n",
            "[Step 13930] Loss: 2.7241\n",
            "[Step 13940] Loss: 2.4198\n",
            "[Step 13950] Loss: 2.6796\n",
            "[Step 13960] Loss: 2.3539\n",
            "[Step 13970] Loss: 2.5396\n",
            "[Step 13980] Loss: 1.5327\n",
            "[Step 13990] Loss: 2.4179\n",
            "[Step 14000] Loss: 2.4532\n",
            "[Step 14010] Loss: 2.4899\n",
            "[Step 14020] Loss: 1.6556\n",
            "[Step 14030] Loss: 1.9905\n",
            "[Step 14040] Loss: 2.3549\n",
            "[Step 14050] Loss: 2.2639\n",
            "[Step 14060] Loss: 1.9705\n",
            "[Step 14070] Loss: 1.8552\n",
            "[Step 14080] Loss: 2.4761\n",
            "[Step 14090] Loss: 1.6035\n",
            "[Step 14100] Loss: 2.4082\n",
            "[Step 14110] Loss: 2.4529\n",
            "[Step 14120] Loss: 2.4066\n",
            "[Step 14130] Loss: 1.9573\n",
            "[Step 14140] Loss: 1.8664\n",
            "[Step 14150] Loss: 2.1726\n",
            "[Step 14160] Loss: 2.1290\n",
            "[Step 14170] Loss: 2.2819\n",
            "[Step 14180] Loss: 2.9005\n",
            "[Step 14190] Loss: 2.3090\n",
            "[Step 14200] Loss: 2.4386\n",
            "[Step 14210] Loss: 2.3786\n",
            "[Step 14220] Loss: 2.2979\n",
            "[Step 14230] Loss: 2.0618\n",
            "[Step 14240] Loss: 2.7646\n",
            "[Step 14250] Loss: 2.2361\n",
            "[Step 14260] Loss: 1.9604\n",
            "[Step 14270] Loss: 1.8375\n",
            "[Step 14280] Loss: 2.0205\n",
            "[Step 14290] Loss: 2.3091\n",
            "[Step 14300] Loss: 1.9917\n",
            "[Step 14310] Loss: 1.9035\n",
            "[Step 14320] Loss: 2.0648\n",
            "[Step 14330] Loss: 2.4876\n",
            "[Step 14340] Loss: 2.5091\n",
            "[Step 14350] Loss: 2.3005\n",
            "[Step 14360] Loss: 2.6011\n",
            "[Step 14370] Loss: 2.1247\n",
            "[Step 14380] Loss: 1.8014\n",
            "[Step 14390] Loss: 2.1563\n",
            "[Step 14400] Loss: 2.2009\n",
            "[Step 14410] Loss: 1.9499\n",
            "[Step 14420] Loss: 1.8033\n",
            "[Step 14430] Loss: 2.0894\n",
            "[Step 14440] Loss: 2.3176\n",
            "[Step 14450] Loss: 2.5526\n",
            "[Step 14460] Loss: 2.5991\n",
            "[Step 14470] Loss: 1.9751\n",
            "[Step 14480] Loss: 2.2864\n",
            "[Step 14490] Loss: 2.1697\n",
            "[Step 14500] Loss: 2.4686\n",
            "[Step 14510] Loss: 2.4481\n",
            "[Step 14520] Loss: 2.1419\n",
            "[Step 14530] Loss: 2.4759\n",
            "[Step 14540] Loss: 2.3365\n",
            "[Step 14550] Loss: 2.1311\n",
            "[Step 14560] Loss: 2.3694\n",
            "[Step 14570] Loss: 2.4009\n",
            "[Step 14580] Loss: 1.9691\n",
            "[Step 14590] Loss: 2.5804\n",
            "[Step 14600] Loss: 2.4033\n",
            "[Step 14610] Loss: 2.2497\n",
            "[Step 14620] Loss: 2.0215\n",
            "[Step 14630] Loss: 2.9694\n",
            "[Step 14640] Loss: 2.0552\n",
            "[Step 14650] Loss: 1.9489\n",
            "[Step 14660] Loss: 1.8282\n",
            "[Step 14670] Loss: 2.3108\n",
            "[Step 14680] Loss: 2.6272\n",
            "[Step 14690] Loss: 2.1233\n",
            "[Step 14700] Loss: 1.9604\n",
            "[Step 14710] Loss: 2.6793\n",
            "[Step 14720] Loss: 2.1677\n",
            "[Step 14730] Loss: 2.0647\n",
            "[Step 14740] Loss: 2.2162\n",
            "[Step 14750] Loss: 2.2725\n",
            "[Step 14760] Loss: 2.2388\n",
            "[Step 14770] Loss: 1.9943\n",
            "[Step 14780] Loss: 2.3479\n",
            "[Step 14790] Loss: 2.5744\n",
            "[Step 14800] Loss: 2.3505\n",
            "[Step 14810] Loss: 2.2513\n",
            "[Step 14820] Loss: 2.5540\n",
            "[Step 14830] Loss: 2.3138\n",
            "[Step 14840] Loss: 2.3812\n",
            "[Step 14850] Loss: 2.5385\n",
            "[Step 14860] Loss: 2.4381\n",
            "[Step 14870] Loss: 2.1352\n",
            "[Step 14880] Loss: 2.5235\n",
            "[Step 14890] Loss: 2.5811\n",
            "[Step 14900] Loss: 3.1484\n",
            "[Step 14910] Loss: 1.9894\n",
            "[Step 14920] Loss: 1.7673\n",
            "[Step 14930] Loss: 2.2073\n",
            "[Step 14940] Loss: 1.9409\n",
            "[Step 14950] Loss: 2.5632\n",
            "[Step 14960] Loss: 2.1777\n",
            "[Step 14970] Loss: 2.3149\n",
            "[Step 14980] Loss: 2.5392\n",
            "[Step 14990] Loss: 2.1994\n",
            "[Step 15000] Loss: 2.4755\n",
            "[Step 15010] Loss: 2.7863\n",
            "[Step 15020] Loss: 3.1496\n",
            "[Step 15030] Loss: 1.9238\n",
            "[Step 15040] Loss: 2.0301\n",
            "[Step 15050] Loss: 2.5547\n",
            "[Step 15060] Loss: 2.9914\n",
            "[Step 15070] Loss: 1.9680\n",
            "[Step 15080] Loss: 2.0245\n",
            "[Step 15090] Loss: 1.5973\n",
            "[Step 15100] Loss: 2.2703\n",
            "[Step 15110] Loss: 2.2277\n",
            "[Step 15120] Loss: 2.5062\n",
            "[Step 15130] Loss: 2.2295\n",
            "[Step 15140] Loss: 2.1326\n",
            "[Step 15150] Loss: 2.1425\n",
            "ðŸ“˜ Epoch 48 - Avg Training Loss: 2.2598\n",
            "ðŸ“Š Final Validation â€” Loss: 2.3065 | Accuracy: 0.4117 | Precision: 0.4010\n",
            "[Step 15160] Loss: 2.1771\n",
            "[Step 15170] Loss: 2.2776\n",
            "[Step 15180] Loss: 2.3927\n",
            "[Step 15190] Loss: 1.7100\n",
            "[Step 15200] Loss: 2.5475\n",
            "[Step 15210] Loss: 2.1464\n",
            "[Step 15220] Loss: 1.9730\n",
            "[Step 15230] Loss: 2.2186\n",
            "[Step 15240] Loss: 1.8410\n",
            "[Step 15250] Loss: 2.1702\n",
            "[Step 15260] Loss: 2.2124\n",
            "[Step 15270] Loss: 1.9709\n",
            "[Step 15280] Loss: 2.3173\n",
            "[Step 15290] Loss: 3.2408\n",
            "[Step 15300] Loss: 1.7260\n",
            "[Step 15310] Loss: 2.4224\n",
            "[Step 15320] Loss: 2.4253\n",
            "[Step 15330] Loss: 1.9701\n",
            "[Step 15340] Loss: 2.4481\n",
            "[Step 15350] Loss: 2.4773\n",
            "[Step 15360] Loss: 2.2676\n",
            "[Step 15370] Loss: 1.7381\n",
            "[Step 15380] Loss: 1.8696\n",
            "[Step 15390] Loss: 2.3933\n",
            "[Step 15400] Loss: 2.2708\n",
            "[Step 15410] Loss: 2.0209\n",
            "[Step 15420] Loss: 1.9984\n",
            "[Step 15430] Loss: 2.0133\n",
            "[Step 15440] Loss: 1.8598\n",
            "[Step 15450] Loss: 1.9299\n",
            "[Step 15460] Loss: 2.1796\n",
            "[Step 15470] Loss: 2.1920\n",
            "[Step 15480] Loss: 3.0684\n",
            "[Step 15490] Loss: 1.8810\n",
            "[Step 15500] Loss: 3.1313\n",
            "[Step 15510] Loss: 2.1029\n",
            "[Step 15520] Loss: 2.4814\n",
            "[Step 15530] Loss: 3.0415\n",
            "[Step 15540] Loss: 1.9115\n",
            "[Step 15550] Loss: 2.2789\n",
            "[Step 15560] Loss: 1.7705\n",
            "[Step 15570] Loss: 2.0911\n",
            "[Step 15580] Loss: 2.1723\n",
            "[Step 15590] Loss: 2.6872\n",
            "[Step 15600] Loss: 2.5452\n",
            "[Step 15610] Loss: 2.0476\n",
            "[Step 15620] Loss: 2.1576\n",
            "[Step 15630] Loss: 2.2048\n",
            "[Step 15640] Loss: 1.6263\n",
            "[Step 15650] Loss: 2.3241\n",
            "[Step 15660] Loss: 2.3487\n",
            "[Step 15670] Loss: 2.5495\n",
            "[Step 15680] Loss: 1.8145\n",
            "[Step 15690] Loss: 1.5693\n",
            "[Step 15700] Loss: 2.3605\n",
            "[Step 15710] Loss: 2.0718\n",
            "[Step 15720] Loss: 2.3383\n",
            "[Step 15730] Loss: 1.8599\n",
            "[Step 15740] Loss: 2.1793\n",
            "[Step 15750] Loss: 2.0852\n",
            "[Step 15760] Loss: 2.8534\n",
            "[Step 15770] Loss: 2.5077\n",
            "[Step 15780] Loss: 2.4486\n",
            "[Step 15790] Loss: 2.5224\n",
            "[Step 15800] Loss: 1.6341\n",
            "[Step 15810] Loss: 2.2020\n",
            "[Step 15820] Loss: 1.8780\n",
            "[Step 15830] Loss: 2.0600\n",
            "[Step 15840] Loss: 1.9144\n",
            "[Step 15850] Loss: 2.5623\n",
            "[Step 15860] Loss: 2.3089\n",
            "[Step 15870] Loss: 2.5744\n",
            "[Step 15880] Loss: 2.1360\n",
            "[Step 15890] Loss: 2.4098\n",
            "[Step 15900] Loss: 2.4462\n",
            "[Step 15910] Loss: 2.3803\n",
            "[Step 15920] Loss: 1.9126\n",
            "[Step 15930] Loss: 2.9411\n",
            "[Step 15940] Loss: 2.3332\n",
            "[Step 15950] Loss: 2.6368\n",
            "[Step 15960] Loss: 2.1880\n",
            "[Step 15970] Loss: 2.2371\n",
            "[Step 15980] Loss: 2.5650\n",
            "[Step 15990] Loss: 2.1300\n",
            "[Step 16000] Loss: 2.2401\n",
            "[Step 16010] Loss: 2.0512\n",
            "[Step 16020] Loss: 2.4573\n",
            "[Step 16030] Loss: 2.2014\n",
            "[Step 16040] Loss: 2.5364\n",
            "[Step 16050] Loss: 2.1762\n",
            "[Step 16060] Loss: 2.1663\n",
            "[Step 16070] Loss: 2.3614\n",
            "[Step 16080] Loss: 2.6234\n",
            "[Step 16090] Loss: 1.6332\n",
            "[Step 16100] Loss: 1.7708\n",
            "[Step 16110] Loss: 1.8544\n",
            "[Step 16120] Loss: 2.2025\n",
            "[Step 16130] Loss: 2.2466\n",
            "[Step 16140] Loss: 2.2496\n",
            "[Step 16150] Loss: 2.4077\n",
            "[Step 16160] Loss: 1.9317\n",
            "[Step 16170] Loss: 2.5454\n",
            "[Step 16180] Loss: 1.8924\n",
            "[Step 16190] Loss: 2.1853\n",
            "[Step 16200] Loss: 2.4772\n",
            "[Step 16210] Loss: 2.1152\n",
            "[Step 16220] Loss: 2.2764\n",
            "[Step 16230] Loss: 2.3437\n",
            "[Step 16240] Loss: 2.8671\n",
            "[Step 16250] Loss: 1.9019\n",
            "[Step 16260] Loss: 2.0850\n",
            "[Step 16270] Loss: 1.9294\n",
            "[Step 16280] Loss: 2.6226\n",
            "[Step 16290] Loss: 2.7513\n",
            "[Step 16300] Loss: 2.1677\n",
            "[Step 16310] Loss: 1.9890\n",
            "[Step 16320] Loss: 2.0605\n",
            "[Step 16330] Loss: 2.5532\n",
            "[Step 16340] Loss: 2.1840\n",
            "[Step 16350] Loss: 2.2464\n",
            "[Step 16360] Loss: 2.2716\n",
            "[Step 16370] Loss: 2.5870\n",
            "[Step 16380] Loss: 2.3399\n",
            "[Step 16390] Loss: 2.0232\n",
            "[Step 16400] Loss: 2.3881\n",
            "[Step 16410] Loss: 1.9190\n",
            "[Step 16420] Loss: 2.0397\n",
            "[Step 16430] Loss: 2.5000\n",
            "[Step 16440] Loss: 2.0826\n",
            "[Step 16450] Loss: 2.2031\n",
            "[Step 16460] Loss: 2.8632\n",
            "[Step 16470] Loss: 3.0152\n",
            "[Step 16480] Loss: 1.7137\n",
            "[Step 16490] Loss: 2.2180\n",
            "[Step 16500] Loss: 1.9836\n",
            "[Step 16510] Loss: 2.0450\n",
            "[Step 16520] Loss: 2.3852\n",
            "[Step 16530] Loss: 1.9101\n",
            "[Step 16540] Loss: 2.3715\n",
            "[Step 16550] Loss: 2.1232\n",
            "[Step 16560] Loss: 2.3113\n",
            "[Step 16570] Loss: 2.3172\n",
            "[Step 16580] Loss: 2.3970\n",
            "[Step 16590] Loss: 1.7020\n",
            "[Step 16600] Loss: 2.5680\n",
            "[Step 16610] Loss: 1.9934\n",
            "[Step 16620] Loss: 2.2113\n",
            "[Step 16630] Loss: 2.1645\n",
            "[Step 16640] Loss: 2.3193\n",
            "[Step 16650] Loss: 2.4986\n",
            "[Step 16660] Loss: 2.4772\n",
            "[Step 16670] Loss: 2.1058\n",
            "[Step 16680] Loss: 2.7637\n",
            "[Step 16690] Loss: 1.7857\n",
            "[Step 16700] Loss: 2.1180\n",
            "[Step 16710] Loss: 2.9257\n",
            "[Step 16720] Loss: 2.0677\n",
            "[Step 16730] Loss: 2.7578\n",
            "[Step 16740] Loss: 2.7054\n",
            "[Step 16750] Loss: 2.1233\n",
            "[Step 16760] Loss: 2.5068\n",
            "[Step 16770] Loss: 2.3341\n",
            "[Step 16780] Loss: 2.4628\n",
            "[Step 16790] Loss: 2.6073\n",
            "[Step 16800] Loss: 2.1088\n",
            "[Step 16810] Loss: 2.3589\n",
            "[Step 16820] Loss: 2.3040\n",
            "[Step 16830] Loss: 2.3641\n",
            "[Step 16840] Loss: 1.8889\n",
            "[Step 16850] Loss: 2.2125\n",
            "[Step 16860] Loss: 1.7661\n",
            "[Step 16870] Loss: 2.6262\n",
            "[Step 16880] Loss: 2.0142\n",
            "[Step 16890] Loss: 2.9611\n",
            "[Step 16900] Loss: 2.4577\n",
            "[Step 16910] Loss: 2.1352\n",
            "[Step 16920] Loss: 2.1532\n",
            "[Step 16930] Loss: 2.6364\n",
            "[Step 16940] Loss: 2.5232\n",
            "[Step 16950] Loss: 2.3180\n",
            "[Step 16960] Loss: 1.8372\n",
            "[Step 16970] Loss: 1.9239\n",
            "[Step 16980] Loss: 1.7603\n",
            "[Step 16990] Loss: 2.3693\n",
            "[Step 17000] Loss: 2.0321\n",
            "[Step 17010] Loss: 2.1845\n",
            "[Step 17020] Loss: 2.2197\n",
            "[Step 17030] Loss: 2.3558\n",
            "[Step 17040] Loss: 1.9387\n",
            "ðŸ“˜ Epoch 49 - Avg Training Loss: 2.2291\n",
            "ðŸ“Š Final Validation â€” Loss: 2.2887 | Accuracy: 0.4200 | Precision: 0.4083\n",
            "[Step 17050] Loss: 2.0282\n",
            "[Step 17060] Loss: 2.4325\n",
            "[Step 17070] Loss: 2.4351\n",
            "[Step 17080] Loss: 1.8788\n",
            "[Step 17090] Loss: 2.2117\n",
            "[Step 17100] Loss: 2.1386\n",
            "[Step 17110] Loss: 2.6264\n",
            "[Step 17120] Loss: 2.5136\n",
            "[Step 17130] Loss: 1.8600\n",
            "[Step 17140] Loss: 2.2748\n",
            "[Step 17150] Loss: 2.5379\n",
            "[Step 17160] Loss: 2.2017\n",
            "[Step 17170] Loss: 1.9770\n",
            "[Step 17180] Loss: 2.2199\n",
            "[Step 17190] Loss: 2.0650\n",
            "[Step 17200] Loss: 2.4367\n",
            "[Step 17210] Loss: 2.1778\n",
            "[Step 17220] Loss: 2.5742\n",
            "[Step 17230] Loss: 2.0417\n",
            "[Step 17240] Loss: 2.0822\n",
            "[Step 17250] Loss: 2.2476\n",
            "[Step 17260] Loss: 2.1530\n",
            "[Step 17270] Loss: 2.2861\n",
            "[Step 17280] Loss: 1.7874\n",
            "[Step 17290] Loss: 2.0921\n",
            "[Step 17300] Loss: 2.1766\n",
            "[Step 17310] Loss: 2.5088\n",
            "[Step 17320] Loss: 2.0942\n",
            "[Step 17330] Loss: 2.2827\n",
            "[Step 17340] Loss: 2.4575\n",
            "[Step 17350] Loss: 2.3254\n",
            "[Step 17360] Loss: 2.3173\n",
            "[Step 17370] Loss: 2.1621\n",
            "[Step 17380] Loss: 2.1496\n",
            "[Step 17390] Loss: 2.2755\n",
            "[Step 17400] Loss: 2.0738\n",
            "[Step 17410] Loss: 1.5082\n",
            "[Step 17420] Loss: 2.4074\n",
            "[Step 17430] Loss: 2.0911\n",
            "[Step 17440] Loss: 1.9884\n",
            "[Step 17450] Loss: 2.0015\n",
            "[Step 17460] Loss: 2.1667\n",
            "[Step 17470] Loss: 2.1709\n",
            "[Step 17480] Loss: 2.0224\n",
            "[Step 17490] Loss: 1.7900\n",
            "[Step 17500] Loss: 2.1014\n",
            "[Step 17510] Loss: 2.1851\n",
            "[Step 17520] Loss: 2.4647\n",
            "[Step 17530] Loss: 2.1570\n",
            "[Step 17540] Loss: 2.2711\n",
            "[Step 17550] Loss: 1.9927\n",
            "[Step 17560] Loss: 2.4183\n",
            "[Step 17570] Loss: 2.7255\n",
            "[Step 17580] Loss: 2.2126\n",
            "[Step 17590] Loss: 2.1079\n",
            "[Step 17600] Loss: 2.4043\n",
            "[Step 17610] Loss: 2.3363\n",
            "[Step 17620] Loss: 2.4853\n",
            "[Step 17630] Loss: 2.6243\n",
            "[Step 17640] Loss: 2.0051\n",
            "[Step 17650] Loss: 1.6572\n",
            "[Step 17660] Loss: 2.7397\n",
            "[Step 17670] Loss: 2.3006\n",
            "[Step 17680] Loss: 1.8762\n",
            "[Step 17690] Loss: 2.3217\n",
            "[Step 17700] Loss: 2.0423\n",
            "[Step 17710] Loss: 2.2331\n",
            "[Step 17720] Loss: 2.1916\n",
            "[Step 17730] Loss: 2.1535\n",
            "[Step 17740] Loss: 1.8260\n",
            "[Step 17750] Loss: 2.2964\n",
            "[Step 17760] Loss: 2.1989\n",
            "[Step 17770] Loss: 2.3357\n",
            "[Step 17780] Loss: 2.0913\n",
            "[Step 17790] Loss: 2.0320\n",
            "[Step 17800] Loss: 1.9514\n",
            "[Step 17810] Loss: 2.0369\n",
            "[Step 17820] Loss: 1.7061\n",
            "[Step 17830] Loss: 2.2731\n",
            "[Step 17840] Loss: 1.9432\n",
            "[Step 17850] Loss: 2.0581\n",
            "[Step 17860] Loss: 2.3483\n",
            "[Step 17870] Loss: 2.1683\n",
            "[Step 17880] Loss: 2.4043\n",
            "[Step 17890] Loss: 2.4517\n",
            "[Step 17900] Loss: 2.3123\n",
            "[Step 17910] Loss: 1.9228\n",
            "[Step 17920] Loss: 2.5047\n",
            "[Step 17930] Loss: 2.4324\n",
            "[Step 17940] Loss: 2.3125\n",
            "[Step 17950] Loss: 2.6484\n",
            "[Step 17960] Loss: 1.7700\n",
            "[Step 17970] Loss: 2.3132\n",
            "[Step 17980] Loss: 2.4108\n",
            "[Step 17990] Loss: 2.0771\n",
            "[Step 18000] Loss: 1.8946\n",
            "[Step 18010] Loss: 1.7922\n",
            "[Step 18020] Loss: 1.7908\n",
            "[Step 18030] Loss: 2.1704\n",
            "[Step 18040] Loss: 2.1490\n",
            "[Step 18050] Loss: 2.1584\n",
            "[Step 18060] Loss: 2.2163\n",
            "[Step 18070] Loss: 2.3145\n",
            "[Step 18080] Loss: 1.8501\n",
            "[Step 18090] Loss: 2.3231\n",
            "[Step 18100] Loss: 2.2891\n",
            "[Step 18110] Loss: 1.8527\n",
            "[Step 18120] Loss: 2.5816\n",
            "[Step 18130] Loss: 2.3264\n",
            "[Step 18140] Loss: 2.6335\n",
            "[Step 18150] Loss: 2.3425\n",
            "[Step 18160] Loss: 2.4845\n",
            "[Step 18170] Loss: 2.2685\n",
            "[Step 18180] Loss: 2.3070\n",
            "[Step 18190] Loss: 2.0421\n",
            "[Step 18200] Loss: 2.5192\n",
            "[Step 18210] Loss: 1.9125\n",
            "[Step 18220] Loss: 2.3843\n",
            "[Step 18230] Loss: 2.4719\n",
            "[Step 18240] Loss: 2.1814\n",
            "[Step 18250] Loss: 2.1694\n",
            "[Step 18260] Loss: 1.9186\n",
            "[Step 18270] Loss: 1.9971\n",
            "[Step 18280] Loss: 2.1432\n",
            "[Step 18290] Loss: 2.1122\n",
            "[Step 18300] Loss: 2.1386\n",
            "[Step 18310] Loss: 1.7525\n",
            "[Step 18320] Loss: 1.9197\n",
            "[Step 18330] Loss: 2.3652\n",
            "[Step 18340] Loss: 2.4302\n",
            "[Step 18350] Loss: 2.8566\n",
            "[Step 18360] Loss: 1.8360\n",
            "[Step 18370] Loss: 1.8105\n",
            "[Step 18380] Loss: 2.1308\n",
            "[Step 18390] Loss: 2.2631\n",
            "[Step 18400] Loss: 1.7024\n",
            "[Step 18410] Loss: 1.9237\n",
            "[Step 18420] Loss: 2.5974\n",
            "[Step 18430] Loss: 2.4155\n",
            "[Step 18440] Loss: 2.0970\n",
            "[Step 18450] Loss: 2.8742\n",
            "[Step 18460] Loss: 2.5169\n",
            "[Step 18470] Loss: 2.6231\n",
            "[Step 18480] Loss: 2.3636\n",
            "[Step 18490] Loss: 1.7941\n",
            "[Step 18500] Loss: 2.1982\n",
            "[Step 18510] Loss: 2.3252\n",
            "[Step 18520] Loss: 2.1582\n",
            "[Step 18530] Loss: 2.1069\n",
            "[Step 18540] Loss: 1.9710\n",
            "[Step 18550] Loss: 2.6060\n",
            "[Step 18560] Loss: 2.1992\n",
            "[Step 18570] Loss: 2.8683\n",
            "[Step 18580] Loss: 2.1646\n",
            "[Step 18590] Loss: 2.4586\n",
            "[Step 18600] Loss: 2.9016\n",
            "[Step 18610] Loss: 2.1225\n",
            "[Step 18620] Loss: 1.8197\n",
            "[Step 18630] Loss: 1.8838\n",
            "[Step 18640] Loss: 2.3004\n",
            "[Step 18650] Loss: 2.2525\n",
            "[Step 18660] Loss: 2.5122\n",
            "[Step 18670] Loss: 2.3910\n",
            "[Step 18680] Loss: 2.1161\n",
            "[Step 18690] Loss: 2.5516\n",
            "[Step 18700] Loss: 2.1691\n",
            "[Step 18710] Loss: 2.4722\n",
            "[Step 18720] Loss: 2.0629\n",
            "[Step 18730] Loss: 2.0367\n",
            "[Step 18740] Loss: 1.8252\n",
            "[Step 18750] Loss: 2.1949\n",
            "[Step 18760] Loss: 2.1986\n",
            "[Step 18770] Loss: 2.0723\n",
            "[Step 18780] Loss: 1.7954\n",
            "[Step 18790] Loss: 2.4558\n",
            "[Step 18800] Loss: 2.3985\n",
            "[Step 18810] Loss: 1.9483\n",
            "[Step 18820] Loss: 2.3639\n",
            "[Step 18830] Loss: 2.0285\n",
            "[Step 18840] Loss: 2.5742\n",
            "[Step 18850] Loss: 1.9489\n",
            "[Step 18860] Loss: 1.9182\n",
            "[Step 18870] Loss: 2.1685\n",
            "[Step 18880] Loss: 2.5451\n",
            "[Step 18890] Loss: 2.1740\n",
            "[Step 18900] Loss: 2.0868\n",
            "[Step 18910] Loss: 2.4655\n",
            "[Step 18920] Loss: 2.0776\n",
            "[Step 18930] Loss: 2.2831\n",
            "[Step 18940] Loss: 2.2795\n",
            "ðŸ“˜ Epoch 50 - Avg Training Loss: 2.2058\n",
            "ðŸ“Š Final Validation â€” Loss: 2.2824 | Accuracy: 0.4191 | Precision: 0.4053\n",
            "[Step 18950] Loss: 2.2787\n",
            "[Step 18960] Loss: 2.1126\n",
            "[Step 18970] Loss: 2.0256\n",
            "[Step 18980] Loss: 2.3422\n",
            "[Step 18990] Loss: 2.0894\n",
            "[Step 19000] Loss: 2.7888\n",
            "[Step 19010] Loss: 2.0595\n",
            "[Step 19020] Loss: 2.3219\n",
            "[Step 19030] Loss: 2.3730\n",
            "[Step 19040] Loss: 2.1804\n",
            "[Step 19050] Loss: 2.1767\n",
            "[Step 19060] Loss: 2.0308\n",
            "[Step 19070] Loss: 2.1886\n",
            "[Step 19080] Loss: 2.4247\n",
            "[Step 19090] Loss: 2.1153\n",
            "[Step 19100] Loss: 1.9168\n",
            "[Step 19110] Loss: 2.0596\n",
            "[Step 19120] Loss: 1.8273\n",
            "[Step 19130] Loss: 2.3801\n",
            "[Step 19140] Loss: 2.4545\n",
            "[Step 19150] Loss: 2.1901\n",
            "[Step 19160] Loss: 2.3675\n",
            "[Step 19170] Loss: 2.1288\n",
            "[Step 19180] Loss: 2.3432\n",
            "[Step 19190] Loss: 2.4748\n",
            "[Step 19200] Loss: 1.5418\n",
            "[Step 19210] Loss: 2.0349\n",
            "[Step 19220] Loss: 2.7647\n",
            "[Step 19230] Loss: 1.6380\n",
            "[Step 19240] Loss: 2.2084\n",
            "[Step 19250] Loss: 1.8825\n",
            "[Step 19260] Loss: 1.3459\n",
            "[Step 19270] Loss: 1.9772\n",
            "[Step 19280] Loss: 2.0597\n",
            "[Step 19290] Loss: 2.1397\n",
            "[Step 19300] Loss: 2.2863\n",
            "[Step 19310] Loss: 2.6107\n",
            "[Step 19320] Loss: 2.5705\n",
            "[Step 19330] Loss: 2.6334\n",
            "[Step 19340] Loss: 2.3186\n",
            "[Step 19350] Loss: 1.8207\n",
            "[Step 19360] Loss: 1.8694\n",
            "[Step 19370] Loss: 2.4808\n",
            "[Step 19380] Loss: 1.9765\n",
            "[Step 19390] Loss: 2.0935\n",
            "[Step 19400] Loss: 1.7166\n",
            "[Step 19410] Loss: 1.8757\n",
            "[Step 19420] Loss: 2.3539\n",
            "[Step 19430] Loss: 2.2621\n",
            "[Step 19440] Loss: 1.6846\n",
            "[Step 19450] Loss: 2.1011\n",
            "[Step 19460] Loss: 2.2581\n",
            "[Step 19470] Loss: 2.7163\n",
            "[Step 19480] Loss: 2.1282\n",
            "[Step 19490] Loss: 2.1013\n",
            "[Step 19500] Loss: 1.8771\n",
            "[Step 19510] Loss: 2.5642\n",
            "[Step 19520] Loss: 1.7504\n",
            "[Step 19530] Loss: 2.0418\n",
            "[Step 19540] Loss: 2.8346\n",
            "[Step 19550] Loss: 2.0620\n",
            "[Step 19560] Loss: 2.1113\n",
            "[Step 19570] Loss: 2.3654\n",
            "[Step 19580] Loss: 2.3944\n",
            "[Step 19590] Loss: 1.9053\n",
            "[Step 19600] Loss: 2.2307\n",
            "[Step 19610] Loss: 1.8217\n",
            "[Step 19620] Loss: 2.2833\n",
            "[Step 19630] Loss: 2.0773\n",
            "[Step 19640] Loss: 2.3813\n",
            "[Step 19650] Loss: 1.9763\n",
            "[Step 19660] Loss: 2.2676\n",
            "[Step 19670] Loss: 2.3542\n",
            "[Step 19680] Loss: 1.9159\n",
            "[Step 19690] Loss: 2.1972\n",
            "[Step 19700] Loss: 2.6942\n",
            "[Step 19710] Loss: 1.7770\n",
            "[Step 19720] Loss: 2.3042\n",
            "[Step 19730] Loss: 2.0939\n",
            "[Step 19740] Loss: 1.8613\n",
            "[Step 19750] Loss: 1.9146\n",
            "[Step 19760] Loss: 1.9600\n",
            "[Step 19770] Loss: 2.1767\n",
            "[Step 19780] Loss: 1.9665\n",
            "[Step 19790] Loss: 1.9233\n",
            "[Step 19800] Loss: 1.8670\n",
            "[Step 19810] Loss: 2.5011\n",
            "[Step 19820] Loss: 1.9421\n",
            "[Step 19830] Loss: 1.9102\n",
            "[Step 19840] Loss: 2.0015\n",
            "[Step 19850] Loss: 2.5308\n",
            "[Step 19860] Loss: 2.3819\n",
            "[Step 19870] Loss: 2.3235\n",
            "[Step 19880] Loss: 2.1360\n",
            "[Step 19890] Loss: 2.3188\n",
            "[Step 19900] Loss: 1.9568\n",
            "[Step 19910] Loss: 1.9270\n",
            "[Step 19920] Loss: 2.6368\n",
            "[Step 19930] Loss: 2.3742\n",
            "[Step 19940] Loss: 1.5545\n",
            "[Step 19950] Loss: 2.4400\n",
            "[Step 19960] Loss: 2.2242\n",
            "[Step 19970] Loss: 1.6377\n",
            "[Step 19980] Loss: 2.0403\n",
            "[Step 19990] Loss: 2.5157\n",
            "[Step 20000] Loss: 2.5212\n",
            "[Step 20010] Loss: 2.1861\n",
            "[Step 20020] Loss: 2.2153\n",
            "[Step 20030] Loss: 2.5004\n",
            "[Step 20040] Loss: 2.3690\n",
            "[Step 20050] Loss: 1.7375\n",
            "[Step 20060] Loss: 2.2936\n",
            "[Step 20070] Loss: 1.8053\n",
            "[Step 20080] Loss: 2.0799\n",
            "[Step 20090] Loss: 1.8663\n",
            "[Step 20100] Loss: 1.6287\n",
            "[Step 20110] Loss: 2.2381\n",
            "[Step 20120] Loss: 2.2664\n",
            "[Step 20130] Loss: 1.9131\n",
            "[Step 20140] Loss: 1.4763\n",
            "[Step 20150] Loss: 1.9613\n",
            "[Step 20160] Loss: 1.7738\n",
            "[Step 20170] Loss: 1.8243\n",
            "[Step 20180] Loss: 2.2247\n",
            "[Step 20190] Loss: 2.3401\n",
            "[Step 20200] Loss: 2.1666\n",
            "[Step 20210] Loss: 2.4285\n",
            "[Step 20220] Loss: 1.9965\n",
            "[Step 20230] Loss: 1.8511\n",
            "[Step 20240] Loss: 2.1088\n",
            "[Step 20250] Loss: 2.1674\n",
            "[Step 20260] Loss: 2.0349\n",
            "[Step 20270] Loss: 1.9834\n",
            "[Step 20280] Loss: 1.7011\n",
            "[Step 20290] Loss: 2.2832\n",
            "[Step 20300] Loss: 1.5932\n",
            "[Step 20310] Loss: 2.0278\n",
            "[Step 20320] Loss: 1.8624\n",
            "[Step 20330] Loss: 1.8616\n",
            "[Step 20340] Loss: 2.6821\n",
            "[Step 20350] Loss: 2.4796\n",
            "[Step 20360] Loss: 1.9860\n",
            "[Step 20370] Loss: 2.2155\n",
            "[Step 20380] Loss: 2.0645\n",
            "[Step 20390] Loss: 2.3540\n",
            "[Step 20400] Loss: 2.2755\n",
            "[Step 20410] Loss: 1.8569\n",
            "[Step 20420] Loss: 2.2493\n",
            "[Step 20430] Loss: 2.1658\n",
            "[Step 20440] Loss: 1.6961\n",
            "[Step 20450] Loss: 2.1086\n",
            "[Step 20460] Loss: 2.2942\n",
            "[Step 20470] Loss: 2.2528\n",
            "[Step 20480] Loss: 1.9681\n",
            "[Step 20490] Loss: 2.0133\n",
            "[Step 20500] Loss: 2.2438\n",
            "[Step 20510] Loss: 2.1487\n",
            "[Step 20520] Loss: 1.7678\n",
            "[Step 20530] Loss: 2.4628\n",
            "[Step 20540] Loss: 1.8561\n",
            "[Step 20550] Loss: 1.9717\n",
            "[Step 20560] Loss: 2.6643\n",
            "[Step 20570] Loss: 2.2048\n",
            "[Step 20580] Loss: 2.4654\n",
            "[Step 20590] Loss: 2.6726\n",
            "[Step 20600] Loss: 2.1735\n",
            "[Step 20610] Loss: 2.1267\n",
            "[Step 20620] Loss: 2.3453\n",
            "[Step 20630] Loss: 2.2677\n",
            "[Step 20640] Loss: 2.5430\n",
            "[Step 20650] Loss: 2.0919\n",
            "[Step 20660] Loss: 1.9458\n",
            "[Step 20670] Loss: 2.0660\n",
            "[Step 20680] Loss: 2.6399\n",
            "[Step 20690] Loss: 2.3522\n",
            "[Step 20700] Loss: 2.2568\n",
            "[Step 20710] Loss: 1.4653\n",
            "[Step 20720] Loss: 2.0519\n",
            "[Step 20730] Loss: 2.2770\n",
            "[Step 20740] Loss: 1.8923\n",
            "[Step 20750] Loss: 2.1300\n",
            "[Step 20760] Loss: 2.1262\n",
            "[Step 20770] Loss: 2.6338\n",
            "[Step 20780] Loss: 1.9920\n",
            "[Step 20790] Loss: 1.8128\n",
            "[Step 20800] Loss: 2.0199\n",
            "[Step 20810] Loss: 2.3906\n",
            "[Step 20820] Loss: 2.1269\n",
            "[Step 20830] Loss: 1.8327\n",
            "ðŸ“˜ Epoch 51 - Avg Training Loss: 2.1953\n",
            "ðŸ“Š Final Validation â€” Loss: 2.2833 | Accuracy: 0.4197 | Precision: 0.4099\n",
            "[Step 20840] Loss: 2.5750\n",
            "[Step 20850] Loss: 1.7547\n",
            "[Step 20860] Loss: 1.8935\n",
            "[Step 20870] Loss: 2.7931\n",
            "[Step 20880] Loss: 2.3508\n",
            "[Step 20890] Loss: 2.4279\n",
            "[Step 20900] Loss: 2.3029\n",
            "[Step 20910] Loss: 1.6718\n",
            "[Step 20920] Loss: 2.0711\n",
            "[Step 20930] Loss: 2.4499\n",
            "[Step 20940] Loss: 2.3720\n",
            "[Step 20950] Loss: 2.0551\n",
            "[Step 20960] Loss: 1.9477\n",
            "[Step 20970] Loss: 2.1607\n",
            "[Step 20980] Loss: 2.1601\n",
            "[Step 20990] Loss: 2.3815\n",
            "[Step 21000] Loss: 2.0068\n",
            "[Step 21010] Loss: 1.8812\n",
            "[Step 21020] Loss: 2.1031\n",
            "[Step 21030] Loss: 1.8097\n",
            "[Step 21040] Loss: 2.3146\n",
            "[Step 21050] Loss: 2.3621\n",
            "[Step 21060] Loss: 2.3621\n",
            "[Step 21070] Loss: 2.3636\n",
            "[Step 21080] Loss: 1.6711\n",
            "[Step 21090] Loss: 1.9326\n",
            "[Step 21100] Loss: 2.6082\n",
            "[Step 21110] Loss: 2.0097\n",
            "[Step 21120] Loss: 2.5651\n",
            "[Step 21130] Loss: 2.2973\n",
            "[Step 21140] Loss: 2.0194\n",
            "[Step 21150] Loss: 2.7431\n",
            "[Step 21160] Loss: 2.3341\n",
            "[Step 21170] Loss: 2.3785\n",
            "[Step 21180] Loss: 2.4738\n",
            "[Step 21190] Loss: 2.5679\n",
            "[Step 21200] Loss: 2.0898\n",
            "[Step 21210] Loss: 1.8132\n",
            "[Step 21220] Loss: 2.6358\n",
            "[Step 21230] Loss: 2.2557\n",
            "[Step 21240] Loss: 2.5103\n",
            "[Step 21250] Loss: 2.5462\n",
            "[Step 21260] Loss: 2.1222\n",
            "[Step 21270] Loss: 1.7735\n",
            "[Step 21280] Loss: 2.4459\n",
            "[Step 21290] Loss: 2.3506\n",
            "[Step 21300] Loss: 2.2583\n",
            "[Step 21310] Loss: 2.1358\n",
            "[Step 21320] Loss: 1.9920\n",
            "[Step 21330] Loss: 1.9121\n",
            "[Step 21340] Loss: 2.2161\n",
            "[Step 21350] Loss: 2.1022\n",
            "[Step 21360] Loss: 2.0075\n",
            "[Step 21370] Loss: 2.1426\n",
            "[Step 21380] Loss: 2.3562\n",
            "[Step 21390] Loss: 2.3955\n",
            "[Step 21400] Loss: 2.2019\n",
            "[Step 21410] Loss: 1.8188\n",
            "[Step 21420] Loss: 2.2935\n",
            "[Step 21430] Loss: 2.0185\n",
            "[Step 21440] Loss: 2.2661\n",
            "[Step 21450] Loss: 2.4033\n",
            "[Step 21460] Loss: 2.3682\n",
            "[Step 21470] Loss: 1.8198\n",
            "[Step 21480] Loss: 1.6446\n",
            "[Step 21490] Loss: 2.3594\n",
            "[Step 21500] Loss: 2.6376\n",
            "[Step 21510] Loss: 1.9159\n",
            "[Step 21520] Loss: 2.4947\n",
            "[Step 21530] Loss: 2.3319\n",
            "[Step 21540] Loss: 1.5080\n",
            "[Step 21550] Loss: 2.4092\n",
            "[Step 21560] Loss: 2.0222\n",
            "[Step 21570] Loss: 1.8422\n",
            "[Step 21580] Loss: 1.9239\n",
            "[Step 21590] Loss: 2.2841\n",
            "[Step 21600] Loss: 2.1890\n",
            "[Step 21610] Loss: 2.3104\n",
            "[Step 21620] Loss: 1.7642\n",
            "[Step 21630] Loss: 2.0595\n",
            "[Step 21640] Loss: 2.4530\n",
            "[Step 21650] Loss: 1.8272\n",
            "[Step 21660] Loss: 2.0877\n",
            "[Step 21670] Loss: 2.4152\n",
            "[Step 21680] Loss: 2.3421\n",
            "[Step 21690] Loss: 2.1445\n",
            "[Step 21700] Loss: 2.4306\n",
            "[Step 21710] Loss: 3.0155\n",
            "[Step 21720] Loss: 2.0665\n",
            "[Step 21730] Loss: 1.8750\n",
            "[Step 21740] Loss: 2.0960\n",
            "[Step 21750] Loss: 1.8112\n",
            "[Step 21760] Loss: 2.1669\n",
            "[Step 21770] Loss: 2.5903\n",
            "[Step 21780] Loss: 2.3388\n",
            "[Step 21790] Loss: 2.0090\n",
            "[Step 21800] Loss: 2.0997\n",
            "[Step 21810] Loss: 2.1057\n",
            "[Step 21820] Loss: 2.6279\n",
            "[Step 21830] Loss: 2.6218\n",
            "[Step 21840] Loss: 1.9038\n",
            "[Step 21850] Loss: 1.9474\n",
            "[Step 21860] Loss: 2.7415\n",
            "[Step 21870] Loss: 2.2793\n",
            "[Step 21880] Loss: 1.9529\n",
            "[Step 21890] Loss: 2.5645\n",
            "[Step 21900] Loss: 2.2191\n",
            "[Step 21910] Loss: 1.8694\n",
            "[Step 21920] Loss: 2.6749\n",
            "[Step 21930] Loss: 2.5111\n",
            "[Step 21940] Loss: 2.0260\n",
            "[Step 21950] Loss: 2.2411\n",
            "[Step 21960] Loss: 1.7867\n",
            "[Step 21970] Loss: 2.2542\n",
            "[Step 21980] Loss: 2.0501\n",
            "[Step 21990] Loss: 1.8244\n",
            "[Step 22000] Loss: 2.1940\n",
            "[Step 22010] Loss: 2.1561\n",
            "[Step 22020] Loss: 1.8884\n",
            "[Step 22030] Loss: 2.7801\n",
            "[Step 22040] Loss: 1.7420\n",
            "[Step 22050] Loss: 2.0647\n",
            "[Step 22060] Loss: 2.0753\n",
            "[Step 22070] Loss: 1.9483\n",
            "[Step 22080] Loss: 2.1154\n",
            "[Step 22090] Loss: 2.4708\n",
            "[Step 22100] Loss: 2.5639\n",
            "[Step 22110] Loss: 2.0136\n",
            "[Step 22120] Loss: 2.0019\n",
            "[Step 22130] Loss: 2.7514\n",
            "[Step 22140] Loss: 1.9385\n",
            "[Step 22150] Loss: 2.3332\n",
            "[Step 22160] Loss: 2.3424\n",
            "[Step 22170] Loss: 2.4650\n",
            "[Step 22180] Loss: 1.9112\n",
            "[Step 22190] Loss: 2.4965\n",
            "[Step 22200] Loss: 1.9545\n",
            "[Step 22210] Loss: 2.1323\n",
            "[Step 22220] Loss: 2.0530\n",
            "[Step 22230] Loss: 2.1073\n",
            "[Step 22240] Loss: 2.2186\n",
            "[Step 22250] Loss: 2.2036\n",
            "[Step 22260] Loss: 2.6864\n",
            "[Step 22270] Loss: 2.0882\n",
            "[Step 22280] Loss: 2.4964\n",
            "[Step 22290] Loss: 2.7324\n",
            "[Step 22300] Loss: 1.9105\n",
            "[Step 22310] Loss: 2.1706\n",
            "[Step 22320] Loss: 2.5290\n",
            "[Step 22330] Loss: 1.9418\n",
            "[Step 22340] Loss: 2.2502\n",
            "[Step 22350] Loss: 2.3699\n",
            "[Step 22360] Loss: 2.1062\n",
            "[Step 22370] Loss: 2.4220\n",
            "[Step 22380] Loss: 2.4030\n",
            "[Step 22390] Loss: 1.8973\n",
            "[Step 22400] Loss: 2.4759\n",
            "[Step 22410] Loss: 2.4669\n",
            "[Step 22420] Loss: 1.8615\n",
            "[Step 22430] Loss: 2.4289\n",
            "[Step 22440] Loss: 2.5246\n",
            "[Step 22450] Loss: 2.6961\n",
            "[Step 22460] Loss: 2.3713\n",
            "[Step 22470] Loss: 1.7822\n",
            "[Step 22480] Loss: 2.0225\n",
            "[Step 22490] Loss: 2.1621\n",
            "[Step 22500] Loss: 2.2123\n",
            "[Step 22510] Loss: 1.8065\n",
            "[Step 22520] Loss: 2.5040\n",
            "[Step 22530] Loss: 1.9395\n",
            "[Step 22540] Loss: 2.4329\n",
            "[Step 22550] Loss: 1.7452\n",
            "[Step 22560] Loss: 2.2639\n",
            "[Step 22570] Loss: 2.1524\n",
            "[Step 22580] Loss: 2.6746\n",
            "[Step 22590] Loss: 2.3122\n",
            "[Step 22600] Loss: 2.4526\n",
            "[Step 22610] Loss: 2.2412\n",
            "[Step 22620] Loss: 2.2142\n",
            "[Step 22630] Loss: 1.8474\n",
            "[Step 22640] Loss: 2.1646\n",
            "[Step 22650] Loss: 2.2979\n",
            "[Step 22660] Loss: 2.7206\n",
            "[Step 22670] Loss: 2.3087\n",
            "[Step 22680] Loss: 1.9844\n",
            "[Step 22690] Loss: 2.5879\n",
            "[Step 22700] Loss: 2.9004\n",
            "[Step 22710] Loss: 2.4991\n",
            "[Step 22720] Loss: 2.3151\n",
            "ðŸ“˜ Epoch 52 - Avg Training Loss: 2.2084\n",
            "ðŸ“Š Final Validation â€” Loss: 2.2813 | Accuracy: 0.4195 | Precision: 0.4071\n",
            "[Step 22730] Loss: 2.1339\n",
            "[Step 22740] Loss: 2.4689\n",
            "[Step 22750] Loss: 1.8634\n",
            "[Step 22760] Loss: 2.3633\n",
            "[Step 22770] Loss: 2.5688\n",
            "[Step 22780] Loss: 2.4166\n",
            "[Step 22790] Loss: 2.0585\n",
            "[Step 22800] Loss: 2.2607\n",
            "[Step 22810] Loss: 2.2275\n",
            "[Step 22820] Loss: 2.3135\n",
            "[Step 22830] Loss: 1.9945\n",
            "[Step 22840] Loss: 2.3143\n",
            "[Step 22850] Loss: 1.9488\n",
            "[Step 22860] Loss: 1.8842\n",
            "[Step 22870] Loss: 2.2341\n",
            "[Step 22880] Loss: 2.1051\n",
            "[Step 22890] Loss: 2.1285\n",
            "[Step 22900] Loss: 1.6901\n",
            "[Step 22910] Loss: 2.0404\n",
            "[Step 22920] Loss: 2.2992\n",
            "[Step 22930] Loss: 1.8019\n",
            "[Step 22940] Loss: 2.2014\n",
            "[Step 22950] Loss: 1.9814\n",
            "[Step 22960] Loss: 2.8897\n",
            "[Step 22970] Loss: 2.2620\n",
            "[Step 22980] Loss: 2.1033\n",
            "[Step 22990] Loss: 2.3024\n",
            "[Step 23000] Loss: 1.9767\n",
            "[Step 23010] Loss: 2.3154\n",
            "[Step 23020] Loss: 2.1302\n",
            "[Step 23030] Loss: 2.3574\n",
            "[Step 23040] Loss: 2.7839\n",
            "[Step 23050] Loss: 2.0128\n",
            "[Step 23060] Loss: 2.0456\n",
            "[Step 23070] Loss: 2.4094\n",
            "[Step 23080] Loss: 1.9403\n",
            "[Step 23090] Loss: 2.3522\n",
            "[Step 23100] Loss: 2.4699\n",
            "[Step 23110] Loss: 2.2324\n",
            "[Step 23120] Loss: 2.4222\n",
            "[Step 23130] Loss: 1.9404\n",
            "[Step 23140] Loss: 1.9439\n",
            "[Step 23150] Loss: 1.9092\n",
            "[Step 23160] Loss: 2.3179\n",
            "[Step 23170] Loss: 2.1001\n",
            "[Step 23180] Loss: 2.5060\n",
            "[Step 23190] Loss: 1.8705\n",
            "[Step 23200] Loss: 2.2568\n",
            "[Step 23210] Loss: 2.4426\n",
            "[Step 23220] Loss: 2.0370\n",
            "[Step 23230] Loss: 2.2482\n",
            "[Step 23240] Loss: 2.2814\n",
            "[Step 23250] Loss: 1.9057\n",
            "[Step 23260] Loss: 2.1887\n",
            "[Step 23270] Loss: 2.2233\n",
            "[Step 23280] Loss: 2.2788\n",
            "[Step 23290] Loss: 2.6938\n",
            "[Step 23300] Loss: 2.4721\n",
            "[Step 23310] Loss: 2.4620\n",
            "[Step 23320] Loss: 2.4061\n",
            "[Step 23330] Loss: 1.8565\n",
            "[Step 23340] Loss: 2.0245\n",
            "[Step 23350] Loss: 2.3495\n",
            "[Step 23360] Loss: 2.0103\n",
            "[Step 23370] Loss: 2.3525\n",
            "[Step 23380] Loss: 2.5781\n",
            "[Step 23390] Loss: 1.9206\n",
            "[Step 23400] Loss: 1.9723\n",
            "[Step 23410] Loss: 2.4643\n",
            "[Step 23420] Loss: 2.0263\n",
            "[Step 23430] Loss: 2.4506\n",
            "[Step 23440] Loss: 1.9032\n",
            "[Step 23450] Loss: 2.0883\n",
            "[Step 23460] Loss: 1.8885\n",
            "[Step 23470] Loss: 2.1825\n",
            "[Step 23480] Loss: 2.2293\n",
            "[Step 23490] Loss: 2.1204\n",
            "[Step 23500] Loss: 2.5582\n",
            "[Step 23510] Loss: 1.8682\n",
            "[Step 23520] Loss: 2.4768\n",
            "[Step 23530] Loss: 2.1547\n",
            "[Step 23540] Loss: 1.7556\n",
            "[Step 23550] Loss: 2.1296\n",
            "[Step 23560] Loss: 1.7542\n",
            "[Step 23570] Loss: 1.8665\n",
            "[Step 23580] Loss: 2.4003\n",
            "[Step 23590] Loss: 2.8317\n",
            "[Step 23600] Loss: 2.6884\n",
            "[Step 23610] Loss: 1.4667\n",
            "[Step 23620] Loss: 1.8364\n",
            "[Step 23630] Loss: 2.1234\n",
            "[Step 23640] Loss: 1.7735\n",
            "[Step 23650] Loss: 2.1574\n",
            "[Step 23660] Loss: 2.0644\n",
            "[Step 23670] Loss: 2.0897\n",
            "[Step 23680] Loss: 2.4722\n",
            "[Step 23690] Loss: 2.2145\n",
            "[Step 23700] Loss: 2.1066\n",
            "[Step 23710] Loss: 2.4319\n",
            "[Step 23720] Loss: 2.3593\n",
            "[Step 23730] Loss: 2.3541\n",
            "[Step 23740] Loss: 2.0693\n",
            "[Step 23750] Loss: 2.0897\n",
            "[Step 23760] Loss: 2.0281\n",
            "[Step 23770] Loss: 2.3191\n",
            "[Step 23780] Loss: 2.0444\n",
            "[Step 23790] Loss: 2.1660\n",
            "[Step 23800] Loss: 2.5147\n",
            "[Step 23810] Loss: 2.2578\n",
            "[Step 23820] Loss: 2.2910\n",
            "[Step 23830] Loss: 2.5850\n",
            "[Step 23840] Loss: 2.5649\n",
            "[Step 23850] Loss: 2.1287\n",
            "[Step 23860] Loss: 1.7664\n",
            "[Step 23870] Loss: 2.6444\n",
            "[Step 23880] Loss: 2.8122\n",
            "[Step 23890] Loss: 2.2315\n",
            "[Step 23900] Loss: 2.0697\n",
            "[Step 23910] Loss: 2.3330\n",
            "[Step 23920] Loss: 2.0197\n",
            "[Step 23930] Loss: 2.4307\n",
            "[Step 23940] Loss: 2.0255\n",
            "[Step 23950] Loss: 2.2399\n",
            "[Step 23960] Loss: 1.9071\n",
            "[Step 23970] Loss: 2.0996\n",
            "[Step 23980] Loss: 2.5407\n",
            "[Step 23990] Loss: 2.3123\n",
            "[Step 24000] Loss: 2.3456\n",
            "[Step 24010] Loss: 2.2442\n",
            "[Step 24020] Loss: 2.5563\n",
            "[Step 24030] Loss: 2.7081\n",
            "[Step 24040] Loss: 2.1102\n",
            "[Step 24050] Loss: 1.8965\n",
            "[Step 24060] Loss: 2.0678\n",
            "[Step 24070] Loss: 2.0711\n",
            "[Step 24080] Loss: 2.5875\n",
            "[Step 24090] Loss: 2.1911\n",
            "[Step 24100] Loss: 1.7343\n",
            "[Step 24110] Loss: 2.3911\n",
            "[Step 24120] Loss: 2.2606\n",
            "[Step 24130] Loss: 2.2449\n",
            "[Step 24140] Loss: 1.9573\n",
            "[Step 24150] Loss: 2.1029\n",
            "[Step 24160] Loss: 2.1704\n",
            "[Step 24170] Loss: 1.7658\n",
            "[Step 24180] Loss: 2.1015\n",
            "[Step 24190] Loss: 1.8286\n",
            "[Step 24200] Loss: 2.0921\n",
            "[Step 24210] Loss: 1.6397\n",
            "[Step 24220] Loss: 1.6171\n",
            "[Step 24230] Loss: 2.0657\n",
            "[Step 24240] Loss: 2.4562\n",
            "[Step 24250] Loss: 2.0727\n",
            "[Step 24260] Loss: 2.1151\n",
            "[Step 24270] Loss: 2.2167\n",
            "[Step 24280] Loss: 2.3241\n",
            "[Step 24290] Loss: 2.1435\n",
            "[Step 24300] Loss: 2.6444\n",
            "[Step 24310] Loss: 2.3027\n",
            "[Step 24320] Loss: 1.8633\n",
            "[Step 24330] Loss: 2.3989\n",
            "[Step 24340] Loss: 2.6931\n",
            "[Step 24350] Loss: 2.4916\n",
            "[Step 24360] Loss: 2.6842\n",
            "[Step 24370] Loss: 2.4587\n",
            "[Step 24380] Loss: 2.1634\n",
            "[Step 24390] Loss: 2.1901\n",
            "[Step 24400] Loss: 2.2234\n",
            "[Step 24410] Loss: 2.2778\n",
            "[Step 24420] Loss: 2.0294\n",
            "[Step 24430] Loss: 2.4580\n",
            "[Step 24440] Loss: 2.3649\n",
            "[Step 24450] Loss: 2.9685\n",
            "[Step 24460] Loss: 2.0677\n",
            "[Step 24470] Loss: 2.1751\n",
            "[Step 24480] Loss: 1.6643\n",
            "[Step 24490] Loss: 1.9549\n",
            "[Step 24500] Loss: 2.2478\n",
            "[Step 24510] Loss: 1.9955\n",
            "[Step 24520] Loss: 2.3850\n",
            "[Step 24530] Loss: 2.1106\n",
            "[Step 24540] Loss: 2.1231\n",
            "[Step 24550] Loss: 2.7611\n",
            "[Step 24560] Loss: 2.2888\n",
            "[Step 24570] Loss: 2.1617\n",
            "[Step 24580] Loss: 2.3561\n",
            "[Step 24590] Loss: 2.1226\n",
            "[Step 24600] Loss: 1.9671\n",
            "[Step 24610] Loss: 2.4918\n",
            "[Step 24620] Loss: 1.9890\n",
            "ðŸ“˜ Epoch 53 - Avg Training Loss: 2.2198\n",
            "ðŸ“Š Final Validation â€” Loss: 2.2897 | Accuracy: 0.4180 | Precision: 0.4058\n",
            "[Step 24630] Loss: 1.7640\n",
            "[Step 24640] Loss: 2.4724\n",
            "[Step 24650] Loss: 2.0269\n",
            "[Step 24660] Loss: 2.2097\n",
            "[Step 24670] Loss: 1.7812\n",
            "[Step 24680] Loss: 2.4190\n",
            "[Step 24690] Loss: 2.1529\n",
            "[Step 24700] Loss: 2.1130\n",
            "[Step 24710] Loss: 2.3922\n",
            "[Step 24720] Loss: 2.5856\n",
            "[Step 24730] Loss: 2.2119\n",
            "[Step 24740] Loss: 2.3025\n",
            "[Step 24750] Loss: 1.9563\n",
            "[Step 24760] Loss: 1.9958\n",
            "[Step 24770] Loss: 2.4417\n",
            "[Step 24780] Loss: 2.0167\n",
            "[Step 24790] Loss: 2.4029\n",
            "[Step 24800] Loss: 2.7640\n",
            "[Step 24810] Loss: 2.2405\n",
            "[Step 24820] Loss: 1.6000\n",
            "[Step 24830] Loss: 2.1077\n",
            "[Step 24840] Loss: 2.0308\n",
            "[Step 24850] Loss: 2.0171\n",
            "[Step 24860] Loss: 2.6960\n",
            "[Step 24870] Loss: 2.3192\n",
            "[Step 24880] Loss: 2.5226\n",
            "[Step 24890] Loss: 2.3092\n",
            "[Step 24900] Loss: 2.1927\n",
            "[Step 24910] Loss: 2.1682\n",
            "[Step 24920] Loss: 2.2864\n",
            "[Step 24930] Loss: 2.3272\n",
            "[Step 24940] Loss: 2.6437\n",
            "[Step 24950] Loss: 2.1339\n",
            "[Step 24960] Loss: 2.7755\n",
            "[Step 24970] Loss: 2.3491\n",
            "[Step 24980] Loss: 2.3238\n",
            "[Step 24990] Loss: 2.4281\n",
            "[Step 25000] Loss: 2.1386\n",
            "[Step 25010] Loss: 2.7739\n",
            "[Step 25020] Loss: 2.5376\n",
            "[Step 25030] Loss: 1.8050\n",
            "[Step 25040] Loss: 2.2639\n",
            "[Step 25050] Loss: 2.1710\n",
            "[Step 25060] Loss: 2.2472\n",
            "[Step 25070] Loss: 2.6255\n",
            "[Step 25080] Loss: 1.9837\n",
            "[Step 25090] Loss: 2.5986\n",
            "[Step 25100] Loss: 2.2002\n",
            "[Step 25110] Loss: 2.1397\n",
            "[Step 25120] Loss: 2.2640\n",
            "[Step 25130] Loss: 2.1445\n",
            "[Step 25140] Loss: 2.0934\n",
            "[Step 25150] Loss: 1.6987\n",
            "[Step 25160] Loss: 1.7057\n",
            "[Step 25170] Loss: 2.1514\n",
            "[Step 25180] Loss: 2.3802\n",
            "[Step 25190] Loss: 2.3394\n",
            "[Step 25200] Loss: 2.1704\n",
            "[Step 25210] Loss: 2.8145\n",
            "[Step 25220] Loss: 2.3890\n",
            "[Step 25230] Loss: 2.7409\n",
            "[Step 25240] Loss: 2.0997\n",
            "[Step 25250] Loss: 2.1408\n",
            "[Step 25260] Loss: 2.5892\n",
            "[Step 25270] Loss: 2.5758\n",
            "[Step 25280] Loss: 2.4529\n",
            "[Step 25290] Loss: 1.6733\n",
            "[Step 25300] Loss: 2.5886\n",
            "[Step 25310] Loss: 2.2841\n",
            "[Step 25320] Loss: 1.7954\n",
            "[Step 25330] Loss: 2.2505\n",
            "[Step 25340] Loss: 2.3679\n",
            "[Step 25350] Loss: 2.3763\n",
            "[Step 25360] Loss: 2.1242\n",
            "[Step 25370] Loss: 2.0562\n",
            "[Step 25380] Loss: 1.7667\n",
            "[Step 25390] Loss: 2.2969\n",
            "[Step 25400] Loss: 2.4203\n",
            "[Step 25410] Loss: 2.1078\n",
            "[Step 25420] Loss: 2.3809\n",
            "[Step 25430] Loss: 2.0072\n",
            "[Step 25440] Loss: 2.3801\n",
            "[Step 25450] Loss: 2.6746\n",
            "[Step 25460] Loss: 2.4505\n",
            "[Step 25470] Loss: 2.3644\n",
            "[Step 25480] Loss: 2.2100\n",
            "[Step 25490] Loss: 2.7177\n",
            "[Step 25500] Loss: 3.0479\n",
            "[Step 25510] Loss: 2.1196\n",
            "[Step 25520] Loss: 2.3682\n",
            "[Step 25530] Loss: 2.2263\n",
            "[Step 25540] Loss: 1.7068\n",
            "[Step 25550] Loss: 2.3463\n",
            "[Step 25560] Loss: 2.4014\n",
            "[Step 25570] Loss: 2.1662\n",
            "[Step 25580] Loss: 1.8542\n",
            "[Step 25590] Loss: 2.2740\n",
            "[Step 25600] Loss: 2.2180\n",
            "[Step 25610] Loss: 1.8756\n",
            "[Step 25620] Loss: 2.3799\n",
            "[Step 25630] Loss: 2.5581\n",
            "[Step 25640] Loss: 2.6038\n",
            "[Step 25650] Loss: 2.2734\n",
            "[Step 25660] Loss: 2.7391\n",
            "[Step 25670] Loss: 2.4300\n",
            "[Step 25680] Loss: 2.4252\n",
            "[Step 25690] Loss: 2.6796\n",
            "[Step 25700] Loss: 1.7182\n",
            "[Step 25710] Loss: 2.3046\n",
            "[Step 25720] Loss: 2.2090\n",
            "[Step 25730] Loss: 2.1460\n",
            "[Step 25740] Loss: 2.6422\n",
            "[Step 25750] Loss: 2.5911\n",
            "[Step 25760] Loss: 2.3334\n",
            "[Step 25770] Loss: 1.9611\n",
            "[Step 25780] Loss: 2.1737\n",
            "[Step 25790] Loss: 2.2797\n",
            "[Step 25800] Loss: 2.2384\n",
            "[Step 25810] Loss: 2.2617\n",
            "[Step 25820] Loss: 2.5830\n",
            "[Step 25830] Loss: 1.8431\n",
            "[Step 25840] Loss: 1.7840\n",
            "[Step 25850] Loss: 2.6154\n",
            "[Step 25860] Loss: 1.9530\n",
            "[Step 25870] Loss: 2.1769\n",
            "[Step 25880] Loss: 2.1041\n",
            "[Step 25890] Loss: 1.9698\n",
            "[Step 25900] Loss: 2.6287\n",
            "[Step 25910] Loss: 2.3123\n",
            "[Step 25920] Loss: 1.9621\n",
            "[Step 25930] Loss: 2.2501\n",
            "[Step 25940] Loss: 2.2557\n",
            "[Step 25950] Loss: 2.4920\n",
            "[Step 25960] Loss: 2.9336\n",
            "[Step 25970] Loss: 2.2080\n",
            "[Step 25980] Loss: 2.2921\n",
            "[Step 25990] Loss: 1.9968\n",
            "[Step 26000] Loss: 2.0621\n",
            "[Step 26010] Loss: 2.2296\n",
            "[Step 26020] Loss: 2.1156\n",
            "[Step 26030] Loss: 2.4746\n",
            "[Step 26040] Loss: 1.9929\n",
            "[Step 26050] Loss: 2.3290\n",
            "[Step 26060] Loss: 2.2156\n",
            "[Step 26070] Loss: 2.3773\n",
            "[Step 26080] Loss: 2.5147\n",
            "[Step 26090] Loss: 2.2489\n",
            "[Step 26100] Loss: 1.7448\n",
            "[Step 26110] Loss: 1.9487\n",
            "[Step 26120] Loss: 2.1680\n",
            "[Step 26130] Loss: 2.1152\n",
            "[Step 26140] Loss: 2.1517\n",
            "[Step 26150] Loss: 2.2168\n",
            "[Step 26160] Loss: 2.4492\n",
            "[Step 26170] Loss: 2.0571\n",
            "[Step 26180] Loss: 2.7679\n",
            "[Step 26190] Loss: 2.0325\n",
            "[Step 26200] Loss: 1.6346\n",
            "[Step 26210] Loss: 2.6543\n",
            "[Step 26220] Loss: 2.1082\n",
            "[Step 26230] Loss: 1.6785\n",
            "[Step 26240] Loss: 1.9484\n",
            "[Step 26250] Loss: 1.7868\n",
            "[Step 26260] Loss: 2.7777\n",
            "[Step 26270] Loss: 2.2302\n",
            "[Step 26280] Loss: 1.8125\n",
            "[Step 26290] Loss: 2.5829\n",
            "[Step 26300] Loss: 3.4886\n",
            "[Step 26310] Loss: 1.5604\n",
            "[Step 26320] Loss: 2.7643\n",
            "[Step 26330] Loss: 1.9573\n",
            "[Step 26340] Loss: 2.2203\n",
            "[Step 26350] Loss: 2.5548\n",
            "[Step 26360] Loss: 2.1666\n",
            "[Step 26370] Loss: 2.7563\n",
            "[Step 26380] Loss: 2.3197\n",
            "[Step 26390] Loss: 2.6000\n",
            "[Step 26400] Loss: 2.4966\n",
            "[Step 26410] Loss: 2.2471\n",
            "[Step 26420] Loss: 1.6201\n",
            "[Step 26430] Loss: 2.7364\n",
            "[Step 26440] Loss: 2.6662\n",
            "[Step 26450] Loss: 2.5435\n",
            "[Step 26460] Loss: 1.6993\n",
            "[Step 26470] Loss: 2.0106\n",
            "[Step 26480] Loss: 2.4494\n",
            "[Step 26490] Loss: 2.8859\n",
            "[Step 26500] Loss: 2.2585\n",
            "[Step 26510] Loss: 2.5260\n",
            "ðŸ“˜ Epoch 54 - Avg Training Loss: 2.2432\n",
            "ðŸ“Š Final Validation â€” Loss: 2.3099 | Accuracy: 0.4164 | Precision: 0.4044\n",
            "[Step 26520] Loss: 2.4037\n",
            "[Step 26530] Loss: 2.0420\n",
            "[Step 26540] Loss: 2.2707\n",
            "[Step 26550] Loss: 2.0862\n",
            "[Step 26560] Loss: 1.8668\n",
            "[Step 26570] Loss: 1.8991\n",
            "[Step 26580] Loss: 2.6390\n",
            "[Step 26590] Loss: 1.9547\n",
            "[Step 26600] Loss: 2.2665\n",
            "[Step 26610] Loss: 1.9394\n",
            "[Step 26620] Loss: 1.6236\n",
            "[Step 26630] Loss: 2.2973\n",
            "[Step 26640] Loss: 2.7113\n",
            "[Step 26650] Loss: 2.7978\n",
            "[Step 26660] Loss: 2.2001\n",
            "[Step 26670] Loss: 2.4705\n",
            "[Step 26680] Loss: 2.0503\n",
            "[Step 26690] Loss: 2.1222\n",
            "[Step 26700] Loss: 2.2956\n",
            "[Step 26710] Loss: 2.4812\n",
            "[Step 26720] Loss: 1.7350\n",
            "[Step 26730] Loss: 1.7969\n",
            "[Step 26740] Loss: 1.8547\n",
            "[Step 26750] Loss: 2.1180\n",
            "[Step 26760] Loss: 2.4697\n",
            "[Step 26770] Loss: 2.6794\n",
            "[Step 26780] Loss: 2.5596\n",
            "[Step 26790] Loss: 1.6289\n",
            "[Step 26800] Loss: 1.7519\n",
            "[Step 26810] Loss: 2.1448\n",
            "[Step 26820] Loss: 2.4228\n",
            "[Step 26830] Loss: 1.7728\n",
            "[Step 26840] Loss: 2.6784\n",
            "[Step 26850] Loss: 2.6383\n",
            "[Step 26860] Loss: 2.0370\n",
            "[Step 26870] Loss: 2.2240\n",
            "[Step 26880] Loss: 1.9783\n",
            "[Step 26890] Loss: 2.6028\n",
            "[Step 26900] Loss: 2.7413\n",
            "[Step 26910] Loss: 1.9486\n",
            "[Step 26920] Loss: 1.8755\n",
            "[Step 26930] Loss: 1.9645\n",
            "[Step 26940] Loss: 2.3404\n",
            "[Step 26950] Loss: 2.3600\n",
            "[Step 26960] Loss: 2.3262\n",
            "[Step 26970] Loss: 2.4414\n",
            "[Step 26980] Loss: 2.2797\n",
            "[Step 26990] Loss: 1.8732\n",
            "[Step 27000] Loss: 2.3735\n",
            "[Step 27010] Loss: 2.5299\n",
            "[Step 27020] Loss: 1.9988\n",
            "[Step 27030] Loss: 2.4962\n",
            "[Step 27040] Loss: 2.7791\n",
            "[Step 27050] Loss: 2.5913\n",
            "[Step 27060] Loss: 1.7064\n",
            "[Step 27070] Loss: 2.4040\n",
            "[Step 27080] Loss: 1.4790\n",
            "[Step 27090] Loss: 2.7250\n",
            "[Step 27100] Loss: 2.0787\n",
            "[Step 27110] Loss: 2.0492\n",
            "[Step 27120] Loss: 2.5735\n",
            "[Step 27130] Loss: 2.1579\n",
            "[Step 27140] Loss: 2.7916\n",
            "[Step 27150] Loss: 2.1772\n",
            "[Step 27160] Loss: 2.5560\n",
            "[Step 27170] Loss: 2.1610\n",
            "[Step 27180] Loss: 2.7789\n",
            "[Step 27190] Loss: 2.0856\n",
            "[Step 27200] Loss: 2.5741\n",
            "[Step 27210] Loss: 2.6815\n",
            "[Step 27220] Loss: 2.7532\n",
            "[Step 27230] Loss: 2.2049\n",
            "[Step 27240] Loss: 2.5813\n",
            "[Step 27250] Loss: 2.3733\n",
            "[Step 27260] Loss: 2.3937\n",
            "[Step 27270] Loss: 2.3404\n",
            "[Step 27280] Loss: 2.1617\n",
            "[Step 27290] Loss: 2.3372\n",
            "[Step 27300] Loss: 2.7225\n",
            "[Step 27310] Loss: 2.0021\n",
            "[Step 27320] Loss: 1.9260\n",
            "[Step 27330] Loss: 1.8102\n",
            "[Step 27340] Loss: 2.4357\n",
            "[Step 27350] Loss: 2.6401\n",
            "[Step 27360] Loss: 2.8094\n",
            "[Step 27370] Loss: 2.1842\n",
            "[Step 27380] Loss: 2.8415\n",
            "[Step 27390] Loss: 2.0998\n",
            "[Step 27400] Loss: 2.5443\n",
            "[Step 27410] Loss: 2.4602\n",
            "[Step 27420] Loss: 1.7837\n",
            "[Step 27430] Loss: 1.8169\n",
            "[Step 27440] Loss: 2.4423\n",
            "[Step 27450] Loss: 2.4640\n",
            "[Step 27460] Loss: 1.7189\n",
            "[Step 27470] Loss: 2.5133\n",
            "[Step 27480] Loss: 2.1170\n",
            "[Step 27490] Loss: 2.2001\n",
            "[Step 27500] Loss: 1.8283\n",
            "[Step 27510] Loss: 2.2045\n",
            "[Step 27520] Loss: 3.3158\n",
            "[Step 27530] Loss: 2.2356\n",
            "[Step 27540] Loss: 2.2269\n",
            "[Step 27550] Loss: 2.1041\n",
            "[Step 27560] Loss: 2.3412\n",
            "[Step 27570] Loss: 2.4486\n",
            "[Step 27580] Loss: 2.7298\n",
            "[Step 27590] Loss: 2.0486\n",
            "[Step 27600] Loss: 2.2880\n",
            "[Step 27610] Loss: 1.8113\n",
            "[Step 27620] Loss: 2.0186\n",
            "[Step 27630] Loss: 2.2443\n",
            "[Step 27640] Loss: 1.9682\n",
            "[Step 27650] Loss: 2.0835\n",
            "[Step 27660] Loss: 2.0351\n",
            "[Step 27670] Loss: 1.7877\n",
            "[Step 27680] Loss: 2.3984\n",
            "[Step 27690] Loss: 2.3464\n",
            "[Step 27700] Loss: 2.6910\n",
            "[Step 27710] Loss: 2.1653\n",
            "[Step 27720] Loss: 2.1353\n",
            "[Step 27730] Loss: 2.0458\n",
            "[Step 27740] Loss: 2.0312\n",
            "[Step 27750] Loss: 1.7705\n",
            "[Step 27760] Loss: 2.5488\n",
            "[Step 27770] Loss: 2.4637\n",
            "[Step 27780] Loss: 2.1585\n",
            "[Step 27790] Loss: 1.7606\n",
            "[Step 27800] Loss: 2.2608\n",
            "[Step 27810] Loss: 2.2211\n",
            "[Step 27820] Loss: 1.9913\n",
            "[Step 27830] Loss: 2.0705\n",
            "[Step 27840] Loss: 2.1942\n",
            "[Step 27850] Loss: 2.3920\n",
            "[Step 27860] Loss: 2.1731\n",
            "[Step 27870] Loss: 2.1448\n",
            "[Step 27880] Loss: 2.4818\n",
            "[Step 27890] Loss: 1.8317\n",
            "[Step 27900] Loss: 2.1960\n",
            "[Step 27910] Loss: 1.9028\n",
            "[Step 27920] Loss: 2.6394\n",
            "[Step 27930] Loss: 2.2143\n",
            "[Step 27940] Loss: 2.2322\n",
            "[Step 27950] Loss: 1.7639\n",
            "[Step 27960] Loss: 2.0721\n",
            "[Step 27970] Loss: 1.9868\n",
            "[Step 27980] Loss: 2.1730\n",
            "[Step 27990] Loss: 2.7161\n",
            "[Step 28000] Loss: 2.1821\n",
            "[Step 28010] Loss: 2.5201\n",
            "[Step 28020] Loss: 2.5945\n",
            "[Step 28030] Loss: 2.4939\n",
            "[Step 28040] Loss: 2.3331\n",
            "[Step 28050] Loss: 1.9110\n",
            "[Step 28060] Loss: 2.4199\n",
            "[Step 28070] Loss: 2.1889\n",
            "[Step 28080] Loss: 1.8440\n",
            "[Step 28090] Loss: 2.7156\n",
            "[Step 28100] Loss: 2.6046\n",
            "[Step 28110] Loss: 2.6385\n",
            "[Step 28120] Loss: 2.1705\n",
            "[Step 28130] Loss: 2.8443\n",
            "[Step 28140] Loss: 1.9049\n",
            "[Step 28150] Loss: 2.3808\n",
            "[Step 28160] Loss: 2.0055\n",
            "[Step 28170] Loss: 2.1190\n",
            "[Step 28180] Loss: 2.1480\n",
            "[Step 28190] Loss: 2.3278\n",
            "[Step 28200] Loss: 2.7754\n",
            "[Step 28210] Loss: 2.2475\n",
            "[Step 28220] Loss: 2.4135\n",
            "[Step 28230] Loss: 2.1595\n",
            "[Step 28240] Loss: 2.0803\n",
            "[Step 28250] Loss: 2.1359\n",
            "[Step 28260] Loss: 2.4977\n",
            "[Step 28270] Loss: 2.4929\n",
            "[Step 28280] Loss: 2.3126\n",
            "[Step 28290] Loss: 2.7105\n",
            "[Step 28300] Loss: 2.3767\n",
            "[Step 28310] Loss: 2.1845\n",
            "[Step 28320] Loss: 2.5417\n",
            "[Step 28330] Loss: 2.6211\n",
            "[Step 28340] Loss: 2.5536\n",
            "[Step 28350] Loss: 2.7170\n",
            "[Step 28360] Loss: 2.7884\n",
            "[Step 28370] Loss: 2.3310\n",
            "[Step 28380] Loss: 2.0679\n",
            "[Step 28390] Loss: 2.4444\n",
            "[Step 28400] Loss: 2.6887\n",
            "[Step 28410] Loss: 2.5494\n",
            "ðŸ“˜ Epoch 55 - Avg Training Loss: 2.2688\n",
            "ðŸ“Š Final Validation â€” Loss: 2.3589 | Accuracy: 0.4020 | Precision: 0.4004\n",
            "âœ… Continued training complete\n"
          ]
        }
      ],
      "source": [
        "num_classes = 101\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "checkpoint_path = '/content/drive/My Drive/NexHack/checkpoint_4.pth'\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "new_training_args = {\n",
        "    \"learning_rate\": 3e-4,\n",
        "    \"weight_decay\": 5e-4,\n",
        "    \"num_additional_epochs\": 15,\n",
        "    \"logging_steps\": 10,\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=new_training_args[\"learning_rate\"],\n",
        "                       weight_decay=new_training_args[\"weight_decay\"])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model.to(device)\n",
        "\n",
        "starting_epoch = checkpoint['epoch']\n",
        "global_step = 0\n",
        "train_losses = []\n",
        "\n",
        "def evaluate_val(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(accuracy)\n",
        "    val_precisions.append(precision)\n",
        "\n",
        "    print(f\"ðŸ“Š Final Validation â€” Loss: {avg_val_loss:.4f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f}\")\n",
        "\n",
        "for epoch in range(starting_epoch, starting_epoch + new_training_args[\"num_additional_epochs\"]):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % new_training_args[\"logging_steps\"] == 0:\n",
        "            print(f\"[Step {global_step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"ðŸ“˜ Epoch {epoch + 1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    evaluate_val(model, val_loader, criterion, device)\n",
        "\n",
        "print(\"âœ… Continued training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8OGyPK7otoSf",
        "outputId": "620d94e4-87e1-49e8-aef0-662e4ed664da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Saved training checkpoint to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "save_path = '/content/drive/My Drive/NexHack/model_weights_5.pth'\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=True)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model_weights_1 = MyResNet50()\n",
        "\n",
        "torch.save(model_weights_1.state_dict(), 'model_weights_5.pth')\n",
        "torch.save(model_weights_1.state_dict(), save_path)\n",
        "\n",
        "save_path = '/content/drive/My Drive/NexHack/model_full_5.pth'\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=True)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model_full_1 = MyResNet50()\n",
        "\n",
        "torch.save(model_full_1, 'model_full_5.pth')\n",
        "torch.save(model_full_1.state_dict(), save_path)\n",
        "\n",
        "save_path = '/content/drive/My Drive/NexHack/checkpoint_5.pth'\n",
        "\n",
        "checkpoint = {\n",
        "    'epoch': 40,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'training_args': new_training_args\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, save_path)\n",
        "\n",
        "print(\"âœ… Saved training checkpoint to Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg9RXgvluLFY",
        "outputId": "6795be0b-a22a-48af-aa5b-3893230b54b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 10] Loss: 3.0304\n",
            "[Step 20] Loss: 3.1425\n",
            "[Step 30] Loss: 2.7836\n",
            "[Step 40] Loss: 2.8386\n",
            "[Step 50] Loss: 3.0405\n",
            "[Step 60] Loss: 2.8248\n",
            "[Step 70] Loss: 2.7701\n",
            "[Step 80] Loss: 2.4632\n",
            "[Step 90] Loss: 2.5223\n",
            "[Step 100] Loss: 2.9752\n",
            "[Step 110] Loss: 2.5034\n",
            "[Step 120] Loss: 3.2254\n",
            "[Step 130] Loss: 2.3561\n",
            "[Step 140] Loss: 3.0030\n",
            "[Step 150] Loss: 2.3091\n",
            "[Step 160] Loss: 2.9455\n",
            "[Step 170] Loss: 3.0533\n",
            "[Step 180] Loss: 2.9488\n",
            "[Step 190] Loss: 2.3972\n",
            "[Step 200] Loss: 3.3894\n",
            "[Step 210] Loss: 2.9466\n",
            "[Step 220] Loss: 2.8679\n",
            "[Step 230] Loss: 2.9533\n",
            "[Step 240] Loss: 2.7291\n",
            "[Step 250] Loss: 3.4245\n",
            "[Step 260] Loss: 2.3871\n",
            "[Step 270] Loss: 2.9051\n",
            "[Step 280] Loss: 2.7041\n",
            "[Step 290] Loss: 2.6215\n",
            "[Step 300] Loss: 2.9355\n",
            "[Step 310] Loss: 2.8402\n",
            "[Step 320] Loss: 2.7117\n",
            "[Step 330] Loss: 3.3348\n",
            "[Step 340] Loss: 3.0667\n",
            "[Step 350] Loss: 2.9721\n",
            "[Step 360] Loss: 3.0521\n",
            "[Step 370] Loss: 2.9260\n",
            "[Step 380] Loss: 3.1146\n",
            "[Step 390] Loss: 2.8377\n",
            "[Step 400] Loss: 2.9644\n",
            "[Step 410] Loss: 2.9458\n",
            "[Step 420] Loss: 3.3993\n",
            "[Step 430] Loss: 2.6172\n",
            "[Step 440] Loss: 3.0325\n",
            "[Step 450] Loss: 2.6063\n",
            "[Step 460] Loss: 2.9924\n",
            "[Step 470] Loss: 3.0551\n",
            "[Step 480] Loss: 3.0765\n",
            "[Step 490] Loss: 2.8371\n",
            "[Step 500] Loss: 3.0565\n",
            "[Step 510] Loss: 3.4030\n",
            "[Step 520] Loss: 2.7751\n",
            "[Step 530] Loss: 2.8606\n",
            "[Step 540] Loss: 2.7959\n",
            "[Step 550] Loss: 2.8119\n",
            "[Step 560] Loss: 3.1720\n",
            "[Step 570] Loss: 2.8873\n",
            "[Step 580] Loss: 2.8500\n",
            "[Step 590] Loss: 3.3387\n",
            "[Step 600] Loss: 2.8357\n",
            "[Step 610] Loss: 2.5654\n",
            "[Step 620] Loss: 3.4129\n",
            "[Step 630] Loss: 2.8707\n",
            "[Step 640] Loss: 2.9318\n",
            "[Step 650] Loss: 2.6757\n",
            "[Step 660] Loss: 2.8917\n",
            "[Step 670] Loss: 2.9419\n",
            "[Step 680] Loss: 2.7943\n",
            "[Step 690] Loss: 2.4870\n",
            "[Step 700] Loss: 2.6630\n",
            "[Step 710] Loss: 2.6056\n",
            "[Step 720] Loss: 2.9547\n",
            "[Step 730] Loss: 3.1619\n",
            "[Step 740] Loss: 2.9354\n",
            "[Step 750] Loss: 2.6276\n",
            "[Step 760] Loss: 2.8424\n",
            "[Step 770] Loss: 3.0079\n",
            "[Step 780] Loss: 3.0034\n",
            "[Step 790] Loss: 2.6892\n",
            "[Step 800] Loss: 3.3000\n",
            "[Step 810] Loss: 3.0705\n",
            "[Step 820] Loss: 3.1054\n",
            "[Step 830] Loss: 3.2407\n",
            "[Step 840] Loss: 3.1629\n",
            "[Step 850] Loss: 2.7352\n",
            "[Step 860] Loss: 2.7981\n",
            "[Step 870] Loss: 2.8096\n",
            "[Step 880] Loss: 2.9727\n",
            "[Step 890] Loss: 2.6877\n",
            "[Step 900] Loss: 3.0548\n",
            "[Step 910] Loss: 3.2053\n",
            "[Step 920] Loss: 2.6755\n",
            "[Step 930] Loss: 2.9106\n",
            "[Step 940] Loss: 3.1815\n",
            "[Step 950] Loss: 2.9267\n",
            "[Step 960] Loss: 2.5821\n",
            "[Step 970] Loss: 2.5775\n",
            "[Step 980] Loss: 2.8140\n",
            "[Step 990] Loss: 3.2199\n",
            "[Step 1000] Loss: 2.7290\n",
            "[Step 1010] Loss: 2.5067\n",
            "[Step 1020] Loss: 2.5276\n",
            "[Step 1030] Loss: 2.5053\n",
            "[Step 1040] Loss: 3.1551\n",
            "[Step 1050] Loss: 2.7019\n",
            "[Step 1060] Loss: 2.6703\n",
            "[Step 1070] Loss: 2.7549\n",
            "[Step 1080] Loss: 3.1212\n",
            "[Step 1090] Loss: 2.9144\n",
            "[Step 1100] Loss: 3.3314\n",
            "[Step 1110] Loss: 3.4239\n",
            "[Step 1120] Loss: 2.7684\n",
            "[Step 1130] Loss: 2.5255\n",
            "[Step 1140] Loss: 2.9820\n",
            "[Step 1150] Loss: 2.2477\n",
            "[Step 1160] Loss: 2.9580\n",
            "[Step 1170] Loss: 2.9464\n",
            "[Step 1180] Loss: 3.1317\n",
            "[Step 1190] Loss: 2.5537\n",
            "[Step 1200] Loss: 2.6239\n",
            "[Step 1210] Loss: 2.8728\n",
            "[Step 1220] Loss: 2.9518\n",
            "[Step 1230] Loss: 3.3001\n",
            "[Step 1240] Loss: 3.1466\n",
            "[Step 1250] Loss: 2.8545\n",
            "[Step 1260] Loss: 2.9944\n",
            "[Step 1270] Loss: 2.5677\n",
            "[Step 1280] Loss: 2.2709\n",
            "[Step 1290] Loss: 2.9878\n",
            "[Step 1300] Loss: 2.8162\n",
            "[Step 1310] Loss: 2.9024\n",
            "[Step 1320] Loss: 2.8328\n",
            "[Step 1330] Loss: 2.7627\n",
            "[Step 1340] Loss: 2.6542\n",
            "[Step 1350] Loss: 3.2093\n",
            "[Step 1360] Loss: 2.8782\n",
            "[Step 1370] Loss: 2.9025\n",
            "[Step 1380] Loss: 2.8353\n",
            "[Step 1390] Loss: 3.1630\n",
            "[Step 1400] Loss: 2.7562\n",
            "[Step 1410] Loss: 2.7043\n",
            "[Step 1420] Loss: 2.7351\n",
            "[Step 1430] Loss: 2.7168\n",
            "[Step 1440] Loss: 2.7161\n",
            "[Step 1450] Loss: 3.1317\n",
            "[Step 1460] Loss: 2.7193\n",
            "[Step 1470] Loss: 2.7538\n",
            "[Step 1480] Loss: 2.5088\n",
            "[Step 1490] Loss: 3.0624\n",
            "[Step 1500] Loss: 2.7119\n",
            "[Step 1510] Loss: 2.7678\n",
            "[Step 1520] Loss: 2.7228\n",
            "[Step 1530] Loss: 2.8052\n",
            "[Step 1540] Loss: 2.8858\n",
            "[Step 1550] Loss: 2.8532\n",
            "[Step 1560] Loss: 2.6203\n",
            "[Step 1570] Loss: 2.7500\n",
            "[Step 1580] Loss: 3.1664\n",
            "[Step 1590] Loss: 2.6283\n",
            "[Step 1600] Loss: 2.5065\n",
            "[Step 1610] Loss: 2.3938\n",
            "[Step 1620] Loss: 2.9302\n",
            "[Step 1630] Loss: 2.5175\n",
            "[Step 1640] Loss: 2.9178\n",
            "[Step 1650] Loss: 3.3518\n",
            "[Step 1660] Loss: 2.8739\n",
            "[Step 1670] Loss: 2.4918\n",
            "[Step 1680] Loss: 2.8787\n",
            "[Step 1690] Loss: 2.8998\n",
            "[Step 1700] Loss: 2.6312\n",
            "[Step 1710] Loss: 2.2790\n",
            "[Step 1720] Loss: 2.8061\n",
            "[Step 1730] Loss: 3.0562\n",
            "[Step 1740] Loss: 3.1084\n",
            "[Step 1750] Loss: 2.5621\n",
            "[Step 1760] Loss: 2.5932\n",
            "[Step 1770] Loss: 2.7779\n",
            "[Step 1780] Loss: 2.8590\n",
            "[Step 1790] Loss: 2.9643\n",
            "[Step 1800] Loss: 3.0695\n",
            "[Step 1810] Loss: 2.8381\n",
            "[Step 1820] Loss: 3.0014\n",
            "[Step 1830] Loss: 3.3460\n",
            "[Step 1840] Loss: 2.7866\n",
            "[Step 1850] Loss: 2.9776\n",
            "[Step 1860] Loss: 2.8505\n",
            "[Step 1870] Loss: 2.9759\n",
            "[Step 1880] Loss: 2.7652\n",
            "[Step 1890] Loss: 2.8803\n",
            "ðŸ“˜ Epoch 41 - Avg Training Loss: 2.8828\n",
            "ðŸ” Validation â€” Loss: 2.8267 | Accuracy: 41.10% | Precision: 39.74%\n",
            "[Step 1900] Loss: 3.0125\n",
            "[Step 1910] Loss: 3.2120\n",
            "[Step 1920] Loss: 2.7044\n",
            "[Step 1930] Loss: 3.0430\n",
            "[Step 1940] Loss: 2.7450\n",
            "[Step 1950] Loss: 2.7147\n",
            "[Step 1960] Loss: 2.5751\n",
            "[Step 1970] Loss: 2.4687\n",
            "[Step 1980] Loss: 2.6015\n",
            "[Step 1990] Loss: 2.9713\n",
            "[Step 2000] Loss: 3.1295\n",
            "[Step 2010] Loss: 2.7706\n",
            "[Step 2020] Loss: 2.8674\n",
            "[Step 2030] Loss: 2.6732\n",
            "[Step 2040] Loss: 3.0356\n",
            "[Step 2050] Loss: 2.7185\n",
            "[Step 2060] Loss: 3.0130\n",
            "[Step 2070] Loss: 2.8620\n",
            "[Step 2080] Loss: 2.6870\n",
            "[Step 2090] Loss: 2.6649\n",
            "[Step 2100] Loss: 3.3146\n",
            "[Step 2110] Loss: 2.8722\n",
            "[Step 2120] Loss: 3.0225\n",
            "[Step 2130] Loss: 2.5775\n",
            "[Step 2140] Loss: 2.9140\n",
            "[Step 2150] Loss: 3.2395\n",
            "[Step 2160] Loss: 2.8308\n",
            "[Step 2170] Loss: 3.3156\n",
            "[Step 2180] Loss: 2.8418\n",
            "[Step 2190] Loss: 2.8528\n",
            "[Step 2200] Loss: 2.3627\n",
            "[Step 2210] Loss: 3.2258\n",
            "[Step 2220] Loss: 2.8774\n",
            "[Step 2230] Loss: 3.6032\n",
            "[Step 2240] Loss: 2.9073\n",
            "[Step 2250] Loss: 2.5207\n",
            "[Step 2260] Loss: 2.5388\n",
            "[Step 2270] Loss: 3.3566\n",
            "[Step 2280] Loss: 2.6977\n",
            "[Step 2290] Loss: 2.6096\n",
            "[Step 2300] Loss: 3.0185\n",
            "[Step 2310] Loss: 2.6566\n",
            "[Step 2320] Loss: 2.4581\n",
            "[Step 2330] Loss: 2.7125\n",
            "[Step 2340] Loss: 3.0434\n",
            "[Step 2350] Loss: 2.7918\n",
            "[Step 2360] Loss: 2.8984\n",
            "[Step 2370] Loss: 2.8095\n",
            "[Step 2380] Loss: 2.7997\n",
            "[Step 2390] Loss: 3.0595\n",
            "[Step 2400] Loss: 2.9599\n",
            "[Step 2410] Loss: 2.6584\n",
            "[Step 2420] Loss: 2.9064\n",
            "[Step 2430] Loss: 2.9523\n",
            "[Step 2440] Loss: 2.6517\n",
            "[Step 2450] Loss: 3.2106\n",
            "[Step 2460] Loss: 2.6617\n",
            "[Step 2470] Loss: 2.7394\n",
            "[Step 2480] Loss: 2.8840\n",
            "[Step 2490] Loss: 2.5628\n",
            "[Step 2500] Loss: 3.1100\n",
            "[Step 2510] Loss: 2.4959\n",
            "[Step 2520] Loss: 2.9473\n",
            "[Step 2530] Loss: 2.9703\n",
            "[Step 2540] Loss: 2.8204\n",
            "[Step 2550] Loss: 2.7519\n",
            "[Step 2560] Loss: 2.8921\n",
            "[Step 2570] Loss: 2.8052\n",
            "[Step 2580] Loss: 2.4169\n",
            "[Step 2590] Loss: 2.3995\n",
            "[Step 2600] Loss: 2.7613\n",
            "[Step 2610] Loss: 3.2476\n",
            "[Step 2620] Loss: 2.8677\n",
            "[Step 2630] Loss: 3.2181\n",
            "[Step 2640] Loss: 2.8132\n",
            "[Step 2650] Loss: 2.8933\n",
            "[Step 2660] Loss: 2.5144\n",
            "[Step 2670] Loss: 2.6449\n",
            "[Step 2680] Loss: 2.9951\n",
            "[Step 2690] Loss: 2.8338\n",
            "[Step 2700] Loss: 3.0706\n",
            "[Step 2710] Loss: 2.7148\n",
            "[Step 2720] Loss: 2.8966\n",
            "[Step 2730] Loss: 2.5367\n",
            "[Step 2740] Loss: 2.8696\n",
            "[Step 2750] Loss: 2.9950\n",
            "[Step 2760] Loss: 2.2827\n",
            "[Step 2770] Loss: 2.9654\n",
            "[Step 2780] Loss: 3.1723\n",
            "[Step 2790] Loss: 2.7619\n",
            "[Step 2800] Loss: 3.0968\n",
            "[Step 2810] Loss: 3.2188\n",
            "[Step 2820] Loss: 3.1326\n",
            "[Step 2830] Loss: 2.7765\n",
            "[Step 2840] Loss: 2.7793\n",
            "[Step 2850] Loss: 2.6247\n",
            "[Step 2860] Loss: 2.4444\n",
            "[Step 2870] Loss: 2.6548\n",
            "[Step 2880] Loss: 3.1554\n",
            "[Step 2890] Loss: 2.8426\n",
            "[Step 2900] Loss: 2.8045\n",
            "[Step 2910] Loss: 2.4994\n",
            "[Step 2920] Loss: 2.8345\n",
            "[Step 2930] Loss: 2.6834\n",
            "[Step 2940] Loss: 2.7666\n",
            "[Step 2950] Loss: 2.8283\n",
            "[Step 2960] Loss: 2.7811\n",
            "[Step 2970] Loss: 3.1425\n",
            "[Step 2980] Loss: 2.7695\n",
            "[Step 2990] Loss: 3.0676\n",
            "[Step 3000] Loss: 2.8000\n",
            "[Step 3010] Loss: 3.0931\n",
            "[Step 3020] Loss: 3.6357\n",
            "[Step 3030] Loss: 2.7181\n",
            "[Step 3040] Loss: 3.1639\n",
            "[Step 3050] Loss: 2.9082\n",
            "[Step 3060] Loss: 3.0395\n",
            "[Step 3070] Loss: 2.7142\n",
            "[Step 3080] Loss: 2.9515\n",
            "[Step 3090] Loss: 2.8291\n",
            "[Step 3100] Loss: 2.9930\n",
            "[Step 3110] Loss: 2.8881\n",
            "[Step 3120] Loss: 2.9398\n",
            "[Step 3130] Loss: 2.7391\n",
            "[Step 3140] Loss: 2.8989\n",
            "[Step 3150] Loss: 2.3335\n",
            "[Step 3160] Loss: 2.6599\n",
            "[Step 3170] Loss: 3.0477\n",
            "[Step 3180] Loss: 2.6082\n",
            "[Step 3190] Loss: 2.8581\n",
            "[Step 3200] Loss: 3.0406\n",
            "[Step 3210] Loss: 2.8735\n",
            "[Step 3220] Loss: 3.2055\n",
            "[Step 3230] Loss: 2.3350\n",
            "[Step 3240] Loss: 2.3385\n",
            "[Step 3250] Loss: 3.1049\n",
            "[Step 3260] Loss: 2.7621\n",
            "[Step 3270] Loss: 2.5708\n",
            "[Step 3280] Loss: 2.5964\n",
            "[Step 3290] Loss: 2.7691\n",
            "[Step 3300] Loss: 2.5177\n",
            "[Step 3310] Loss: 2.8919\n",
            "[Step 3320] Loss: 2.5949\n",
            "[Step 3330] Loss: 2.9583\n",
            "[Step 3340] Loss: 3.0283\n",
            "[Step 3350] Loss: 3.5249\n",
            "[Step 3360] Loss: 3.6335\n",
            "[Step 3370] Loss: 2.7925\n",
            "[Step 3380] Loss: 2.2390\n",
            "[Step 3390] Loss: 2.9046\n",
            "[Step 3400] Loss: 3.1555\n",
            "[Step 3410] Loss: 3.0871\n",
            "[Step 3420] Loss: 2.9169\n",
            "[Step 3430] Loss: 2.8498\n",
            "[Step 3440] Loss: 2.5558\n",
            "[Step 3450] Loss: 3.1578\n",
            "[Step 3460] Loss: 3.0844\n",
            "[Step 3470] Loss: 2.8457\n",
            "[Step 3480] Loss: 2.8971\n",
            "[Step 3490] Loss: 3.0692\n",
            "[Step 3500] Loss: 2.9208\n",
            "[Step 3510] Loss: 2.7662\n",
            "[Step 3520] Loss: 2.5115\n",
            "[Step 3530] Loss: 2.8029\n",
            "[Step 3540] Loss: 2.7489\n",
            "[Step 3550] Loss: 3.0117\n",
            "[Step 3560] Loss: 3.1584\n",
            "[Step 3570] Loss: 2.5821\n",
            "[Step 3580] Loss: 2.8218\n",
            "[Step 3590] Loss: 2.7971\n",
            "[Step 3600] Loss: 2.4223\n",
            "[Step 3610] Loss: 2.8480\n",
            "[Step 3620] Loss: 2.8413\n",
            "[Step 3630] Loss: 2.5918\n",
            "[Step 3640] Loss: 3.0549\n",
            "[Step 3650] Loss: 2.9139\n",
            "[Step 3660] Loss: 2.7948\n",
            "[Step 3670] Loss: 2.9832\n",
            "[Step 3680] Loss: 2.9166\n",
            "[Step 3690] Loss: 2.7441\n",
            "[Step 3700] Loss: 2.9518\n",
            "[Step 3710] Loss: 2.5622\n",
            "[Step 3720] Loss: 2.9107\n",
            "[Step 3730] Loss: 2.6525\n",
            "[Step 3740] Loss: 2.7137\n",
            "[Step 3750] Loss: 2.7946\n",
            "[Step 3760] Loss: 2.9610\n",
            "[Step 3770] Loss: 2.7572\n",
            "[Step 3780] Loss: 3.3722\n",
            "ðŸ“˜ Epoch 42 - Avg Training Loss: 2.8585\n",
            "ðŸ” Validation â€” Loss: 2.8292 | Accuracy: 40.91% | Precision: 40.10%\n",
            "[Step 3790] Loss: 2.7897\n",
            "[Step 3800] Loss: 3.3511\n",
            "[Step 3810] Loss: 2.4504\n",
            "[Step 3820] Loss: 3.0079\n",
            "[Step 3830] Loss: 2.6564\n",
            "[Step 3840] Loss: 3.2083\n",
            "[Step 3850] Loss: 2.8518\n",
            "[Step 3860] Loss: 2.9199\n",
            "[Step 3870] Loss: 2.7140\n",
            "[Step 3880] Loss: 2.9652\n",
            "[Step 3890] Loss: 2.8855\n",
            "[Step 3900] Loss: 2.9908\n",
            "[Step 3910] Loss: 2.6793\n",
            "[Step 3920] Loss: 2.8020\n",
            "[Step 3930] Loss: 2.9955\n",
            "[Step 3940] Loss: 2.7002\n",
            "[Step 3950] Loss: 2.8405\n",
            "[Step 3960] Loss: 2.9301\n",
            "[Step 3970] Loss: 2.6521\n",
            "[Step 3980] Loss: 2.6383\n",
            "[Step 3990] Loss: 2.7681\n",
            "[Step 4000] Loss: 2.7289\n",
            "[Step 4010] Loss: 2.8805\n",
            "[Step 4020] Loss: 2.7715\n",
            "[Step 4030] Loss: 3.1054\n",
            "[Step 4040] Loss: 3.1840\n",
            "[Step 4050] Loss: 2.7488\n",
            "[Step 4060] Loss: 2.6960\n",
            "[Step 4070] Loss: 2.6639\n",
            "[Step 4080] Loss: 3.2845\n",
            "[Step 4090] Loss: 3.1538\n",
            "[Step 4100] Loss: 2.6955\n",
            "[Step 4110] Loss: 2.9421\n",
            "[Step 4120] Loss: 2.8194\n",
            "[Step 4130] Loss: 2.8624\n",
            "[Step 4140] Loss: 2.7828\n",
            "[Step 4150] Loss: 2.8249\n",
            "[Step 4160] Loss: 2.8370\n",
            "[Step 4170] Loss: 3.1138\n",
            "[Step 4180] Loss: 3.0189\n",
            "[Step 4190] Loss: 2.6720\n",
            "[Step 4200] Loss: 2.7643\n",
            "[Step 4210] Loss: 3.0321\n",
            "[Step 4220] Loss: 2.7392\n",
            "[Step 4230] Loss: 2.9437\n",
            "[Step 4240] Loss: 2.9770\n",
            "[Step 4250] Loss: 2.6023\n",
            "[Step 4260] Loss: 2.7837\n",
            "[Step 4270] Loss: 2.9499\n",
            "[Step 4280] Loss: 2.4951\n",
            "[Step 4290] Loss: 2.8052\n",
            "[Step 4300] Loss: 3.0434\n",
            "[Step 4310] Loss: 2.9045\n",
            "[Step 4320] Loss: 2.8048\n",
            "[Step 4330] Loss: 2.6621\n",
            "[Step 4340] Loss: 2.7141\n",
            "[Step 4350] Loss: 2.9612\n",
            "[Step 4360] Loss: 2.7718\n",
            "[Step 4370] Loss: 2.6797\n",
            "[Step 4380] Loss: 2.5938\n",
            "[Step 4390] Loss: 2.9164\n",
            "[Step 4400] Loss: 2.9978\n",
            "[Step 4410] Loss: 2.7041\n",
            "[Step 4420] Loss: 2.8988\n",
            "[Step 4430] Loss: 2.8716\n",
            "[Step 4440] Loss: 2.5989\n",
            "[Step 4450] Loss: 2.7767\n",
            "[Step 4460] Loss: 2.7646\n",
            "[Step 4470] Loss: 3.3228\n",
            "[Step 4480] Loss: 2.6380\n",
            "[Step 4490] Loss: 2.5222\n",
            "[Step 4500] Loss: 3.0457\n",
            "[Step 4510] Loss: 2.9559\n",
            "[Step 4520] Loss: 2.6701\n",
            "[Step 4530] Loss: 3.0793\n",
            "[Step 4540] Loss: 2.8712\n",
            "[Step 4550] Loss: 2.3734\n",
            "[Step 4560] Loss: 2.8874\n",
            "[Step 4570] Loss: 2.7653\n",
            "[Step 4580] Loss: 3.3297\n",
            "[Step 4590] Loss: 2.8148\n",
            "[Step 4600] Loss: 2.5460\n",
            "[Step 4610] Loss: 2.9167\n",
            "[Step 4620] Loss: 3.0996\n",
            "[Step 4630] Loss: 2.7873\n",
            "[Step 4640] Loss: 2.5388\n",
            "[Step 4650] Loss: 2.9197\n",
            "[Step 4660] Loss: 2.9584\n",
            "[Step 4670] Loss: 3.0165\n",
            "[Step 4680] Loss: 3.0068\n",
            "[Step 4690] Loss: 3.1016\n",
            "[Step 4700] Loss: 2.8015\n",
            "[Step 4710] Loss: 2.8885\n",
            "[Step 4720] Loss: 2.7896\n",
            "[Step 4730] Loss: 3.2877\n",
            "[Step 4740] Loss: 2.6543\n",
            "[Step 4750] Loss: 2.9946\n",
            "[Step 4760] Loss: 2.7377\n",
            "[Step 4770] Loss: 2.7023\n",
            "[Step 4780] Loss: 2.9376\n",
            "[Step 4790] Loss: 2.5566\n",
            "[Step 4800] Loss: 3.0685\n",
            "[Step 4810] Loss: 3.0504\n",
            "[Step 4820] Loss: 2.7262\n",
            "[Step 4830] Loss: 3.1351\n",
            "[Step 4840] Loss: 2.4251\n",
            "[Step 4850] Loss: 2.9001\n",
            "[Step 4860] Loss: 2.9009\n",
            "[Step 4870] Loss: 2.5627\n",
            "[Step 4880] Loss: 2.5706\n",
            "[Step 4890] Loss: 2.9208\n",
            "[Step 4900] Loss: 2.6409\n",
            "[Step 4910] Loss: 2.7412\n",
            "[Step 4920] Loss: 2.3930\n",
            "[Step 4930] Loss: 2.3002\n",
            "[Step 4940] Loss: 2.8237\n",
            "[Step 4950] Loss: 2.8596\n",
            "[Step 4960] Loss: 2.7866\n",
            "[Step 4970] Loss: 3.0644\n",
            "[Step 4980] Loss: 2.8744\n",
            "[Step 4990] Loss: 2.8842\n",
            "[Step 5000] Loss: 2.5444\n",
            "[Step 5010] Loss: 3.0710\n",
            "[Step 5020] Loss: 3.0951\n",
            "[Step 5030] Loss: 2.9827\n",
            "[Step 5040] Loss: 3.0736\n",
            "[Step 5050] Loss: 3.1602\n",
            "[Step 5060] Loss: 2.6211\n",
            "[Step 5070] Loss: 2.6808\n",
            "[Step 5080] Loss: 3.0213\n",
            "[Step 5090] Loss: 2.8748\n",
            "[Step 5100] Loss: 2.3852\n",
            "[Step 5110] Loss: 2.8072\n",
            "[Step 5120] Loss: 2.9453\n",
            "[Step 5130] Loss: 3.3225\n",
            "[Step 5140] Loss: 2.7124\n",
            "[Step 5150] Loss: 2.6377\n",
            "[Step 5160] Loss: 3.1493\n",
            "[Step 5170] Loss: 2.7942\n",
            "[Step 5180] Loss: 2.4439\n",
            "[Step 5190] Loss: 2.8862\n",
            "[Step 5200] Loss: 2.9636\n",
            "[Step 5210] Loss: 3.4762\n",
            "[Step 5220] Loss: 2.9824\n",
            "[Step 5230] Loss: 2.7051\n",
            "[Step 5240] Loss: 3.1860\n",
            "[Step 5250] Loss: 3.0903\n",
            "[Step 5260] Loss: 3.1123\n",
            "[Step 5270] Loss: 3.0418\n",
            "[Step 5280] Loss: 3.0714\n",
            "[Step 5290] Loss: 2.5531\n",
            "[Step 5300] Loss: 3.0241\n",
            "[Step 5310] Loss: 2.9070\n",
            "[Step 5320] Loss: 2.6951\n",
            "[Step 5330] Loss: 2.4911\n",
            "[Step 5340] Loss: 2.8581\n",
            "[Step 5350] Loss: 2.9407\n",
            "[Step 5360] Loss: 2.8238\n",
            "[Step 5370] Loss: 3.4432\n",
            "[Step 5380] Loss: 2.6715\n",
            "[Step 5390] Loss: 2.8625\n",
            "[Step 5400] Loss: 2.6504\n",
            "[Step 5410] Loss: 2.5225\n",
            "[Step 5420] Loss: 2.6468\n",
            "[Step 5430] Loss: 2.5989\n",
            "[Step 5440] Loss: 3.0331\n",
            "[Step 5450] Loss: 2.7060\n",
            "[Step 5460] Loss: 2.6724\n",
            "[Step 5470] Loss: 2.7864\n",
            "[Step 5480] Loss: 2.7039\n",
            "[Step 5490] Loss: 3.3229\n",
            "[Step 5500] Loss: 2.8193\n",
            "[Step 5510] Loss: 2.6057\n",
            "[Step 5520] Loss: 3.0231\n",
            "[Step 5530] Loss: 2.7840\n",
            "[Step 5540] Loss: 2.8109\n",
            "[Step 5550] Loss: 2.9244\n",
            "[Step 5560] Loss: 3.0607\n",
            "[Step 5570] Loss: 2.9326\n",
            "[Step 5580] Loss: 2.7767\n",
            "[Step 5590] Loss: 2.9851\n",
            "[Step 5600] Loss: 3.0884\n",
            "[Step 5610] Loss: 2.7451\n",
            "[Step 5620] Loss: 2.6728\n",
            "[Step 5630] Loss: 3.4272\n",
            "[Step 5640] Loss: 2.9600\n",
            "[Step 5650] Loss: 2.2338\n",
            "[Step 5660] Loss: 2.7983\n",
            "[Step 5670] Loss: 2.8566\n",
            "[Step 5680] Loss: 3.2581\n",
            "ðŸ“˜ Epoch 43 - Avg Training Loss: 2.8329\n",
            "ðŸ” Validation â€” Loss: 2.8112 | Accuracy: 41.67% | Precision: 40.99%\n",
            "[Step 5690] Loss: 2.6271\n",
            "[Step 5700] Loss: 3.1991\n",
            "[Step 5710] Loss: 2.9748\n",
            "[Step 5720] Loss: 2.5881\n",
            "[Step 5730] Loss: 3.2321\n",
            "[Step 5740] Loss: 2.5545\n",
            "[Step 5750] Loss: 3.0743\n",
            "[Step 5760] Loss: 2.8612\n",
            "[Step 5770] Loss: 2.8257\n",
            "[Step 5780] Loss: 2.5333\n",
            "[Step 5790] Loss: 2.8892\n",
            "[Step 5800] Loss: 2.8075\n",
            "[Step 5810] Loss: 2.9774\n",
            "[Step 5820] Loss: 2.9010\n",
            "[Step 5830] Loss: 2.6493\n",
            "[Step 5840] Loss: 2.8519\n",
            "[Step 5850] Loss: 2.8500\n",
            "[Step 5860] Loss: 2.8717\n",
            "[Step 5870] Loss: 2.7821\n",
            "[Step 5880] Loss: 2.9996\n",
            "[Step 5890] Loss: 2.8665\n",
            "[Step 5900] Loss: 2.6454\n",
            "[Step 5910] Loss: 2.5528\n",
            "[Step 5920] Loss: 2.3969\n",
            "[Step 5930] Loss: 2.3474\n",
            "[Step 5940] Loss: 2.7866\n",
            "[Step 5950] Loss: 3.0667\n",
            "[Step 5960] Loss: 3.1854\n",
            "[Step 5970] Loss: 2.6050\n",
            "[Step 5980] Loss: 2.8422\n",
            "[Step 5990] Loss: 2.9254\n",
            "[Step 6000] Loss: 2.4263\n",
            "[Step 6010] Loss: 2.7413\n",
            "[Step 6020] Loss: 2.4536\n",
            "[Step 6030] Loss: 2.7250\n",
            "[Step 6040] Loss: 2.9875\n",
            "[Step 6050] Loss: 2.7757\n",
            "[Step 6060] Loss: 2.7699\n",
            "[Step 6070] Loss: 2.6200\n",
            "[Step 6080] Loss: 2.7270\n",
            "[Step 6090] Loss: 2.6008\n",
            "[Step 6100] Loss: 2.5594\n",
            "[Step 6110] Loss: 2.8102\n",
            "[Step 6120] Loss: 2.8474\n",
            "[Step 6130] Loss: 2.8463\n",
            "[Step 6140] Loss: 2.6526\n",
            "[Step 6150] Loss: 3.1164\n",
            "[Step 6160] Loss: 2.6244\n",
            "[Step 6170] Loss: 2.8456\n",
            "[Step 6180] Loss: 2.6906\n",
            "[Step 6190] Loss: 2.7349\n",
            "[Step 6200] Loss: 3.2450\n",
            "[Step 6210] Loss: 2.5075\n",
            "[Step 6220] Loss: 3.4080\n",
            "[Step 6230] Loss: 2.5382\n",
            "[Step 6240] Loss: 3.0102\n",
            "[Step 6250] Loss: 2.7894\n",
            "[Step 6260] Loss: 2.6458\n",
            "[Step 6270] Loss: 2.6926\n",
            "[Step 6280] Loss: 2.7396\n",
            "[Step 6290] Loss: 3.0184\n",
            "[Step 6300] Loss: 2.7674\n",
            "[Step 6310] Loss: 2.8320\n",
            "[Step 6320] Loss: 2.7386\n",
            "[Step 6330] Loss: 2.6692\n",
            "[Step 6340] Loss: 2.9486\n",
            "[Step 6350] Loss: 2.7936\n",
            "[Step 6360] Loss: 2.3871\n",
            "[Step 6370] Loss: 2.3895\n",
            "[Step 6380] Loss: 3.2181\n",
            "[Step 6390] Loss: 2.8873\n",
            "[Step 6400] Loss: 2.5527\n",
            "[Step 6410] Loss: 3.0426\n",
            "[Step 6420] Loss: 2.7289\n",
            "[Step 6430] Loss: 2.6790\n",
            "[Step 6440] Loss: 2.7452\n",
            "[Step 6450] Loss: 2.9899\n",
            "[Step 6460] Loss: 3.1133\n",
            "[Step 6470] Loss: 2.7979\n",
            "[Step 6480] Loss: 2.5918\n",
            "[Step 6490] Loss: 2.4384\n",
            "[Step 6500] Loss: 2.7397\n",
            "[Step 6510] Loss: 2.9997\n",
            "[Step 6520] Loss: 2.8869\n",
            "[Step 6530] Loss: 2.8773\n",
            "[Step 6540] Loss: 2.5448\n",
            "[Step 6550] Loss: 2.6010\n",
            "[Step 6560] Loss: 2.7759\n",
            "[Step 6570] Loss: 2.6646\n",
            "[Step 6580] Loss: 2.7299\n",
            "[Step 6590] Loss: 2.9132\n",
            "[Step 6600] Loss: 2.6254\n",
            "[Step 6610] Loss: 2.7669\n",
            "[Step 6620] Loss: 2.9463\n",
            "[Step 6630] Loss: 2.7766\n",
            "[Step 6640] Loss: 2.7543\n",
            "[Step 6650] Loss: 2.9700\n",
            "[Step 6660] Loss: 2.4861\n",
            "[Step 6670] Loss: 2.7355\n",
            "[Step 6680] Loss: 2.7939\n",
            "[Step 6690] Loss: 2.3051\n",
            "[Step 6700] Loss: 3.0309\n",
            "[Step 6710] Loss: 2.4632\n",
            "[Step 6720] Loss: 2.9632\n",
            "[Step 6730] Loss: 2.7839\n",
            "[Step 6740] Loss: 2.4067\n",
            "[Step 6750] Loss: 2.9013\n",
            "[Step 6760] Loss: 3.0087\n",
            "[Step 6770] Loss: 2.7267\n",
            "[Step 6780] Loss: 3.0849\n",
            "[Step 6790] Loss: 2.7428\n",
            "[Step 6800] Loss: 3.0833\n",
            "[Step 6810] Loss: 2.6002\n",
            "[Step 6820] Loss: 2.7375\n",
            "[Step 6830] Loss: 2.6218\n",
            "[Step 6840] Loss: 2.7629\n",
            "[Step 6850] Loss: 2.7060\n",
            "[Step 6860] Loss: 3.0155\n",
            "[Step 6870] Loss: 2.7924\n",
            "[Step 6880] Loss: 2.5830\n",
            "[Step 6890] Loss: 2.9633\n",
            "[Step 6900] Loss: 2.8330\n",
            "[Step 6910] Loss: 2.6925\n",
            "[Step 6920] Loss: 3.1535\n",
            "[Step 6930] Loss: 3.0034\n",
            "[Step 6940] Loss: 2.5892\n",
            "[Step 6950] Loss: 2.9721\n",
            "[Step 6960] Loss: 2.5553\n",
            "[Step 6970] Loss: 2.7234\n",
            "[Step 6980] Loss: 3.1099\n",
            "[Step 6990] Loss: 2.8930\n",
            "[Step 7000] Loss: 2.5595\n",
            "[Step 7010] Loss: 2.9403\n",
            "[Step 7020] Loss: 2.7766\n",
            "[Step 7030] Loss: 2.5630\n",
            "[Step 7040] Loss: 2.9036\n",
            "[Step 7050] Loss: 3.0065\n",
            "[Step 7060] Loss: 2.6542\n",
            "[Step 7070] Loss: 2.8121\n",
            "[Step 7080] Loss: 3.0366\n",
            "[Step 7090] Loss: 2.5929\n",
            "[Step 7100] Loss: 2.9038\n",
            "[Step 7110] Loss: 2.5693\n",
            "[Step 7120] Loss: 2.2073\n",
            "[Step 7130] Loss: 2.6535\n",
            "[Step 7140] Loss: 2.7583\n",
            "[Step 7150] Loss: 2.6006\n",
            "[Step 7160] Loss: 3.0290\n",
            "[Step 7170] Loss: 2.2548\n",
            "[Step 7180] Loss: 3.0618\n",
            "[Step 7190] Loss: 3.2928\n",
            "[Step 7200] Loss: 2.7947\n",
            "[Step 7210] Loss: 2.8358\n",
            "[Step 7220] Loss: 3.0353\n",
            "[Step 7230] Loss: 3.2404\n",
            "[Step 7240] Loss: 2.7342\n",
            "[Step 7250] Loss: 2.6706\n",
            "[Step 7260] Loss: 2.8040\n",
            "[Step 7270] Loss: 2.6154\n",
            "[Step 7280] Loss: 2.6371\n",
            "[Step 7290] Loss: 2.5304\n",
            "[Step 7300] Loss: 3.2484\n",
            "[Step 7310] Loss: 2.7406\n",
            "[Step 7320] Loss: 3.3561\n",
            "[Step 7330] Loss: 2.8411\n",
            "[Step 7340] Loss: 2.8243\n",
            "[Step 7350] Loss: 2.6778\n",
            "[Step 7360] Loss: 2.9473\n",
            "[Step 7370] Loss: 2.7364\n",
            "[Step 7380] Loss: 2.1929\n",
            "[Step 7390] Loss: 2.7498\n",
            "[Step 7400] Loss: 3.2289\n",
            "[Step 7410] Loss: 2.7267\n",
            "[Step 7420] Loss: 2.8427\n",
            "[Step 7430] Loss: 2.7011\n",
            "[Step 7440] Loss: 2.9030\n",
            "[Step 7450] Loss: 2.9300\n",
            "[Step 7460] Loss: 2.9873\n",
            "[Step 7470] Loss: 2.5794\n",
            "[Step 7480] Loss: 3.1103\n",
            "[Step 7490] Loss: 2.8317\n",
            "[Step 7500] Loss: 2.7630\n",
            "[Step 7510] Loss: 2.6732\n",
            "[Step 7520] Loss: 2.6017\n",
            "[Step 7530] Loss: 2.8851\n",
            "[Step 7540] Loss: 2.9566\n",
            "[Step 7550] Loss: 2.9562\n",
            "[Step 7560] Loss: 2.6276\n",
            "[Step 7570] Loss: 3.0245\n",
            "ðŸ“˜ Epoch 44 - Avg Training Loss: 2.8127\n",
            "ðŸ” Validation â€” Loss: 2.7914 | Accuracy: 41.91% | Precision: 40.92%\n",
            "[Step 7580] Loss: 2.6582\n",
            "[Step 7590] Loss: 2.8879\n",
            "[Step 7600] Loss: 2.8314\n",
            "[Step 7610] Loss: 2.6992\n",
            "[Step 7620] Loss: 2.8672\n",
            "[Step 7630] Loss: 2.8058\n",
            "[Step 7640] Loss: 2.7329\n",
            "[Step 7650] Loss: 2.7960\n",
            "[Step 7660] Loss: 2.6660\n",
            "[Step 7670] Loss: 2.7275\n",
            "[Step 7680] Loss: 2.6002\n",
            "[Step 7690] Loss: 2.7671\n",
            "[Step 7700] Loss: 2.8307\n",
            "[Step 7710] Loss: 3.2936\n",
            "[Step 7720] Loss: 2.5754\n",
            "[Step 7730] Loss: 3.1786\n",
            "[Step 7740] Loss: 2.5756\n",
            "[Step 7750] Loss: 2.9238\n",
            "[Step 7760] Loss: 2.7703\n",
            "[Step 7770] Loss: 2.6685\n",
            "[Step 7780] Loss: 3.0435\n",
            "[Step 7790] Loss: 2.9931\n",
            "[Step 7800] Loss: 2.8816\n",
            "[Step 7810] Loss: 2.8442\n",
            "[Step 7820] Loss: 2.7350\n",
            "[Step 7830] Loss: 2.9837\n",
            "[Step 7840] Loss: 2.7702\n",
            "[Step 7850] Loss: 2.6859\n",
            "[Step 7860] Loss: 3.1266\n",
            "[Step 7870] Loss: 3.3143\n",
            "[Step 7880] Loss: 2.7793\n",
            "[Step 7890] Loss: 3.0405\n",
            "[Step 7900] Loss: 3.0375\n",
            "[Step 7910] Loss: 2.8512\n",
            "[Step 7920] Loss: 2.8157\n",
            "[Step 7930] Loss: 2.9505\n",
            "[Step 7940] Loss: 3.2572\n",
            "[Step 7950] Loss: 2.7139\n",
            "[Step 7960] Loss: 2.8206\n",
            "[Step 7970] Loss: 2.9095\n",
            "[Step 7980] Loss: 2.5386\n",
            "[Step 7990] Loss: 2.7394\n",
            "[Step 8000] Loss: 2.8271\n",
            "[Step 8010] Loss: 2.4129\n",
            "[Step 8020] Loss: 2.7283\n",
            "[Step 8030] Loss: 3.0450\n",
            "[Step 8040] Loss: 2.6149\n",
            "[Step 8050] Loss: 2.3582\n",
            "[Step 8060] Loss: 2.7003\n",
            "[Step 8070] Loss: 2.4826\n",
            "[Step 8080] Loss: 2.6441\n",
            "[Step 8090] Loss: 2.5320\n",
            "[Step 8100] Loss: 2.7575\n",
            "[Step 8110] Loss: 2.6558\n",
            "[Step 8120] Loss: 2.9499\n",
            "[Step 8130] Loss: 2.4808\n",
            "[Step 8140] Loss: 2.7729\n",
            "[Step 8150] Loss: 3.0716\n",
            "[Step 8160] Loss: 3.0295\n",
            "[Step 8170] Loss: 2.5368\n",
            "[Step 8180] Loss: 3.2664\n",
            "[Step 8190] Loss: 3.3837\n",
            "[Step 8200] Loss: 2.9344\n",
            "[Step 8210] Loss: 2.8006\n",
            "[Step 8220] Loss: 2.5845\n",
            "[Step 8230] Loss: 3.0296\n",
            "[Step 8240] Loss: 2.7937\n",
            "[Step 8250] Loss: 2.5040\n",
            "[Step 8260] Loss: 2.3269\n",
            "[Step 8270] Loss: 2.7907\n",
            "[Step 8280] Loss: 2.7346\n",
            "[Step 8290] Loss: 2.9928\n",
            "[Step 8300] Loss: 2.7195\n",
            "[Step 8310] Loss: 2.6282\n",
            "[Step 8320] Loss: 2.8307\n",
            "[Step 8330] Loss: 2.9320\n",
            "[Step 8340] Loss: 2.6731\n",
            "[Step 8350] Loss: 2.6811\n",
            "[Step 8360] Loss: 2.7027\n",
            "[Step 8370] Loss: 3.3119\n",
            "[Step 8380] Loss: 3.0684\n",
            "[Step 8390] Loss: 2.4167\n",
            "[Step 8400] Loss: 2.9062\n",
            "[Step 8410] Loss: 3.1899\n",
            "[Step 8420] Loss: 2.4314\n",
            "[Step 8430] Loss: 2.5676\n",
            "[Step 8440] Loss: 2.5600\n",
            "[Step 8450] Loss: 2.7031\n",
            "[Step 8460] Loss: 3.2733\n",
            "[Step 8470] Loss: 2.8259\n",
            "[Step 8480] Loss: 2.7407\n",
            "[Step 8490] Loss: 2.8369\n",
            "[Step 8500] Loss: 3.1775\n",
            "[Step 8510] Loss: 3.0650\n",
            "[Step 8520] Loss: 2.9379\n",
            "[Step 8530] Loss: 3.3700\n",
            "[Step 8540] Loss: 2.5667\n",
            "[Step 8550] Loss: 2.8156\n",
            "[Step 8560] Loss: 2.9037\n",
            "[Step 8570] Loss: 2.7456\n",
            "[Step 8580] Loss: 2.3556\n",
            "[Step 8590] Loss: 3.1166\n",
            "[Step 8600] Loss: 2.7066\n",
            "[Step 8610] Loss: 2.6221\n",
            "[Step 8620] Loss: 2.5871\n",
            "[Step 8630] Loss: 2.5355\n",
            "[Step 8640] Loss: 2.7056\n",
            "[Step 8650] Loss: 2.3715\n",
            "[Step 8660] Loss: 2.7722\n",
            "[Step 8670] Loss: 3.0109\n",
            "[Step 8680] Loss: 2.8563\n",
            "[Step 8690] Loss: 2.5272\n",
            "[Step 8700] Loss: 2.9154\n",
            "[Step 8710] Loss: 2.8194\n",
            "[Step 8720] Loss: 3.1727\n",
            "[Step 8730] Loss: 3.0113\n",
            "[Step 8740] Loss: 2.6100\n",
            "[Step 8750] Loss: 2.7639\n",
            "[Step 8760] Loss: 2.7222\n",
            "[Step 8770] Loss: 2.4621\n",
            "[Step 8780] Loss: 3.2064\n",
            "[Step 8790] Loss: 2.8909\n",
            "[Step 8800] Loss: 2.6879\n",
            "[Step 8810] Loss: 2.7360\n",
            "[Step 8820] Loss: 2.7336\n",
            "[Step 8830] Loss: 2.6818\n",
            "[Step 8840] Loss: 2.9955\n",
            "[Step 8850] Loss: 2.9969\n",
            "[Step 8860] Loss: 2.6698\n",
            "[Step 8870] Loss: 2.7062\n",
            "[Step 8880] Loss: 2.6869\n",
            "[Step 8890] Loss: 2.8243\n",
            "[Step 8900] Loss: 2.6603\n",
            "[Step 8910] Loss: 2.8191\n",
            "[Step 8920] Loss: 2.6010\n",
            "[Step 8930] Loss: 2.5436\n",
            "[Step 8940] Loss: 3.0375\n",
            "[Step 8950] Loss: 2.9812\n",
            "[Step 8960] Loss: 2.8034\n",
            "[Step 8970] Loss: 2.6505\n",
            "[Step 8980] Loss: 3.0255\n",
            "[Step 8990] Loss: 2.7798\n",
            "[Step 9000] Loss: 3.0893\n",
            "[Step 9010] Loss: 3.1009\n",
            "[Step 9020] Loss: 3.0947\n",
            "[Step 9030] Loss: 2.9753\n",
            "[Step 9040] Loss: 2.8762\n",
            "[Step 9050] Loss: 2.6400\n",
            "[Step 9060] Loss: 3.0102\n",
            "[Step 9070] Loss: 2.5562\n",
            "[Step 9080] Loss: 2.7699\n",
            "[Step 9090] Loss: 2.6032\n",
            "[Step 9100] Loss: 2.7087\n",
            "[Step 9110] Loss: 2.5340\n",
            "[Step 9120] Loss: 2.6028\n",
            "[Step 9130] Loss: 2.4955\n",
            "[Step 9140] Loss: 2.9442\n",
            "[Step 9150] Loss: 2.7952\n",
            "[Step 9160] Loss: 2.5409\n",
            "[Step 9170] Loss: 2.5067\n",
            "[Step 9180] Loss: 2.6172\n",
            "[Step 9190] Loss: 2.6461\n",
            "[Step 9200] Loss: 3.1399\n",
            "[Step 9210] Loss: 2.9205\n",
            "[Step 9220] Loss: 2.4680\n",
            "[Step 9230] Loss: 2.5125\n",
            "[Step 9240] Loss: 3.0459\n",
            "[Step 9250] Loss: 2.2165\n",
            "[Step 9260] Loss: 2.6582\n",
            "[Step 9270] Loss: 3.1250\n",
            "[Step 9280] Loss: 2.6941\n",
            "[Step 9290] Loss: 3.0316\n",
            "[Step 9300] Loss: 2.5206\n",
            "[Step 9310] Loss: 3.1858\n",
            "[Step 9320] Loss: 2.6236\n",
            "[Step 9330] Loss: 2.9610\n",
            "[Step 9340] Loss: 2.7317\n",
            "[Step 9350] Loss: 2.4663\n",
            "[Step 9360] Loss: 2.5963\n",
            "[Step 9370] Loss: 3.0060\n",
            "[Step 9380] Loss: 2.3873\n",
            "[Step 9390] Loss: 2.8114\n",
            "[Step 9400] Loss: 2.9303\n",
            "[Step 9410] Loss: 3.0460\n",
            "[Step 9420] Loss: 2.4913\n",
            "[Step 9430] Loss: 2.7167\n",
            "[Step 9440] Loss: 2.7672\n",
            "[Step 9450] Loss: 2.9970\n",
            "[Step 9460] Loss: 2.8489\n",
            "[Step 9470] Loss: 3.1211\n",
            "ðŸ“˜ Epoch 45 - Avg Training Loss: 2.7936\n",
            "ðŸ” Validation â€” Loss: 2.7762 | Accuracy: 42.50% | Precision: 40.91%\n",
            "[Step 9480] Loss: 2.7414\n",
            "[Step 9490] Loss: 2.7484\n",
            "[Step 9500] Loss: 2.7751\n",
            "[Step 9510] Loss: 2.8368\n",
            "[Step 9520] Loss: 2.9194\n",
            "[Step 9530] Loss: 2.4902\n",
            "[Step 9540] Loss: 2.5022\n",
            "[Step 9550] Loss: 3.0355\n",
            "[Step 9560] Loss: 2.5178\n",
            "[Step 9570] Loss: 2.7436\n",
            "[Step 9580] Loss: 3.2251\n",
            "[Step 9590] Loss: 2.8748\n",
            "[Step 9600] Loss: 3.1303\n",
            "[Step 9610] Loss: 2.7945\n",
            "[Step 9620] Loss: 2.8169\n",
            "[Step 9630] Loss: 2.9692\n",
            "[Step 9640] Loss: 3.0634\n",
            "[Step 9650] Loss: 2.8112\n",
            "[Step 9660] Loss: 2.6606\n",
            "[Step 9670] Loss: 2.6306\n",
            "[Step 9680] Loss: 2.7615\n",
            "[Step 9690] Loss: 2.8372\n",
            "[Step 9700] Loss: 2.9285\n",
            "[Step 9710] Loss: 2.4891\n",
            "[Step 9720] Loss: 2.8690\n",
            "[Step 9730] Loss: 2.6379\n",
            "[Step 9740] Loss: 2.5575\n",
            "[Step 9750] Loss: 2.7637\n",
            "[Step 9760] Loss: 2.8851\n",
            "[Step 9770] Loss: 2.9048\n",
            "[Step 9780] Loss: 2.5846\n",
            "[Step 9790] Loss: 2.7093\n",
            "[Step 9800] Loss: 2.4799\n",
            "[Step 9810] Loss: 2.7851\n",
            "[Step 9820] Loss: 2.7825\n",
            "[Step 9830] Loss: 2.5247\n",
            "[Step 9840] Loss: 2.7753\n",
            "[Step 9850] Loss: 3.1341\n",
            "[Step 9860] Loss: 2.6567\n",
            "[Step 9870] Loss: 2.8697\n",
            "[Step 9880] Loss: 3.3598\n",
            "[Step 9890] Loss: 2.6753\n",
            "[Step 9900] Loss: 3.1471\n",
            "[Step 9910] Loss: 2.4416\n",
            "[Step 9920] Loss: 2.8040\n",
            "[Step 9930] Loss: 2.8539\n",
            "[Step 9940] Loss: 2.6828\n",
            "[Step 9950] Loss: 2.7851\n",
            "[Step 9960] Loss: 2.5746\n",
            "[Step 9970] Loss: 2.9476\n",
            "[Step 9980] Loss: 3.0773\n",
            "[Step 9990] Loss: 2.8205\n",
            "[Step 10000] Loss: 3.2021\n",
            "[Step 10010] Loss: 2.9770\n",
            "[Step 10020] Loss: 2.6166\n",
            "[Step 10030] Loss: 2.5918\n",
            "[Step 10040] Loss: 2.6487\n",
            "[Step 10050] Loss: 2.7168\n",
            "[Step 10060] Loss: 2.9710\n",
            "[Step 10070] Loss: 3.0904\n",
            "[Step 10080] Loss: 2.8254\n",
            "[Step 10090] Loss: 2.9618\n",
            "[Step 10100] Loss: 2.5908\n",
            "[Step 10110] Loss: 2.7677\n",
            "[Step 10120] Loss: 2.8387\n",
            "[Step 10130] Loss: 2.9493\n",
            "[Step 10140] Loss: 2.8287\n",
            "[Step 10150] Loss: 2.8531\n",
            "[Step 10160] Loss: 3.0496\n",
            "[Step 10170] Loss: 2.6535\n",
            "[Step 10180] Loss: 2.7705\n",
            "[Step 10190] Loss: 2.5692\n",
            "[Step 10200] Loss: 3.2236\n",
            "[Step 10210] Loss: 3.0614\n",
            "[Step 10220] Loss: 3.2034\n",
            "[Step 10230] Loss: 2.8188\n",
            "[Step 10240] Loss: 2.8147\n",
            "[Step 10250] Loss: 2.8080\n",
            "[Step 10260] Loss: 2.7889\n",
            "[Step 10270] Loss: 3.0912\n",
            "[Step 10280] Loss: 2.7592\n",
            "[Step 10290] Loss: 2.5328\n",
            "[Step 10300] Loss: 2.9612\n",
            "[Step 10310] Loss: 2.8613\n",
            "[Step 10320] Loss: 2.5148\n",
            "[Step 10330] Loss: 2.8139\n",
            "[Step 10340] Loss: 2.9141\n",
            "[Step 10350] Loss: 2.8167\n",
            "[Step 10360] Loss: 2.8052\n",
            "[Step 10370] Loss: 3.0296\n",
            "[Step 10380] Loss: 2.8196\n",
            "[Step 10390] Loss: 3.1375\n",
            "[Step 10400] Loss: 2.8305\n",
            "[Step 10410] Loss: 2.8483\n",
            "[Step 10420] Loss: 2.6692\n",
            "[Step 10430] Loss: 2.6866\n",
            "[Step 10440] Loss: 2.6469\n",
            "[Step 10450] Loss: 3.1307\n",
            "[Step 10460] Loss: 2.7211\n",
            "[Step 10470] Loss: 3.1776\n",
            "[Step 10480] Loss: 2.6337\n",
            "[Step 10490] Loss: 2.8031\n",
            "[Step 10500] Loss: 3.0329\n",
            "[Step 10510] Loss: 2.9674\n",
            "[Step 10520] Loss: 3.1533\n",
            "[Step 10530] Loss: 2.4934\n",
            "[Step 10540] Loss: 2.9520\n",
            "[Step 10550] Loss: 2.6660\n",
            "[Step 10560] Loss: 2.9535\n",
            "[Step 10570] Loss: 3.1605\n",
            "[Step 10580] Loss: 2.4761\n",
            "[Step 10590] Loss: 3.1323\n",
            "[Step 10600] Loss: 2.8364\n",
            "[Step 10610] Loss: 2.7044\n",
            "[Step 10620] Loss: 2.7998\n",
            "[Step 10630] Loss: 2.7906\n",
            "[Step 10640] Loss: 2.9701\n",
            "[Step 10650] Loss: 2.9274\n",
            "[Step 10660] Loss: 2.8784\n",
            "[Step 10670] Loss: 2.9739\n",
            "[Step 10680] Loss: 3.0713\n",
            "[Step 10690] Loss: 3.1135\n",
            "[Step 10700] Loss: 2.7865\n",
            "[Step 10710] Loss: 3.0046\n",
            "[Step 10720] Loss: 3.0007\n",
            "[Step 10730] Loss: 2.5789\n",
            "[Step 10740] Loss: 2.6787\n",
            "[Step 10750] Loss: 2.2911\n",
            "[Step 10760] Loss: 2.7396\n",
            "[Step 10770] Loss: 3.3442\n",
            "[Step 10780] Loss: 2.3948\n",
            "[Step 10790] Loss: 2.7869\n",
            "[Step 10800] Loss: 2.9038\n",
            "[Step 10810] Loss: 3.2835\n",
            "[Step 10820] Loss: 3.1528\n",
            "[Step 10830] Loss: 3.2958\n",
            "[Step 10840] Loss: 2.5782\n",
            "[Step 10850] Loss: 2.5728\n",
            "[Step 10860] Loss: 3.0540\n",
            "[Step 10870] Loss: 2.9680\n",
            "[Step 10880] Loss: 3.1158\n",
            "[Step 10890] Loss: 2.8810\n",
            "[Step 10900] Loss: 2.8091\n",
            "[Step 10910] Loss: 2.7251\n",
            "[Step 10920] Loss: 2.8858\n",
            "[Step 10930] Loss: 2.8035\n",
            "[Step 10940] Loss: 2.7843\n",
            "[Step 10950] Loss: 2.8370\n",
            "[Step 10960] Loss: 2.8963\n",
            "[Step 10970] Loss: 2.9954\n",
            "[Step 10980] Loss: 2.8070\n",
            "[Step 10990] Loss: 2.7822\n",
            "[Step 11000] Loss: 2.8532\n",
            "[Step 11010] Loss: 2.7204\n",
            "[Step 11020] Loss: 3.2894\n",
            "[Step 11030] Loss: 2.6822\n",
            "[Step 11040] Loss: 2.7944\n",
            "[Step 11050] Loss: 2.5041\n",
            "[Step 11060] Loss: 2.8777\n",
            "[Step 11070] Loss: 2.8995\n",
            "[Step 11080] Loss: 3.0722\n",
            "[Step 11090] Loss: 3.1439\n",
            "[Step 11100] Loss: 2.6729\n",
            "[Step 11110] Loss: 3.4130\n",
            "[Step 11120] Loss: 2.9714\n",
            "[Step 11130] Loss: 3.1644\n",
            "[Step 11140] Loss: 2.6444\n",
            "[Step 11150] Loss: 2.7882\n",
            "[Step 11160] Loss: 3.0740\n",
            "[Step 11170] Loss: 2.8199\n",
            "[Step 11180] Loss: 3.2373\n",
            "[Step 11190] Loss: 2.6066\n",
            "[Step 11200] Loss: 2.7681\n",
            "[Step 11210] Loss: 2.9306\n",
            "[Step 11220] Loss: 2.8445\n",
            "[Step 11230] Loss: 2.6988\n",
            "[Step 11240] Loss: 2.6412\n",
            "[Step 11250] Loss: 3.0177\n",
            "[Step 11260] Loss: 2.8210\n",
            "[Step 11270] Loss: 3.0040\n",
            "[Step 11280] Loss: 3.1182\n",
            "[Step 11290] Loss: 2.8943\n",
            "[Step 11300] Loss: 2.8678\n",
            "[Step 11310] Loss: 2.9600\n",
            "[Step 11320] Loss: 3.0126\n",
            "[Step 11330] Loss: 2.6596\n",
            "[Step 11340] Loss: 2.7650\n",
            "[Step 11350] Loss: 2.6827\n",
            "[Step 11360] Loss: 2.4828\n",
            "ðŸ“˜ Epoch 46 - Avg Training Loss: 2.8438\n",
            "ðŸ” Validation â€” Loss: 2.8596 | Accuracy: 40.01% | Precision: 39.50%\n",
            "[Step 11370] Loss: 2.5802\n",
            "[Step 11380] Loss: 2.9517\n",
            "[Step 11390] Loss: 2.8719\n",
            "[Step 11400] Loss: 2.9458\n",
            "[Step 11410] Loss: 3.2156\n",
            "[Step 11420] Loss: 2.8922\n",
            "[Step 11430] Loss: 2.7580\n",
            "[Step 11440] Loss: 3.0122\n",
            "[Step 11450] Loss: 2.4983\n",
            "[Step 11460] Loss: 2.8892\n",
            "[Step 11470] Loss: 2.7264\n",
            "[Step 11480] Loss: 2.8363\n",
            "[Step 11490] Loss: 2.8822\n",
            "[Step 11500] Loss: 2.6351\n",
            "[Step 11510] Loss: 3.2085\n",
            "[Step 11520] Loss: 2.6903\n",
            "[Step 11530] Loss: 2.8101\n",
            "[Step 11540] Loss: 3.3382\n",
            "[Step 11550] Loss: 2.7582\n",
            "[Step 11560] Loss: 2.5869\n",
            "[Step 11570] Loss: 3.1398\n",
            "[Step 11580] Loss: 2.9908\n",
            "[Step 11590] Loss: 2.5150\n",
            "[Step 11600] Loss: 2.5754\n",
            "[Step 11610] Loss: 3.1614\n",
            "[Step 11620] Loss: 2.8356\n",
            "[Step 11630] Loss: 2.6622\n",
            "[Step 11640] Loss: 2.9531\n",
            "[Step 11650] Loss: 3.2274\n",
            "[Step 11660] Loss: 2.8257\n",
            "[Step 11670] Loss: 2.5644\n",
            "[Step 11680] Loss: 2.8479\n",
            "[Step 11690] Loss: 2.9967\n",
            "[Step 11700] Loss: 2.5237\n",
            "[Step 11710] Loss: 2.9079\n",
            "[Step 11720] Loss: 2.5542\n",
            "[Step 11730] Loss: 2.7322\n",
            "[Step 11740] Loss: 2.5237\n",
            "[Step 11750] Loss: 2.8794\n",
            "[Step 11760] Loss: 2.5765\n",
            "[Step 11770] Loss: 3.2214\n",
            "[Step 11780] Loss: 3.0152\n",
            "[Step 11790] Loss: 2.8863\n",
            "[Step 11800] Loss: 3.0479\n",
            "[Step 11810] Loss: 2.6168\n",
            "[Step 11820] Loss: 2.4922\n",
            "[Step 11830] Loss: 3.3539\n",
            "[Step 11840] Loss: 3.0590\n",
            "[Step 11850] Loss: 3.5332\n",
            "[Step 11860] Loss: 2.7050\n",
            "[Step 11870] Loss: 2.7225\n",
            "[Step 11880] Loss: 3.0075\n",
            "[Step 11890] Loss: 2.9500\n",
            "[Step 11900] Loss: 2.6854\n",
            "[Step 11910] Loss: 3.1743\n",
            "[Step 11920] Loss: 2.5597\n",
            "[Step 11930] Loss: 2.6272\n",
            "[Step 11940] Loss: 3.2326\n",
            "[Step 11950] Loss: 2.7135\n",
            "[Step 11960] Loss: 2.5663\n",
            "[Step 11970] Loss: 2.6799\n",
            "[Step 11980] Loss: 2.6497\n",
            "[Step 11990] Loss: 2.9663\n",
            "[Step 12000] Loss: 2.3519\n",
            "[Step 12010] Loss: 2.7382\n",
            "[Step 12020] Loss: 2.8252\n",
            "[Step 12030] Loss: 2.6187\n",
            "[Step 12040] Loss: 2.6695\n",
            "[Step 12050] Loss: 3.1096\n",
            "[Step 12060] Loss: 2.8901\n",
            "[Step 12070] Loss: 2.9712\n",
            "[Step 12080] Loss: 2.9282\n",
            "[Step 12090] Loss: 3.1314\n",
            "[Step 12100] Loss: 2.7987\n",
            "[Step 12110] Loss: 2.7805\n",
            "[Step 12120] Loss: 2.7002\n",
            "[Step 12130] Loss: 2.6866\n",
            "[Step 12140] Loss: 2.8927\n",
            "[Step 12150] Loss: 3.0243\n",
            "[Step 12160] Loss: 2.6123\n",
            "[Step 12170] Loss: 2.8466\n",
            "[Step 12180] Loss: 3.2059\n",
            "[Step 12190] Loss: 2.8607\n",
            "[Step 12200] Loss: 2.6800\n",
            "[Step 12210] Loss: 2.9983\n",
            "[Step 12220] Loss: 2.9218\n",
            "[Step 12230] Loss: 2.9017\n",
            "[Step 12240] Loss: 2.8434\n",
            "[Step 12250] Loss: 2.8454\n",
            "[Step 12260] Loss: 2.8840\n",
            "[Step 12270] Loss: 2.6510\n",
            "[Step 12280] Loss: 2.9768\n",
            "[Step 12290] Loss: 2.7411\n",
            "[Step 12300] Loss: 2.2652\n",
            "[Step 12310] Loss: 2.8868\n",
            "[Step 12320] Loss: 2.2599\n",
            "[Step 12330] Loss: 2.5505\n",
            "[Step 12340] Loss: 2.3500\n",
            "[Step 12350] Loss: 2.7218\n",
            "[Step 12360] Loss: 2.8795\n",
            "[Step 12370] Loss: 2.6226\n",
            "[Step 12380] Loss: 2.8732\n",
            "[Step 12390] Loss: 2.7744\n",
            "[Step 12400] Loss: 3.1728\n",
            "[Step 12410] Loss: 2.8961\n",
            "[Step 12420] Loss: 2.7322\n",
            "[Step 12430] Loss: 2.5799\n",
            "[Step 12440] Loss: 2.6307\n",
            "[Step 12450] Loss: 3.3355\n",
            "[Step 12460] Loss: 2.8683\n",
            "[Step 12470] Loss: 2.9477\n",
            "[Step 12480] Loss: 2.7579\n",
            "[Step 12490] Loss: 3.1538\n",
            "[Step 12500] Loss: 3.4424\n",
            "[Step 12510] Loss: 2.5516\n",
            "[Step 12520] Loss: 2.5896\n",
            "[Step 12530] Loss: 2.6900\n",
            "[Step 12540] Loss: 2.5513\n",
            "[Step 12550] Loss: 2.5997\n",
            "[Step 12560] Loss: 3.0920\n",
            "[Step 12570] Loss: 2.4709\n",
            "[Step 12580] Loss: 2.7638\n",
            "[Step 12590] Loss: 3.0887\n",
            "[Step 12600] Loss: 2.5071\n",
            "[Step 12610] Loss: 2.9802\n",
            "[Step 12620] Loss: 2.8264\n",
            "[Step 12630] Loss: 3.1192\n",
            "[Step 12640] Loss: 3.1864\n",
            "[Step 12650] Loss: 3.0710\n",
            "[Step 12660] Loss: 2.6855\n",
            "[Step 12670] Loss: 2.6322\n",
            "[Step 12680] Loss: 2.7896\n",
            "[Step 12690] Loss: 2.7239\n",
            "[Step 12700] Loss: 2.7814\n",
            "[Step 12710] Loss: 2.8153\n",
            "[Step 12720] Loss: 3.1775\n",
            "[Step 12730] Loss: 2.6420\n",
            "[Step 12740] Loss: 2.7648\n",
            "[Step 12750] Loss: 3.0301\n",
            "[Step 12760] Loss: 2.9088\n",
            "[Step 12770] Loss: 2.9696\n",
            "[Step 12780] Loss: 2.8639\n",
            "[Step 12790] Loss: 2.7724\n",
            "[Step 12800] Loss: 3.1326\n",
            "[Step 12810] Loss: 3.1471\n",
            "[Step 12820] Loss: 3.1352\n",
            "[Step 12830] Loss: 2.8028\n",
            "[Step 12840] Loss: 2.8877\n",
            "[Step 12850] Loss: 2.7570\n",
            "[Step 12860] Loss: 3.2179\n",
            "[Step 12870] Loss: 2.8328\n",
            "[Step 12880] Loss: 2.8819\n",
            "[Step 12890] Loss: 3.1281\n",
            "[Step 12900] Loss: 3.0990\n",
            "[Step 12910] Loss: 2.8498\n",
            "[Step 12920] Loss: 2.7382\n",
            "[Step 12930] Loss: 2.9330\n",
            "[Step 12940] Loss: 3.0209\n",
            "[Step 12950] Loss: 3.0966\n",
            "[Step 12960] Loss: 2.8640\n",
            "[Step 12970] Loss: 3.1772\n",
            "[Step 12980] Loss: 2.7472\n",
            "[Step 12990] Loss: 2.8301\n",
            "[Step 13000] Loss: 2.7677\n",
            "[Step 13010] Loss: 2.7001\n",
            "[Step 13020] Loss: 2.7219\n",
            "[Step 13030] Loss: 2.5423\n",
            "[Step 13040] Loss: 3.0201\n",
            "[Step 13050] Loss: 2.5962\n",
            "[Step 13060] Loss: 2.7954\n",
            "[Step 13070] Loss: 3.0809\n",
            "[Step 13080] Loss: 2.6699\n",
            "[Step 13090] Loss: 2.7291\n",
            "[Step 13100] Loss: 2.8016\n",
            "[Step 13110] Loss: 2.6492\n",
            "[Step 13120] Loss: 2.8084\n",
            "[Step 13130] Loss: 2.9098\n",
            "[Step 13140] Loss: 2.6923\n",
            "[Step 13150] Loss: 2.6502\n",
            "[Step 13160] Loss: 2.7685\n",
            "[Step 13170] Loss: 2.6978\n",
            "[Step 13180] Loss: 2.7353\n",
            "[Step 13190] Loss: 2.4547\n",
            "[Step 13200] Loss: 3.5329\n",
            "[Step 13210] Loss: 2.9488\n",
            "[Step 13220] Loss: 3.2998\n",
            "[Step 13230] Loss: 2.3939\n",
            "[Step 13240] Loss: 2.7289\n",
            "[Step 13250] Loss: 2.6814\n",
            "ðŸ“˜ Epoch 47 - Avg Training Loss: 2.8331\n",
            "ðŸ” Validation â€” Loss: 2.8426 | Accuracy: 40.13% | Precision: 39.31%\n",
            "[Step 13260] Loss: 2.7331\n",
            "[Step 13270] Loss: 3.0210\n",
            "[Step 13280] Loss: 2.5359\n",
            "[Step 13290] Loss: 3.0405\n",
            "[Step 13300] Loss: 3.2729\n",
            "[Step 13310] Loss: 3.2307\n",
            "[Step 13320] Loss: 2.8860\n",
            "[Step 13330] Loss: 2.8542\n",
            "[Step 13340] Loss: 2.8681\n",
            "[Step 13350] Loss: 2.8358\n",
            "[Step 13360] Loss: 2.8901\n",
            "[Step 13370] Loss: 2.9569\n",
            "[Step 13380] Loss: 2.8194\n",
            "[Step 13390] Loss: 2.8915\n",
            "[Step 13400] Loss: 2.8182\n",
            "[Step 13410] Loss: 3.0635\n",
            "[Step 13420] Loss: 2.4848\n",
            "[Step 13430] Loss: 2.7709\n",
            "[Step 13440] Loss: 2.6709\n",
            "[Step 13450] Loss: 2.7258\n",
            "[Step 13460] Loss: 2.8391\n",
            "[Step 13470] Loss: 2.9165\n",
            "[Step 13480] Loss: 2.8569\n",
            "[Step 13490] Loss: 3.0557\n",
            "[Step 13500] Loss: 2.8835\n",
            "[Step 13510] Loss: 2.6916\n",
            "[Step 13520] Loss: 3.2610\n",
            "[Step 13530] Loss: 2.7755\n",
            "[Step 13540] Loss: 2.8363\n",
            "[Step 13550] Loss: 2.9070\n",
            "[Step 13560] Loss: 3.0560\n",
            "[Step 13570] Loss: 2.7754\n",
            "[Step 13580] Loss: 2.6405\n",
            "[Step 13590] Loss: 3.0452\n",
            "[Step 13600] Loss: 2.6822\n",
            "[Step 13610] Loss: 2.5809\n",
            "[Step 13620] Loss: 2.8448\n",
            "[Step 13630] Loss: 2.7552\n",
            "[Step 13640] Loss: 2.8305\n",
            "[Step 13650] Loss: 2.9809\n",
            "[Step 13660] Loss: 2.6845\n",
            "[Step 13670] Loss: 2.9805\n",
            "[Step 13680] Loss: 2.6417\n",
            "[Step 13690] Loss: 2.5548\n",
            "[Step 13700] Loss: 2.8140\n",
            "[Step 13710] Loss: 2.5330\n",
            "[Step 13720] Loss: 3.0436\n",
            "[Step 13730] Loss: 2.8090\n",
            "[Step 13740] Loss: 2.8569\n",
            "[Step 13750] Loss: 2.6781\n",
            "[Step 13760] Loss: 3.2849\n",
            "[Step 13770] Loss: 3.0349\n",
            "[Step 13780] Loss: 2.6132\n",
            "[Step 13790] Loss: 2.8611\n",
            "[Step 13800] Loss: 2.4263\n",
            "[Step 13810] Loss: 2.5022\n",
            "[Step 13820] Loss: 3.1208\n",
            "[Step 13830] Loss: 3.1436\n",
            "[Step 13840] Loss: 3.0376\n",
            "[Step 13850] Loss: 2.8523\n",
            "[Step 13860] Loss: 2.4686\n",
            "[Step 13870] Loss: 2.3080\n",
            "[Step 13880] Loss: 2.9207\n",
            "[Step 13890] Loss: 2.5013\n",
            "[Step 13900] Loss: 2.5383\n",
            "[Step 13910] Loss: 3.1379\n",
            "[Step 13920] Loss: 3.0324\n",
            "[Step 13930] Loss: 2.9313\n",
            "[Step 13940] Loss: 2.9444\n",
            "[Step 13950] Loss: 2.8027\n",
            "[Step 13960] Loss: 3.2466\n",
            "[Step 13970] Loss: 2.6665\n",
            "[Step 13980] Loss: 2.8718\n",
            "[Step 13990] Loss: 3.0211\n",
            "[Step 14000] Loss: 3.3618\n",
            "[Step 14010] Loss: 2.8503\n",
            "[Step 14020] Loss: 3.1006\n",
            "[Step 14030] Loss: 2.4853\n",
            "[Step 14040] Loss: 2.9986\n",
            "[Step 14050] Loss: 2.9357\n",
            "[Step 14060] Loss: 2.7879\n",
            "[Step 14070] Loss: 2.8585\n",
            "[Step 14080] Loss: 2.7665\n",
            "[Step 14090] Loss: 3.0362\n",
            "[Step 14100] Loss: 3.1153\n",
            "[Step 14110] Loss: 2.9041\n",
            "[Step 14120] Loss: 2.9012\n",
            "[Step 14130] Loss: 2.7062\n",
            "[Step 14140] Loss: 3.0700\n",
            "[Step 14150] Loss: 3.2010\n",
            "[Step 14160] Loss: 2.6474\n",
            "[Step 14170] Loss: 2.6657\n",
            "[Step 14180] Loss: 2.5551\n",
            "[Step 14190] Loss: 2.6167\n",
            "[Step 14200] Loss: 2.8481\n",
            "[Step 14210] Loss: 3.1070\n",
            "[Step 14220] Loss: 2.8794\n",
            "[Step 14230] Loss: 2.8841\n",
            "[Step 14240] Loss: 2.7927\n",
            "[Step 14250] Loss: 3.2786\n",
            "[Step 14260] Loss: 2.6717\n",
            "[Step 14270] Loss: 2.3776\n",
            "[Step 14280] Loss: 2.8102\n",
            "[Step 14290] Loss: 2.8924\n",
            "[Step 14300] Loss: 2.9887\n",
            "[Step 14310] Loss: 3.0662\n",
            "[Step 14320] Loss: 3.0688\n",
            "[Step 14330] Loss: 2.6636\n",
            "[Step 14340] Loss: 3.1665\n",
            "[Step 14350] Loss: 2.7303\n",
            "[Step 14360] Loss: 2.6259\n",
            "[Step 14370] Loss: 2.8469\n",
            "[Step 14380] Loss: 2.5963\n",
            "[Step 14390] Loss: 2.7756\n",
            "[Step 14400] Loss: 2.7083\n",
            "[Step 14410] Loss: 3.3583\n",
            "[Step 14420] Loss: 2.5413\n",
            "[Step 14430] Loss: 2.8044\n",
            "[Step 14440] Loss: 2.7023\n",
            "[Step 14450] Loss: 2.5746\n",
            "[Step 14460] Loss: 3.0058\n",
            "[Step 14470] Loss: 2.4875\n",
            "[Step 14480] Loss: 2.9394\n",
            "[Step 14490] Loss: 2.8699\n",
            "[Step 14500] Loss: 2.5679\n",
            "[Step 14510] Loss: 2.9660\n",
            "[Step 14520] Loss: 3.1309\n",
            "[Step 14530] Loss: 3.0248\n",
            "[Step 14540] Loss: 2.6686\n",
            "[Step 14550] Loss: 2.5575\n",
            "[Step 14560] Loss: 2.8323\n",
            "[Step 14570] Loss: 2.9913\n",
            "[Step 14580] Loss: 2.7489\n",
            "[Step 14590] Loss: 3.1248\n",
            "[Step 14600] Loss: 3.4163\n",
            "[Step 14610] Loss: 2.8160\n",
            "[Step 14620] Loss: 2.7402\n",
            "[Step 14630] Loss: 2.6335\n",
            "[Step 14640] Loss: 2.6842\n",
            "[Step 14650] Loss: 3.2235\n",
            "[Step 14660] Loss: 2.7452\n",
            "[Step 14670] Loss: 2.9586\n",
            "[Step 14680] Loss: 2.9545\n",
            "[Step 14690] Loss: 2.3684\n",
            "[Step 14700] Loss: 2.7188\n",
            "[Step 14710] Loss: 2.8967\n",
            "[Step 14720] Loss: 2.7045\n",
            "[Step 14730] Loss: 2.8618\n",
            "[Step 14740] Loss: 3.0190\n",
            "[Step 14750] Loss: 2.8115\n",
            "[Step 14760] Loss: 2.8664\n",
            "[Step 14770] Loss: 2.7869\n",
            "[Step 14780] Loss: 2.7332\n",
            "[Step 14790] Loss: 2.6825\n",
            "[Step 14800] Loss: 3.1045\n",
            "[Step 14810] Loss: 2.7877\n",
            "[Step 14820] Loss: 2.4152\n",
            "[Step 14830] Loss: 2.9904\n",
            "[Step 14840] Loss: 3.0797\n",
            "[Step 14850] Loss: 2.6571\n",
            "[Step 14860] Loss: 3.2035\n",
            "[Step 14870] Loss: 2.6658\n",
            "[Step 14880] Loss: 3.4327\n",
            "[Step 14890] Loss: 2.7338\n",
            "[Step 14900] Loss: 2.3960\n",
            "[Step 14910] Loss: 2.5381\n",
            "[Step 14920] Loss: 2.2114\n",
            "[Step 14930] Loss: 2.9309\n",
            "[Step 14940] Loss: 2.9408\n",
            "[Step 14950] Loss: 2.9025\n",
            "[Step 14960] Loss: 2.9070\n",
            "[Step 14970] Loss: 2.6976\n",
            "[Step 14980] Loss: 2.9842\n",
            "[Step 14990] Loss: 2.7633\n",
            "[Step 15000] Loss: 3.0532\n",
            "[Step 15010] Loss: 2.9108\n",
            "[Step 15020] Loss: 2.5988\n",
            "[Step 15030] Loss: 3.1428\n",
            "[Step 15040] Loss: 2.9073\n",
            "[Step 15050] Loss: 2.7317\n",
            "[Step 15060] Loss: 2.8767\n",
            "[Step 15070] Loss: 2.8918\n",
            "[Step 15080] Loss: 2.6899\n",
            "[Step 15090] Loss: 2.5226\n",
            "[Step 15100] Loss: 2.4477\n",
            "[Step 15110] Loss: 2.8229\n",
            "[Step 15120] Loss: 2.9312\n",
            "[Step 15130] Loss: 2.6806\n",
            "[Step 15140] Loss: 2.5759\n",
            "[Step 15150] Loss: 2.5025\n",
            "ðŸ“˜ Epoch 48 - Avg Training Loss: 2.8165\n",
            "ðŸ” Validation â€” Loss: 2.8148 | Accuracy: 40.95% | Precision: 40.15%\n",
            "[Step 15160] Loss: 2.7535\n",
            "[Step 15170] Loss: 3.1676\n",
            "[Step 15180] Loss: 2.5583\n",
            "[Step 15190] Loss: 2.6391\n",
            "[Step 15200] Loss: 2.5413\n",
            "[Step 15210] Loss: 2.6281\n",
            "[Step 15220] Loss: 2.8850\n",
            "[Step 15230] Loss: 2.6643\n",
            "[Step 15240] Loss: 2.9511\n",
            "[Step 15250] Loss: 3.0368\n",
            "[Step 15260] Loss: 2.8072\n",
            "[Step 15270] Loss: 2.8564\n",
            "[Step 15280] Loss: 2.6992\n",
            "[Step 15290] Loss: 2.5328\n",
            "[Step 15300] Loss: 2.6410\n",
            "[Step 15310] Loss: 2.9422\n",
            "[Step 15320] Loss: 2.9016\n",
            "[Step 15330] Loss: 2.5982\n",
            "[Step 15340] Loss: 2.6054\n",
            "[Step 15350] Loss: 2.6686\n",
            "[Step 15360] Loss: 2.6617\n",
            "[Step 15370] Loss: 2.4487\n",
            "[Step 15380] Loss: 2.8553\n",
            "[Step 15390] Loss: 2.9763\n",
            "[Step 15400] Loss: 2.6224\n",
            "[Step 15410] Loss: 2.8092\n",
            "[Step 15420] Loss: 2.3864\n",
            "[Step 15430] Loss: 2.6182\n",
            "[Step 15440] Loss: 2.2928\n",
            "[Step 15450] Loss: 2.5287\n",
            "[Step 15460] Loss: 2.9033\n",
            "[Step 15470] Loss: 2.9973\n",
            "[Step 15480] Loss: 2.8332\n",
            "[Step 15490] Loss: 3.3435\n",
            "[Step 15500] Loss: 2.8395\n",
            "[Step 15510] Loss: 3.0400\n",
            "[Step 15520] Loss: 2.9747\n",
            "[Step 15530] Loss: 2.8139\n",
            "[Step 15540] Loss: 2.4850\n",
            "[Step 15550] Loss: 2.8121\n",
            "[Step 15560] Loss: 2.8499\n",
            "[Step 15570] Loss: 2.7628\n",
            "[Step 15580] Loss: 2.6412\n",
            "[Step 15590] Loss: 2.4893\n",
            "[Step 15600] Loss: 2.7703\n",
            "[Step 15610] Loss: 3.0735\n",
            "[Step 15620] Loss: 3.0120\n",
            "[Step 15630] Loss: 3.1506\n",
            "[Step 15640] Loss: 2.8998\n",
            "[Step 15650] Loss: 2.8438\n",
            "[Step 15660] Loss: 2.5775\n",
            "[Step 15670] Loss: 2.5579\n",
            "[Step 15680] Loss: 3.2089\n",
            "[Step 15690] Loss: 2.6294\n",
            "[Step 15700] Loss: 2.9483\n",
            "[Step 15710] Loss: 2.9794\n",
            "[Step 15720] Loss: 3.0260\n",
            "[Step 15730] Loss: 2.7196\n",
            "[Step 15740] Loss: 3.0218\n",
            "[Step 15750] Loss: 2.7699\n",
            "[Step 15760] Loss: 2.6981\n",
            "[Step 15770] Loss: 2.9570\n",
            "[Step 15780] Loss: 2.5943\n",
            "[Step 15790] Loss: 2.8320\n",
            "[Step 15800] Loss: 2.7517\n",
            "[Step 15810] Loss: 2.4400\n",
            "[Step 15820] Loss: 2.3112\n",
            "[Step 15830] Loss: 2.7804\n",
            "[Step 15840] Loss: 3.0684\n",
            "[Step 15850] Loss: 2.9871\n",
            "[Step 15860] Loss: 2.4718\n",
            "[Step 15870] Loss: 2.7435\n",
            "[Step 15880] Loss: 2.7781\n",
            "[Step 15890] Loss: 2.3286\n",
            "[Step 15900] Loss: 2.8448\n",
            "[Step 15910] Loss: 2.6436\n",
            "[Step 15920] Loss: 2.9240\n",
            "[Step 15930] Loss: 2.8879\n",
            "[Step 15940] Loss: 2.7333\n",
            "[Step 15950] Loss: 2.8827\n",
            "[Step 15960] Loss: 2.6935\n",
            "[Step 15970] Loss: 2.7295\n",
            "[Step 15980] Loss: 2.7649\n",
            "[Step 15990] Loss: 2.6664\n",
            "[Step 16000] Loss: 2.3909\n",
            "[Step 16010] Loss: 2.7588\n",
            "[Step 16020] Loss: 2.6343\n",
            "[Step 16030] Loss: 2.7754\n",
            "[Step 16040] Loss: 2.9541\n",
            "[Step 16050] Loss: 3.1310\n",
            "[Step 16060] Loss: 2.5343\n",
            "[Step 16070] Loss: 3.1206\n",
            "[Step 16080] Loss: 3.1346\n",
            "[Step 16090] Loss: 2.9636\n",
            "[Step 16100] Loss: 2.7480\n",
            "[Step 16110] Loss: 2.9439\n",
            "[Step 16120] Loss: 2.9139\n",
            "[Step 16130] Loss: 2.7593\n",
            "[Step 16140] Loss: 2.7016\n",
            "[Step 16150] Loss: 2.8226\n",
            "[Step 16160] Loss: 2.5833\n",
            "[Step 16170] Loss: 2.5391\n",
            "[Step 16180] Loss: 2.7005\n",
            "[Step 16190] Loss: 2.6467\n",
            "[Step 16200] Loss: 2.7251\n",
            "[Step 16210] Loss: 2.6434\n",
            "[Step 16220] Loss: 2.9408\n",
            "[Step 16230] Loss: 2.8821\n",
            "[Step 16240] Loss: 2.5640\n",
            "[Step 16250] Loss: 2.3318\n",
            "[Step 16260] Loss: 2.8160\n",
            "[Step 16270] Loss: 3.0152\n",
            "[Step 16280] Loss: 2.8989\n",
            "[Step 16290] Loss: 2.7268\n",
            "[Step 16300] Loss: 2.8300\n",
            "[Step 16310] Loss: 2.7214\n",
            "[Step 16320] Loss: 3.1729\n",
            "[Step 16330] Loss: 2.5625\n",
            "[Step 16340] Loss: 2.8977\n",
            "[Step 16350] Loss: 2.8418\n",
            "[Step 16360] Loss: 2.8109\n",
            "[Step 16370] Loss: 2.9605\n",
            "[Step 16380] Loss: 2.9875\n",
            "[Step 16390] Loss: 2.9271\n",
            "[Step 16400] Loss: 3.0015\n",
            "[Step 16410] Loss: 2.6132\n",
            "[Step 16420] Loss: 2.9138\n",
            "[Step 16430] Loss: 3.0168\n",
            "[Step 16440] Loss: 2.7480\n",
            "[Step 16450] Loss: 2.7642\n",
            "[Step 16460] Loss: 2.9194\n",
            "[Step 16470] Loss: 2.9240\n",
            "[Step 16480] Loss: 2.7399\n",
            "[Step 16490] Loss: 2.6352\n",
            "[Step 16500] Loss: 2.9123\n",
            "[Step 16510] Loss: 2.5922\n",
            "[Step 16520] Loss: 2.6458\n",
            "[Step 16530] Loss: 2.8827\n",
            "[Step 16540] Loss: 2.8315\n",
            "[Step 16550] Loss: 3.1844\n",
            "[Step 16560] Loss: 3.0544\n",
            "[Step 16570] Loss: 2.8566\n",
            "[Step 16580] Loss: 2.6245\n",
            "[Step 16590] Loss: 2.7490\n",
            "[Step 16600] Loss: 2.9768\n",
            "[Step 16610] Loss: 2.9378\n",
            "[Step 16620] Loss: 3.1262\n",
            "[Step 16630] Loss: 3.1685\n",
            "[Step 16640] Loss: 2.6282\n",
            "[Step 16650] Loss: 2.7652\n",
            "[Step 16660] Loss: 2.8634\n",
            "[Step 16670] Loss: 2.8527\n",
            "[Step 16680] Loss: 2.8636\n",
            "[Step 16690] Loss: 2.9091\n",
            "[Step 16700] Loss: 2.6262\n",
            "[Step 16710] Loss: 3.2571\n",
            "[Step 16720] Loss: 2.8315\n",
            "[Step 16730] Loss: 2.7261\n",
            "[Step 16740] Loss: 2.7235\n",
            "[Step 16750] Loss: 2.7525\n",
            "[Step 16760] Loss: 3.1897\n",
            "[Step 16770] Loss: 2.6377\n",
            "[Step 16780] Loss: 3.1508\n",
            "[Step 16790] Loss: 3.1480\n",
            "[Step 16800] Loss: 2.8437\n",
            "[Step 16810] Loss: 2.7360\n",
            "[Step 16820] Loss: 3.0171\n",
            "[Step 16830] Loss: 2.9758\n",
            "[Step 16840] Loss: 2.3637\n",
            "[Step 16850] Loss: 3.1744\n",
            "[Step 16860] Loss: 3.0790\n",
            "[Step 16870] Loss: 3.2228\n",
            "[Step 16880] Loss: 2.8358\n",
            "[Step 16890] Loss: 2.6292\n",
            "[Step 16900] Loss: 2.6481\n",
            "[Step 16910] Loss: 3.0274\n",
            "[Step 16920] Loss: 2.5641\n",
            "[Step 16930] Loss: 2.8226\n",
            "[Step 16940] Loss: 3.2378\n",
            "[Step 16950] Loss: 3.2252\n",
            "[Step 16960] Loss: 2.7060\n",
            "[Step 16970] Loss: 3.2048\n",
            "[Step 16980] Loss: 2.8156\n",
            "[Step 16990] Loss: 2.7119\n",
            "[Step 17000] Loss: 2.7027\n",
            "[Step 17010] Loss: 2.9449\n",
            "[Step 17020] Loss: 3.0545\n",
            "[Step 17030] Loss: 2.5979\n",
            "[Step 17040] Loss: 2.6509\n",
            "ðŸ“˜ Epoch 49 - Avg Training Loss: 2.7917\n",
            "ðŸ” Validation â€” Loss: 2.7941 | Accuracy: 42.15% | Precision: 40.59%\n",
            "[Step 17050] Loss: 2.7752\n",
            "[Step 17060] Loss: 2.8198\n",
            "[Step 17070] Loss: 2.7415\n",
            "[Step 17080] Loss: 2.6003\n",
            "[Step 17090] Loss: 2.9966\n",
            "[Step 17100] Loss: 2.7635\n",
            "[Step 17110] Loss: 3.0797\n",
            "[Step 17120] Loss: 2.7359\n",
            "[Step 17130] Loss: 2.8996\n",
            "[Step 17140] Loss: 2.3974\n",
            "[Step 17150] Loss: 2.7527\n",
            "[Step 17160] Loss: 2.4277\n",
            "[Step 17170] Loss: 2.5974\n",
            "[Step 17180] Loss: 2.9076\n",
            "[Step 17190] Loss: 2.9567\n",
            "[Step 17200] Loss: 2.5324\n",
            "[Step 17210] Loss: 3.0468\n",
            "[Step 17220] Loss: 2.7131\n",
            "[Step 17230] Loss: 2.5653\n",
            "[Step 17240] Loss: 2.8973\n",
            "[Step 17250] Loss: 2.7836\n",
            "[Step 17260] Loss: 3.2920\n",
            "[Step 17270] Loss: 2.5112\n",
            "[Step 17280] Loss: 2.9468\n",
            "[Step 17290] Loss: 2.8127\n",
            "[Step 17300] Loss: 2.4194\n",
            "[Step 17310] Loss: 2.7236\n",
            "[Step 17320] Loss: 2.6001\n",
            "[Step 17330] Loss: 2.6439\n",
            "[Step 17340] Loss: 2.5452\n",
            "[Step 17350] Loss: 2.9424\n",
            "[Step 17360] Loss: 2.4896\n",
            "[Step 17370] Loss: 2.7325\n",
            "[Step 17380] Loss: 2.8157\n",
            "[Step 17390] Loss: 2.4611\n",
            "[Step 17400] Loss: 2.7804\n",
            "[Step 17410] Loss: 2.9570\n",
            "[Step 17420] Loss: 2.8463\n",
            "[Step 17430] Loss: 2.9167\n",
            "[Step 17440] Loss: 2.8011\n",
            "[Step 17450] Loss: 3.0027\n",
            "[Step 17460] Loss: 2.5919\n",
            "[Step 17470] Loss: 3.0516\n",
            "[Step 17480] Loss: 2.9871\n",
            "[Step 17490] Loss: 2.5218\n",
            "[Step 17500] Loss: 2.8008\n",
            "[Step 17510] Loss: 2.8789\n",
            "[Step 17520] Loss: 2.3795\n",
            "[Step 17530] Loss: 2.9190\n",
            "[Step 17540] Loss: 2.7961\n",
            "[Step 17550] Loss: 2.8936\n",
            "[Step 17560] Loss: 3.2339\n",
            "[Step 17570] Loss: 2.9112\n",
            "[Step 17580] Loss: 2.8384\n",
            "[Step 17590] Loss: 2.6804\n",
            "[Step 17600] Loss: 2.8172\n",
            "[Step 17610] Loss: 2.3879\n",
            "[Step 17620] Loss: 3.1518\n",
            "[Step 17630] Loss: 3.1824\n",
            "[Step 17640] Loss: 2.8564\n",
            "[Step 17650] Loss: 2.8809\n",
            "[Step 17660] Loss: 2.8771\n",
            "[Step 17670] Loss: 2.6749\n",
            "[Step 17680] Loss: 2.8925\n",
            "[Step 17690] Loss: 3.0456\n",
            "[Step 17700] Loss: 2.6179\n",
            "[Step 17710] Loss: 2.7033\n",
            "[Step 17720] Loss: 2.9931\n",
            "[Step 17730] Loss: 2.6670\n",
            "[Step 17740] Loss: 3.1280\n",
            "[Step 17750] Loss: 2.9414\n",
            "[Step 17760] Loss: 2.7191\n",
            "[Step 17770] Loss: 2.9727\n",
            "[Step 17780] Loss: 2.7927\n",
            "[Step 17790] Loss: 2.4742\n",
            "[Step 17800] Loss: 2.8580\n",
            "[Step 17810] Loss: 2.6733\n",
            "[Step 17820] Loss: 2.8772\n",
            "[Step 17830] Loss: 2.5155\n",
            "[Step 17840] Loss: 2.8539\n",
            "[Step 17850] Loss: 2.3865\n",
            "[Step 17860] Loss: 2.5400\n",
            "[Step 17870] Loss: 3.0729\n",
            "[Step 17880] Loss: 2.5753\n",
            "[Step 17890] Loss: 2.9963\n",
            "[Step 17900] Loss: 2.7944\n",
            "[Step 17910] Loss: 2.5169\n",
            "[Step 17920] Loss: 2.3629\n",
            "[Step 17930] Loss: 2.8281\n",
            "[Step 17940] Loss: 2.5467\n",
            "[Step 17950] Loss: 2.8661\n",
            "[Step 17960] Loss: 2.8780\n",
            "[Step 17970] Loss: 3.0075\n",
            "[Step 17980] Loss: 2.4669\n",
            "[Step 17990] Loss: 3.0220\n",
            "[Step 18000] Loss: 2.8261\n",
            "[Step 18010] Loss: 2.5516\n",
            "[Step 18020] Loss: 3.0925\n",
            "[Step 18030] Loss: 2.5687\n",
            "[Step 18040] Loss: 2.7652\n",
            "[Step 18050] Loss: 2.9894\n",
            "[Step 18060] Loss: 2.6123\n",
            "[Step 18070] Loss: 2.7589\n",
            "[Step 18080] Loss: 2.5119\n",
            "[Step 18090] Loss: 3.1048\n",
            "[Step 18100] Loss: 2.5415\n",
            "[Step 18110] Loss: 2.4419\n",
            "[Step 18120] Loss: 2.7476\n",
            "[Step 18130] Loss: 2.7113\n",
            "[Step 18140] Loss: 2.5880\n",
            "[Step 18150] Loss: 2.6837\n",
            "[Step 18160] Loss: 2.6528\n",
            "[Step 18170] Loss: 2.6149\n",
            "[Step 18180] Loss: 2.6384\n",
            "[Step 18190] Loss: 2.9213\n",
            "[Step 18200] Loss: 2.1770\n",
            "[Step 18210] Loss: 2.7191\n",
            "[Step 18220] Loss: 2.7092\n",
            "[Step 18230] Loss: 2.9025\n",
            "[Step 18240] Loss: 2.4337\n",
            "[Step 18250] Loss: 2.8363\n",
            "[Step 18260] Loss: 2.5286\n",
            "[Step 18270] Loss: 2.6487\n",
            "[Step 18280] Loss: 3.1334\n",
            "[Step 18290] Loss: 2.9630\n",
            "[Step 18300] Loss: 2.4293\n",
            "[Step 18310] Loss: 2.8396\n",
            "[Step 18320] Loss: 3.1336\n",
            "[Step 18330] Loss: 2.8031\n",
            "[Step 18340] Loss: 2.8203\n",
            "[Step 18350] Loss: 2.5599\n",
            "[Step 18360] Loss: 2.9893\n",
            "[Step 18370] Loss: 2.8005\n",
            "[Step 18380] Loss: 2.5836\n",
            "[Step 18390] Loss: 2.7194\n",
            "[Step 18400] Loss: 2.6472\n",
            "[Step 18410] Loss: 2.6504\n",
            "[Step 18420] Loss: 2.8154\n",
            "[Step 18430] Loss: 2.6947\n",
            "[Step 18440] Loss: 2.4267\n",
            "[Step 18450] Loss: 3.3403\n",
            "[Step 18460] Loss: 3.0659\n",
            "[Step 18470] Loss: 3.0771\n",
            "[Step 18480] Loss: 2.8800\n",
            "[Step 18490] Loss: 2.7151\n",
            "[Step 18500] Loss: 2.9794\n",
            "[Step 18510] Loss: 2.5347\n",
            "[Step 18520] Loss: 2.9068\n",
            "[Step 18530] Loss: 3.2032\n",
            "[Step 18540] Loss: 2.6443\n",
            "[Step 18550] Loss: 2.5300\n",
            "[Step 18560] Loss: 2.7241\n",
            "[Step 18570] Loss: 2.8646\n",
            "[Step 18580] Loss: 2.7810\n",
            "[Step 18590] Loss: 2.8894\n",
            "[Step 18600] Loss: 2.8774\n",
            "[Step 18610] Loss: 2.8891\n",
            "[Step 18620] Loss: 2.8120\n",
            "[Step 18630] Loss: 2.6503\n",
            "[Step 18640] Loss: 2.8594\n",
            "[Step 18650] Loss: 2.6447\n",
            "[Step 18660] Loss: 2.8020\n",
            "[Step 18670] Loss: 2.2302\n",
            "[Step 18680] Loss: 3.0165\n",
            "[Step 18690] Loss: 2.7324\n",
            "[Step 18700] Loss: 2.5474\n",
            "[Step 18710] Loss: 2.8060\n",
            "[Step 18720] Loss: 3.1067\n",
            "[Step 18730] Loss: 2.9000\n",
            "[Step 18740] Loss: 2.7034\n",
            "[Step 18750] Loss: 2.9806\n",
            "[Step 18760] Loss: 2.9892\n",
            "[Step 18770] Loss: 2.7700\n",
            "[Step 18780] Loss: 3.0113\n",
            "[Step 18790] Loss: 2.8606\n",
            "[Step 18800] Loss: 3.3131\n",
            "[Step 18810] Loss: 2.5157\n",
            "[Step 18820] Loss: 2.4923\n",
            "[Step 18830] Loss: 2.5639\n",
            "[Step 18840] Loss: 3.0527\n",
            "[Step 18850] Loss: 2.8084\n",
            "[Step 18860] Loss: 2.8806\n",
            "[Step 18870] Loss: 2.5539\n",
            "[Step 18880] Loss: 2.3799\n",
            "[Step 18890] Loss: 2.5812\n",
            "[Step 18900] Loss: 2.5628\n",
            "[Step 18910] Loss: 3.2186\n",
            "[Step 18920] Loss: 2.9291\n",
            "[Step 18930] Loss: 2.8971\n",
            "[Step 18940] Loss: 2.5611\n",
            "ðŸ“˜ Epoch 50 - Avg Training Loss: 2.7712\n",
            "ðŸ” Validation â€” Loss: 2.7829 | Accuracy: 42.22% | Precision: 40.72%\n",
            "âœ… Continued training complete\n"
          ]
        }
      ],
      "source": [
        "num_classes = 101\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "checkpoint_path = '/content/drive/My Drive/NexHack/checkpoint_4.pth'\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "new_training_args = {\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"num_additional_epochs\": 10,\n",
        "    \"logging_steps\": 10,\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=new_training_args[\"learning_rate\"],\n",
        "    weight_decay=new_training_args[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "starting_epoch = checkpoint['epoch']\n",
        "global_step = 0\n",
        "train_losses = []\n",
        "\n",
        "def evaluate_val(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"ðŸ” Validation â€” Loss: {avg_loss:.4f} | Accuracy: {accuracy*100:.2f}% | Precision: {precision*100:.2f}%\")\n",
        "\n",
        "for epoch in range(starting_epoch, starting_epoch + new_training_args[\"num_additional_epochs\"]):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % new_training_args[\"logging_steps\"] == 0:\n",
        "            print(f\"[Step {global_step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"ðŸ“˜ Epoch {epoch + 1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluate after each epoch\n",
        "    evaluate_val(model, val_loader, criterion, device)\n",
        "\n",
        "print(\"âœ… Continued training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89A1sPtbucGg",
        "outputId": "636ba886-78db-4a2f-c81f-08426fd9a9dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "save_path = '/content/drive/My Drive/NexHack/model_weights_6.pth'\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=True)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model_weights_1 = MyResNet50()\n",
        "\n",
        "torch.save(model_weights_1.state_dict(), 'model_weights_6.pth')\n",
        "torch.save(model_weights_1.state_dict(), save_path)\n",
        "\n",
        "save_path = '/content/drive/My Drive/NexHack/model_full_6.pth'\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=True)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model_full_1 = MyResNet50()\n",
        "\n",
        "torch.save(model_full_1, 'model_full_6.pth')\n",
        "torch.save(model_full_1.state_dict(), save_path)\n",
        "\n",
        "save_path = '/content/drive/My Drive/NexHack/checkpoint_6.pth'\n",
        "\n",
        "checkpoint = {\n",
        "    'epoch': 40,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'training_args': new_training_args\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, save_path)\n",
        "\n",
        "print(\"Saved to Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twcqsnw05e2K",
        "outputId": "cf533637-dfe2-4250-8ad2-b8e82617f510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Final Validation â€” Loss: 2.7829 | Accuracy: 0.4222 | Precision: 0.4072\n"
          ]
        }
      ],
      "source": [
        "val_losses = []\n",
        "val_accuracies = []\n",
        "val_precisions = []\n",
        "\n",
        "def evaluate_val(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(accuracy)\n",
        "    val_precisions.append(precision)\n",
        "\n",
        "    print(f\"ðŸ“Š Final Validation â€” Loss: {avg_val_loss:.4f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f}\")\n",
        "\n",
        "evaluate_val(model, val_loader, criterion, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBm8onL8pjqG",
        "outputId": "f880c24c-c81c-458e-d03d-a24a5b148d6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 42.25%\n",
            "Test Precision (macro): 40.57%\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
        "                         std=[0.229, 0.224, 0.225])   # ImageNet std\n",
        "])\n",
        "\n",
        "test_dataset = datasets.Food101(root=\"./data\", split=\"test\", transform=transform, download=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "num_classes = 101\n",
        "model = models.resnet50(pretrained=False)  # pretrained=False since you're loading your own weights\n",
        "model.fc = torch.nn.Linear(2048, num_classes)\n",
        "\n",
        "checkpoint_path = '/content/drive/My Drive/NexHack/checkpoint_4.pth'\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Test Precision (macro): {prec * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "tFintw8Xqq78",
        "outputId": "8146feab-ea5e-479c-85af-40142e8e76bd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "sample_idx = random.randint(0, len(test_dataset) - 1)\n",
        "image, label = test_dataset[sample_idx]\n",
        "class_names = test_dataset.classes\n",
        "model_input = image.unsqueeze(0).to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(model_input)\n",
        "    pred_idx = torch.argmax(output, dim=1).item()\n",
        "    pred_label = class_names[pred_idx]\n",
        "\n",
        "def imshow(img):\n",
        "    img = img.permute(1, 2, 0).numpy()\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "imshow(image)\n",
        "plt.title(f\"Predicted: {pred_label}\\nTrue: {class_names[label]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Drive and Load CSVs\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from rapidfuzz import process, fuzz\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/NexHack/'\n",
        "\n",
        "food = pd.read_csv(path + 'food.csv', low_memory=False)\n",
        "food_nutrient = pd.read_csv(path + 'food_nutrient.csv', low_memory=False)\n",
        "nutrient = pd.read_csv(path + 'nutrient.csv', low_memory=False)\n",
        "branded_food = pd.read_csv(path + 'branded_food.csv', low_memory=False)\n",
        "\n",
        "sodium_id = nutrient[nutrient['name'] == 'Sodium, Na']['id'].values[0]\n",
        "\n",
        "foundation_df = food[['fdc_id', 'description']].assign(source='foundation')\n",
        "branded_df = branded_food[['fdc_id', 'brand_name', 'subbrand_name', 'short_description']].rename(\n",
        "    columns={'short_description': 'description'}\n",
        ").assign(source='branded')\n",
        "food_all = pd.concat([foundation_df, branded_df], ignore_index=True)\n",
        "food_all['search_name'] = food_all['description'].fillna('').str.lower().str.strip()\n",
        "choices = food_all['search_name'].tolist()\n",
        "\n",
        "def get_sodium(fdc_id):\n",
        "    nutrients = food_nutrient[(food_nutrient['fdc_id'] == fdc_id) & (food_nutrient['nutrient_id'] == sodium_id)]\n",
        "    return None if nutrients.empty else nutrients.iloc[0]['amount']\n",
        "\n",
        "def recommend_water(sodium_mg):\n",
        "    if sodium_mg is None:\n",
        "        return \"Sodium content unavailable.\"\n",
        "    if sodium_mg > 1500:\n",
        "        extra_water = round((sodium_mg - 1500) * 0.003, 2)\n",
        "        return f\"High sodium! Drink at least {extra_water}L extra water.\"\n",
        "    return \"Sodium is within the recommended daily intake.\""
      ],
      "metadata": {
        "id": "oyRZf0Ld6hdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, datasets, transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=101):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        self.base = models.resnet50(pretrained=False)\n",
        "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "model = MyResNet50()\n",
        "checkpoint_path = '/content/drive/My Drive/NexHack/checkpoint_6.pth'\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "train_dataset = datasets.Food101(root=\"./data\", split=\"train\", transform=transform, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "wRbBZ3mg50bb",
        "outputId": "70e9bcbc-55d3-4be7-e15c-04536853994e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for MyResNet50:\n\tMissing key(s) in state_dict: \"base.conv1.weight\", \"base.bn1.weight\", \"base.bn1.bias\", \"base.bn1.running_mean\", \"base.bn1.running_var\", \"base.layer1.0.conv1.weight\", \"base.layer1.0.bn1.weight\", \"base.layer1.0.bn1.bias\", \"base.layer1.0.bn1.running_mean\", \"base.layer1.0.bn1.running_var\", \"base.layer1.0.conv2.weight\", \"base.layer1.0.bn2.weight\", \"base.layer1.0.bn2.bias\", \"base.layer1.0.bn2.running_mean\", \"base.layer1.0.bn2.running_var\", \"base.layer1.0.conv3.weight\", \"base.layer1.0.bn3.weight\", \"base.layer1.0.bn3.bias\", \"base.layer1.0.bn3.running_mean\", \"base.layer1.0.bn3.running_var\", \"base.layer1.0.downsample.0.weight\", \"base.layer1.0.downsample.1.weight\", \"base.layer1.0.downsample.1.bias\", \"base.layer1.0.downsample.1.running_mean\", \"base.layer1.0.downsample.1.running_var\", \"base.layer1.1.conv1.weight\", \"base.layer1.1.bn1.weight\", \"base.layer1.1.bn1.bias\", \"base.layer1.1.bn1.running_mean\", \"base.layer1.1.bn1.running_var\", \"base.layer1.1.conv2.weight\", \"base.layer1.1.bn2.weight\", \"base.layer1.1.bn2.bias\", \"base.layer1.1.bn2.running_mean\", \"base.layer1.1.bn2.running_var\", \"base.layer1.1.conv3.weight\", \"base.layer1.1.bn3.weight\", \"base.layer1.1.bn3.bias\", \"base.layer1.1.bn3.running_mean\", \"base.layer1.1.bn3.running_var\", \"base.layer1.2.conv1.weight\", \"base.layer1.2.bn1.weight\", \"base.layer1.2.bn1.bias\", \"base.layer1.2.bn1.running_mean\", \"base.layer1.2.bn1.running_var\", \"base.layer1.2.conv2.weight\", \"base.layer1.2.bn2.weight\", \"base.layer1.2.bn2.bias\", \"base.layer1.2.bn2.running_mean\", \"base.layer1.2.bn2.running_var\", \"base.layer1.2.conv3.weight\", \"base.layer1.2.bn3.weight\", \"base.layer1.2.bn3.bias\", \"base.layer1.2.bn3.running_mean\", \"base.layer1.2.bn3.running_var\", \"base.layer2.0.conv1.weight\", \"base.layer2.0.bn1.weight\", \"base.layer2.0.bn1.bias\", \"base.layer2.0.bn1.running_mean\", \"base.layer2.0.bn1.running_var\", \"base.layer2.0.conv2.weight\", \"base.layer2.0.bn2.weight\", \"base.layer2.0.bn2.bias\", \"base.layer2.0.bn2.running_mean\", \"base.layer2.0.bn2.running_var\", \"base.layer2.0.conv3.weight\", \"base.layer2.0.bn3.weight\", \"base.layer2.0.bn3.bias\", \"base.layer2.0.bn3.running_mean\", \"base.layer2.0.bn3.running_var\", \"base.layer2.0.downsample.0.weight\", \"base.layer2.0.downsample.1.weight\", \"base.layer2.0.downsample.1.bias\", \"base.layer2.0.downsample.1.running_mean\", \"base.layer2.0.downsample.1.running_var\", \"base.layer2.1.conv1.weight\", \"base.layer2.1.bn1.weight\", \"base.layer2.1.bn1.bias\", \"base.layer2.1.bn1.running_mean\", \"base.layer2.1.bn1.running_var\", \"base.layer2.1.conv2.weight\", \"base.layer2.1.bn2.weight\", \"base.layer2.1.bn2.bias\", \"base.layer2.1.bn2.running_mean\", \"base.layer2.1.bn2.running_var\", \"base.layer2.1.conv3.weight\", \"base.layer2.1.bn3.weight\", \"base.layer2.1.bn3.bias\", \"base.layer2.1.bn3.running_mean\", \"base.layer2.1.bn3.running_var\", \"base.layer2.2.conv1.weight\", \"base.layer2.2.bn1.weight\", \"base.layer2.2.bn1.bias\", \"base.layer2.2.bn1.running_mean\", \"base.layer2.2.bn1.running_var\", \"base.layer2.2.conv2.weight\", \"base.layer2.2.bn2.weight\", \"base.layer2.2.bn2.bias\", \"base.layer2.2.bn2.running_mean\", \"base.layer2.2.bn2.running_var\", \"base.layer2.2.conv3.weight\", \"base.layer2.2.bn3.weight\", \"base.layer2.2.bn3.bias\", \"base.layer2.2.bn3.running_mean\", \"base.layer2.2.bn3.running_var\", \"base.layer2.3.conv1.weight\", \"base.layer2.3.bn1.weight\", \"base.layer2.3.bn1.bias\", \"base.layer2.3.bn1.running_mean\", \"base.layer2.3.bn1.running_var\", \"base.layer2.3.conv2.weight\", \"base.layer2.3.bn2.weight\", \"base.layer2.3.bn2.bias\", \"base.layer2.3.bn2.running_mean\", \"base.layer2.3.bn2.running_var\", \"base.layer2.3.conv3.weight\", \"base.layer2.3.bn3.weight\", \"base.layer2.3.bn3.bias\", \"base.layer2.3.bn3.running_mean\", \"base.layer2.3.bn3.running_var\", \"base.layer3.0.conv1.weight\", \"base.layer3.0.bn1.weight\", \"base.layer3.0.bn1.bias\", \"base.layer3.0.bn1.running_mean\", \"base.layer3.0.bn1.running_var\", \"base.layer3.0.conv2.weight\", \"base.layer3.0.bn2.weight\", \"base.layer3.0.bn2.bias\", \"base.layer3.0.bn2.running_mean\", \"base.layer3.0.bn2.running_var\", \"base.layer3.0.conv3.weight\", \"base.layer3.0.bn3.weight\", \"base.layer3.0.bn3.bias\", \"base.layer3.0.bn3.running_mean\", \"base.layer3.0.bn3.running_var\", \"base.layer3.0.downsample.0.weight\", \"base.layer3.0.downsample.1.weight\", \"base.layer3.0.downsample.1.bias\", \"base.layer3.0.downsample.1.running_mean\", \"base.layer3.0.downsample.1.running_var\", \"base.layer3.1.conv1.weight\", \"base.layer3.1.bn1.weight\", \"base.layer3.1.bn1.bias\", \"base.layer3.1.bn1.running_mean\", \"base.layer3.1.bn1.running_var\", \"base.layer3.1.conv2.weight\", \"base.layer3.1.bn2.weight\", \"base.layer3.1.bn2.bias\", \"base.layer3.1.bn2.running_mean\", \"base.layer3.1.bn2.running_var\", \"base.layer3.1.conv3.weight\", \"base.layer3.1.bn3.weight\", \"base.layer3.1.bn3.bias\", \"base.layer3.1.bn3.running_mean\", \"base.layer3.1.bn3.running_var\", \"base.layer3.2.conv1.weight\", \"base.layer3.2.bn1.weight\", \"base.layer3.2.bn1.bias\", \"base.layer3.2.bn1.running_mean\", \"base.layer3.2.bn1.running_var\", \"base.layer3.2.conv2.weight\", \"base.layer3.2.bn2.weight\", \"base.layer3.2.bn2.bias\", \"base.layer3.2.bn2.running_mean\", \"base.layer3.2.bn2.running_var\", \"base.layer3.2.conv3.weight\", \"base.layer3.2.bn3.weight\", \"base.layer3.2.bn3.bias\", \"base.layer3.2.bn3.running_mean\", \"base.layer3.2.bn3.running_var\", \"base.layer3.3.conv1.weight\", \"base.layer3.3.bn1.weight\", \"base.layer3.3.bn1.bias\", \"base.layer3.3.bn1.running_mean\", \"base.layer3.3.bn1.running_var\", \"base.layer3.3.conv2.weight\", \"base.layer3.3.bn2.weight\", \"base.layer3.3.bn2.bias\", \"base.layer3.3.bn2.running_mean\", \"base.layer3.3.bn2.running_var\", \"base.layer3.3.conv3.weight\", \"base.layer3.3.bn3.weight\", \"base.layer3.3.bn3.bias\", \"base.layer3.3.bn3.running_mean\", \"base.layer3.3.bn3.running_var\", \"base.layer3.4.conv1.weight\", \"base.layer3.4.bn1.weight\", \"base.layer3.4.bn1.bias\", \"base.layer3.4.bn1.running_mean\", \"base.layer3.4.bn1.running_var\", \"base.layer3.4.conv2.weight\", \"base.layer3.4.bn2.weight\", \"base.layer3.4.bn2.bias\", \"base.layer3.4.bn2.running_mean\", \"base.layer3.4.bn2.running_var\", \"base.layer3.4.conv3.weight\", \"base.layer3.4.bn3.weight\", \"base.layer3.4.bn3.bias\", \"base.layer3.4.bn3.running_mean\", \"base.layer3.4.bn3.running_var\", \"base.layer3.5.conv1.weight\", \"base.layer3.5.bn1.weight\", \"base.layer3.5.bn1.bias\", \"base.layer3.5.bn1.running_mean\", \"base.layer3.5.bn1.running_var\", \"base.layer3.5.conv2.weight\", \"base.layer3.5.bn2.weight\", \"base.layer3.5.bn2.bias\", \"base.layer3.5.bn2.running_mean\", \"base.layer3.5.bn2.running_var\", \"base.layer3.5.conv3.weight\", \"base.layer3.5.bn3.weight\", \"base.layer3.5.bn3.bias\", \"base.layer3.5.bn3.running_mean\", \"base.layer3.5.bn3.running_var\", \"base.layer4.0.conv1.weight\", \"base.layer4.0.bn1.weight\", \"base.layer4.0.bn1.bias\", \"base.layer4.0.bn1.running_mean\", \"base.layer4.0.bn1.running_var\", \"base.layer4.0.conv2.weight\", \"base.layer4.0.bn2.weight\", \"base.layer4.0.bn2.bias\", \"base.layer4.0.bn2.running_mean\", \"base.layer4.0.bn2.running_var\", \"base.layer4.0.conv3.weight\", \"base.layer4.0.bn3.weight\", \"base.layer4.0.bn3.bias\", \"base.layer4.0.bn3.running_mean\", \"base.layer4.0.bn3.running_var\", \"base.layer4.0.downsample.0.weight\", \"base.layer4.0.downsample.1.weight\", \"base.layer4.0.downsample.1.bias\", \"base.layer4.0.downsample.1.running_mean\", \"base.layer4.0.downsample.1.running_var\", \"base.layer4.1.conv1.weight\", \"base.layer4.1.bn1.weight\", \"base.layer4.1.bn1.bias\", \"base.layer4.1.bn1.running_mean\", \"base.layer4.1.bn1.running_var\", \"base.layer4.1.conv2.weight\", \"base.layer4.1.bn2.weight\", \"base.layer4.1.bn2.bias\", \"base.layer4.1.bn2.running_mean\", \"base.layer4.1.bn2.running_var\", \"base.layer4.1.conv3.weight\", \"base.layer4.1.bn3.weight\", \"base.layer4.1.bn3.bias\", \"base.layer4.1.bn3.running_mean\", \"base.layer4.1.bn3.running_var\", \"base.layer4.2.conv1.weight\", \"base.layer4.2.bn1.weight\", \"base.layer4.2.bn1.bias\", \"base.layer4.2.bn1.running_mean\", \"base.layer4.2.bn1.running_var\", \"base.layer4.2.conv2.weight\", \"base.layer4.2.bn2.weight\", \"base.layer4.2.bn2.bias\", \"base.layer4.2.bn2.running_mean\", \"base.layer4.2.bn2.running_var\", \"base.layer4.2.conv3.weight\", \"base.layer4.2.bn3.weight\", \"base.layer4.2.bn3.bias\", \"base.layer4.2.bn3.running_mean\", \"base.layer4.2.bn3.running_var\", \"base.fc.weight\", \"base.fc.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.bn3.num_batches_tracked\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.0.downsample.1.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.1.bn3.num_batches_tracked\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.bn2.num_batches_tracked\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\", \"layer1.2.bn3.num_batches_tracked\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.0.bn3.num_batches_tracked\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.downsample.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.bn2.num_batches_tracked\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.1.bn3.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.2.bn3.num_batches_tracked\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.bn1.num_batches_tracked\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.bn2.num_batches_tracked\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer2.3.bn3.num_batches_tracked\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn1.num_batches_tracked\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.bn2.num_batches_tracked\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.0.bn3.num_batches_tracked\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.0.downsample.1.num_batches_tracked\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn1.num_batches_tracked\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.bn2.num_batches_tracked\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.1.bn3.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.2.bn3.num_batches_tracked\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.bn1.num_batches_tracked\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.bn2.num_batches_tracked\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.3.bn3.num_batches_tracked\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.bn1.num_batches_tracked\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.bn2.num_batches_tracked\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.4.bn3.num_batches_tracked\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.bn1.num_batches_tracked\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.bn2.num_batches_tracked\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer3.5.bn3.num_batches_tracked\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn1.num_batches_tracked\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.bn2.num_batches_tracked\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.0.bn3.num_batches_tracked\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.0.downsample.1.num_batches_tracked\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn1.num_batches_tracked\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.bn2.num_batches_tracked\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.1.bn3.num_batches_tracked\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.bn1.num_batches_tracked\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.bn2.num_batches_tracked\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\", \"layer4.2.bn3.num_batches_tracked\", \"fc.weight\", \"fc.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2602351551>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/NexHack/checkpoint_6.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MyResNet50:\n\tMissing key(s) in state_dict: \"base.conv1.weight\", \"base.bn1.weight\", \"base.bn1.bias\", \"base.bn1.running_mean\", \"base.bn1.running_var\", \"base.layer1.0.conv1.weight\", \"base.layer1.0.bn1.weight\", \"base.layer1.0.bn1.bias\", \"base.layer1.0.bn1.running_mean\", \"base.layer1.0.bn1.running_var\", \"base.layer1.0.conv2.weight\", \"base.layer1.0.bn2.weight\", \"base.layer1.0.bn2.bias\", \"base.layer1.0.bn2.running_mean\", \"base.layer1.0.bn2.running_var\", \"base.layer1.0.conv3.weight\", \"base.layer1.0.bn3.weight\", \"base.layer1.0.bn3.bias\", \"base.layer1.0.bn3.running_mean\", \"base.layer1.0.bn3.running_var\", \"base.layer1.0.downsample.0.weight\", \"base.layer1.0.downsample.1.weight\", \"base.layer1.0.downsample.1.bias\", \"base.layer1.0.downsample.1.running_mean\", \"base.layer1.0.downsample.1.running_var\", \"base.layer1.1.conv1.weight\", \"base.layer1.1.bn1.weight\", \"base.layer1.1.bn1.bias\", \"base.layer1.1.bn1.running_mean\", \"base.layer1.1.bn1.running_var\", \"base.layer1.1.conv2.weight\", \"base.layer1.1.bn2.weight\", \"base.layer1.1.bn2.bias\", \"base.layer1.1.bn2.running_mean\", \"base.layer1.1.bn2.running_var\", \"base.layer1.1.conv3.weight\", \"base.layer1.1.bn3.weight\", \"base.layer1.1.bn3.bias\", \"base.layer1.1.bn3.running_mean\", \"base.layer1.1.bn3.running_var\", \"base.layer1.2.conv1.weight\", \"base.layer1.2.bn1.weight\", \"base.layer1.2.bn1.bias\", \"base.layer1.2.bn1.running_mean\", \"base.layer1.2.bn1.running_var\", \"base.layer1.2.conv2.weight\", \"base.layer1.2.bn2.weight\", \"base.layer1.2.bn2.bias\", \"base.layer1.2.bn2.runnin...\n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.bn3.num_batches_tracked\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.0.downsample.1.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.1.bn3.num_batches_tracked\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    pred_idx = output.argmax(dim=1).item()\n",
        "    pred_class = train_dataset.classes[pred_idx]\n",
        "\n",
        "print(f\"\\nðŸ” Predicted food class: {pred_class}\")\n"
      ],
      "metadata": {
        "id": "KYUjbngq6OLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = pred_class.lower().strip()\n",
        "result = process.extractOne(query, choices, scorer=fuzz.token_sort_ratio, score_cutoff=70)\n",
        "\n",
        "if result is None:\n",
        "    print(\"No good match found in nutrient database.\")\n",
        "else:\n",
        "    matched_name, score, index = result\n",
        "    matched_row = food_all.iloc[index]\n",
        "\n",
        "    sodium = get_sodium(matched_row['fdc_id'])\n",
        "    sodium_str = f\"{sodium} mg per 100g\" if sodium else \"No sodium info found\"\n",
        "\n",
        "    print(f\"\\nBest match: {matched_row['description']} (Score: {score})\")\n",
        "    print(f\"Sodium: {sodium_str}\")\n",
        "    print(f\"Data source: {matched_row['source']}\")\n",
        "    print(recommend_water(sodium))\n"
      ],
      "metadata": {
        "id": "LvjnlBcN6OA9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
